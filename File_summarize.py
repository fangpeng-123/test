import os
from langchain_ollama import OllamaLLM
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from PyPDF2 import PdfReader
from docx import Document as DocxDocument

# åŠ è½½æ–‡æ¡£å‡½æ•°
def load_document(file_path):
    ext = os.path.splitext(file_path)[1].lower()

    if ext == ".txt":
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
        return [Document(page_content=text)]
    
    elif ext == ".pdf":
        reader = PdfReader(file_path)
        texts = [page.extract_text() for page in reader.pages if page.extract_text()]
        return [Document(page_content=t) for t in texts]
    
    elif ext == ".docx":
        doc = DocxDocument(file_path)
        texts = [para.text for para in doc.paragraphs if para.text.strip()]
        return [Document(page_content=t) for t in texts]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{ext}")
    
# åˆå§‹åŒ–å¤§æ¨¡å‹
ollm = OllamaLLM(model="qwen3:8b")

# æ€»ç»“å‡½æ•°
def summarize_document(file_path):
    print("ğŸ“„ æ­£åœ¨åŠ è½½æ–‡ä»¶...")
    docs = load_document(file_path)

    print("âœ‚ï¸ æ­£åœ¨åˆ‡å‰²æ–‡ä»¶...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)
    split_docs = text_splitter.split_documents(docs)

    print("ğŸ§  æ­£åœ¨ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ€»ç»“...")
    chain = load_summarize_chain(ollm, chain_type="map_reduce", verbose=False)
    summary = chain.invoke(split_docs)

    return summary['output_text']

# æµ‹è¯•
if __name__ == "__main__":
    file_path = input("è¯·è¾“å…¥ä½ è¦æ€»ç»“çš„æœ¬åœ°æ–‡ä»¶ä½ç½®ï¼ˆæ”¯æŒtxt/pdf/docxï¼‰ï¼š").strip()

    if not os.path.exists(file_path):
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
    else:
        result = summarize_document(file_path)
        print("\nğŸ“ æœ€ç»ˆæ€»ç»“ç»“æœï¼š")
        print(result)