# 基于人脸识别的陌生人检测功能包

**概述：**这是一个基于`ROS2`的实时人脸检测与识别功能包，专门为`RK3588`等嵌入式平台优化设计。系统结合了`YOLOv8`人脸检测和`ArcFace/MobileFaceNet`人脸识别技术，能够实时检测摄像头中的人脸并与已知人脸数据库进行比对，识别特定人员或标记陌生人。

**技术栈：**YOLOv8-face、arcface、 模糊度、OpenCV、图片预处理、Python、ROS2



## 一、人脸检测模型训练

这部分主要围绕如何使用挑选好的数据集在服务器上训练 `YOLOv8-face` 模型并测试模型的检测效果。初次尝试实现人脸检测是使用的`retinaface`但是模型太大考虑到开发板算力有限即使经过量化处理也不能放在放在`RK3588`。所以最终选择了`YOLO`模型作为检测模型。

#### 1.1 准备数据集

##### 1.1.1 下载数据集

- 整理如下三个常见数据集下载网站：

> [roboflow](https://universe.roboflow.com/)
>
> [kaggle](https://www.kaggle.com/)
>
> [modelscope](https://www.modelscope.cn/datasets)

- 正确的数据集格式：下载数据集是需要选择 YOLO 格式否则模型训练是会报错数据格式错误。正确格式如下：

  ```
  ├── test
  │   ├── images
  │   └── labels
  ├── train
  │   ├── images
  │   └── labels
  └── valid
      ├── images
      └── labels
  ```

- 数据集处理：通过`Remmina`软件远程连接将数据集转移到3070服务器上并完成解压。

  ```
  # 文件操作流程
  选择服务器上 此电脑\yls-NUC11PAHi7 上的 _media_yls_1T硬盘4 文件夹	->	选择数据集转移到 D:\face
  ```

​	注意这一步如果看到不到`此电脑\yls-NUC11PAHi7 上的 _media_yls_1T硬盘4`需要重新配置远程连接在共享文件夹选择`1T硬盘`。



#### 1.2 模型训练

##### 1.2.1 创建配置文件

在项目中找到配置文件创建地址并根据项目需要创建配置文件，需要指定数据集位置以及识别的类别。

```bash
# 配置文件创建地址
C:\ultralytics\ultralytics\cfg\datasets
```

```yaml
# 配置文件
train: D:/face/FLASH MDP Face Detection.v7-version2.yolov8/train
val: D:/face/FLASH MDP Face Detection.v7-version2.yolov8/valid
test: D:/face/FLASH MDP Face Detection.v7-version2.yolov8/test

nc: 1
names: ['face']
```



##### 1.2.2 训练脚本

训练脚本除了模型保存的文件夹名称外几乎不需要调整，但是如果遇到过拟合或者需要调整超参数时例如学习率以及轮次时需要额外做出调整。

```python
# from ultralytics import YOLO

# if __name__ == '__main__':
#     # 加载预训练模型
#     model = YOLO("yolov8n.pt")  # 推荐用于训练

#     # 开始训练
#     model.train(
#         project='fire',
#         name='exp_fixed',
#         data="fire.yaml",
#         epochs=300,
#         imgsz=640,
#         device=[0,1],
#         resume=False,
#         batch=64,
#         optimizer='AdamW',
#         lr0=0.005,
#         lrf=0.1,
#         cos_lr=True,
#         weight_decay=0.0001,
#         patience=50,
#         workers=8,
#         augment=True,
#         mosaic=1.0,
#         copy_paste=0.5,
#         mixup=0.2,
#         hsv_h=0.015,
#         hsv_s=0.7,
#         hsv_v=0.4,
#         degrees=10,
#         translate=0.1,
#         scale=0.5,
#         shear=0.0,
#         perspective=0.0,
#         flipud=0.0,
#         fliplr=0.5,
#         bgr=0.0,
#         erasing=0.4,
# )

#     # 验证模型
#     metrics = model.val()

#     # 推理测试图片
#     # results = model([
#     #     "./images/fire1.jpg", "./images/fire3.jpg", "./images/fire5.jpg",
#     #     "./images/fire2.jpg", "./images/fire4.jpg", "./images/fire6.jpg"
#     # ], save=True)


from ultralytics import YOLO

if __name__ == '__main__':
    model = YOLO("yolov8n.pt")
    model.train(
        project='face_recognition',		# 项目文件夹
        name='face_recognition',		# 单次训练数据
        data="face.yaml",
        epochs=300,						# 轮次
        imgsz=640,
        device=[0,1],					# 指定训练设备
        lr0=0.001,						# 学习率
        optimizer='Adam',
    )
```



##### 1.2.3 导出模型

- pt2onnx：这一步可以选择在服务器导出或者在本地进行模型转换导出。`ultralytics`项目训练的模型是`pt`格式并且项目自带模型转换命令或者使用本地的转换代码将模型转为`onnx`格式

  ```bash
  # 激活环境
  conda activate YOLO
  
  # 转换命令（pt -> onnx）
  yolo export model=C:\ultralytics\face_recognition\face_recognition3\weights\best.pt format=onnx
  ```

  ```python
  # python 转换（pt -> onnx）
  import cv2
  from ultralytics import YOLO
  
  pt_path = '/home/yls/YOLOV8-on-RK3588/rk3588/python/flood_detect/model/best_1026.pt'
  model = YOLO(pt_path)
  result = model.predict(source='/home/yls/YOLOV8-on-RK3588/rk3588/python/flood_detect/image/flood.jpeg')
  cv2.imshow('result', result[0].plot())
  cv2.waitKey()
  cv2.destroyAllWindows()
  # model.export(format='onnx')
  model.export(format='onnx', opset=12, imgsz=(640, 640), simplify=True, dynamic=False, int8=False)
  ```

  

- onnx2rknn：将`onnx`再转为`rknn`格式，注意`rknn`格式是在`rk3588`开发板上运行需要使用rknn工具链进行转化使模型可以加速运行。转换需要准备量化数据集并在代码中指定位置以及需要转化的模型位置。

  - 量化数据集：可以从训练数据集中挑选20张左右的图片按照如下格式放在指定文件中：

    ```txt
    ├── check0_base_optimize.onnx
    ├── check2_correct_ops.onnx
    ├── check3_fuse_ops.onnx
    ├── image
    │   ├── flood10.jpg
    │   ├── flood1.jpeg
    │   ├── flood2.jpg
    │   ├── flood3.jpg
    │   ├── flood4.jpg
    │   ├── flood5.jpeg
    │   ├── flood6.jpg
    │   ├── flood7.jpeg
    │   ├── flood8.jpg
    │   ├── flood9.jpg
    │   ├── flood.jpeg
    │   └── imagelist.txt		# 存储每一张量化数据集图片的位置，使用相对位置即可
    ├── img_result.jpg
    ├── model
    │   ├── flood_1026_0914.rknn
    │   ├── flood_1027_1359.rknn
    │   ├── flood_1028_0846.rknn
    │   ├── flood_1029_0841.rknn
    │   ├── flood_1030_1621.rknn
    │   ├── flood_1102_0929.rknn
    │   └── flood_1102_1410.rknn
    └── rknn_infer_x86.py
    ```

    

  ```python
  # 转换核心代码
  def export_rknn():
      """onnx模型转换为rknn
      """
      rknn = RKNN(verbose=True)
  
      rknn.config(
          # see:ultralytics/yolo/data/utils.py
          mean_values=[[0, 0, 0]],
          std_values=[[255, 255, 255]],
          # TODO:使用下面均值、方差后，效果更差：
          # mean_values=[[123.675, 116.28, 103.53]],  # IMAGENET_MEAN = 0.485, 0.456, 0.406
          # std_values=[[58.395, 57.12, 57.375]],  # IMAGENET_STD = 0.229, 0.224, 0.225
          # quant_img_RGB2BGR=True,
          quantized_algorithm='normal',
          quantized_method='channel',
          # quantized_dtype='asymmetric_quantized-16', 
          # optimization_level=2,
          compress_weight=False,  # 压缩模型的权值，可以减小rknn模型的大小。默认值为False。
          # single_core_mode=True,
          # model_pruning=False,  # 修剪模型以减小模型大小，默认值为False。
          target_platform='rk3588'
      )
      rknn.load_onnx(
          model=ONNX_MODEL,
          inputs=['images'],                     # ONNX 的输入节点名
          input_size_list=[[1, 3, 640, 640]],    # 静态形状
          # 获取onnx模型中的以下六个输出，可打开https://netron.app查看节点
          outputs=['/model.22/cv2.0/cv2.0.2/Conv_output_0',
                   '/model.22/cv3.0/cv3.0.2/Conv_output_0',
                   '/model.22/cv2.1/cv2.1.2/Conv_output_0',
                   '/model.22/cv3.1/cv3.1.2/Conv_output_0',
                   '/model.22/cv2.2/cv2.2.2/Conv_output_0',
                   '/model.22/cv3.2/cv3.2.2/Conv_output_0'])
      # rknn.load_onnx(model=ONNX_MODEL)
      rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET, rknn_batch_size=1)
      rknn.export_rknn(RKNN_MODEL)
  
      # 使用accuracy_analysis 接口进行模型量化精度分析
      # rknn.accuracy_analysis(
      #     inputs=['./image/fire.jpg'], # 表示进行推理的图像
      #     output_dir='snapshot', # 表示精度分析的输出目录
      #     target=None, # 默认为None，表示运行在模拟器上
      #     device_id=None, # 设备的编号
      # )
  
      rknn.init_runtime()
      return rknn
  
  ```

  

#### 1.3 开发板测试模型

##### 1.3.1 服务器测试

直接通过`ultralytics`自带的检测命令查看训练后的`pt`模型的识别效果一般置信度大于0.85认为是效果达标不会在后续量化时造成精度锐减。

```bash
yolo predict model=C:\ultralytics\face_recognition\face_recognition3\weights\best.pt source=C:\ultralytics\images\face
```



##### 1.3.2 量化后测试

经过量化处理的`rknn`模型在转换脚本中已经有测试代码如下所示，如果顺利则会看到图片中显示出了需要识别的物体被框选出来并且置信度大于0.85。

```python
# 画图框选	
def draw_boxes(img, boxes, scores, classes):
    """
    Draws bounding boxes and labels on the input image based on the detected objects.

    Args:
        img: The input image to draw detections on.
        boxes: Detected bounding box.
        scores: Corresponding detection score.
        classes: Class ID for the detected object.

    Returns:
        None
    """
    # Get the box, score, and class ID corresponding to the index
    # Draw the detection on the input image
    # Extract the coordinates of the bounding box
    # Retrieve the color for the class ID
    try:
        for box, score, cl in zip(boxes, scores, classes):
            top, left, right, bottom = [int(_b) for _b in box]

            color = color_palette[cl]
            # color = (0, 255, 127)

            # Draw the bounding box on the image
            cv2.rectangle(img, (top, left), (right, bottom), color, 2)

            # Create the label text with class name and score
            label = f'{CLASSES[cl]}: {score:.2f}'

            # Calculate the dimensions of the label text
            (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX,
                                                             0.5, 1)

            # Calculate the position of the label text
            label_x = top
            label_y = left - 10 if left - 10 > label_height else left + 10

            # Draw a filled rectangle as the background for the label text
            cv2.rectangle(img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height),
                          color, cv2.FILLED)

            # Draw the label text on the image
            cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX,
                        0.5, (0, 0, 0), 1, cv2.LINE_AA)
    except TypeError:
        # print('box is none')
        pass

    return img
```



## 二、人脸特征提取识别模型

这部分代码主要介绍如何实现对人脸数据库中图片进行特征提取以及怎么利用模型做到对输入模型的图片进行人脸识别。初步测试时考虑直接对人脸特征的npy数组做余弦相似度的判断但是测试后发现相似度太低并且远距离识别效果太差，现在使用`arcface`模型进行判断。

#### 2.1 服务器模型测试

##### 2.1.1 特征提取

`MobileFaceNetExtractor` 是一个**轻量级人脸特征提取工具类**，基于 ArcFace 算法与 MobileNetV3 骨干网络实现，核心功能是从输入的人脸图像中提取标准化的特征向量，适用于人脸识别、人脸比对、身份验证等场景。该类优化了边缘设备兼容性（如 RK3588），支持双模型加载方案，兼顾灵活性与易用性。

- **核心特性**
  1. **数据类型标准化**：若输入为非 `uint8` 类型（如 float32），则 `(face_img * 255).clip(0, 255).astype(np.uint8)`，确保输入符合模型要求；
  2. **通道数补全**：若为 2 维灰度图，通过 `cv2.COLOR_GRAY2BGR` 转换为 3 通道 BGR 图；
  3. **颜色空间转换**：彩色图从 BGR（OpenCV 默认）转为 RGB（insightface 模型期望输入）；
  4. **图像尺寸缩放**：插值法（`INTER_LINEAR`）缩放到 `(112, 112)`（MobileFaceNet 标准输入尺寸）；
  5. **特征提取**：调用 `self.rec_model.get_feat([face_112])[0]`，获取原始特征向量；
  6. **L2 归一化**：计算特征向量的 L2 范数，非零时返回归一化结果（消除尺度影响，适配余弦相似度计算）；
  7. **异常处理**：提取过程中报错时，打印错误类型、输入图像的形状和数据类型，返回 `None`。

- **初始化流程**

  1. **执行提供者配置**：默认优先使用 GPU（CUDA），无 GPU 时降级为 CPU；

  2. 模型加载双方案

     > 方案 A（本地 ONNX 模型）：若 `model_path` 有效，通过 `insightface.model_zoo.ArcFaceONNX` 加载，调用 `prepare(ctx_id=0)` 初始化（`ctx_id=0` 对应 GPU，需 CPU 则改为 `-1`）；

     > 方案 B（预训练包）：若 `model_path` 无效，通过 `insightface.app.FaceAnalysis` 加载 `buffalo_sc` 包（内置轻量化 MobileFaceNet），初始化检测尺寸为 `(640, 640)`；

  3. **特征维度检测**：用 `(112, 112, 3)` 的虚拟输入测试模型，获取输出特征向量维度，赋值给 `self.feature_dim`；

  4. **异常处理**：加载失败时抛出 `Exception`，包含具体错误信息。

```python
class MobileFaceNetExtractor:
    """ArcFace-MobileNetV3 特征提取器 - 轻量级版本"""
    def __init__(self, model_path=None, providers=None):
        """
        初始化
        Args:
            model_path: 可选，MobileNetV3模型路径(.onnx文件)。如果为None则使用insightface的buffalo_m包
            providers: ONNX执行提供者
        """
        if providers is None:
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
        print(f"  Providers: {providers}")
        try:
            # 方案A：使用本地ONNX模型（推荐用于RK3588）
            if model_path and os.path.exists(model_path):
                print(f"  加载本地模型: {model_path}")
                from insightface.model_zoo import ArcFaceONNX
                self.rec_model = ArcFaceONNX(model_path)
                self.rec_model.prepare(ctx_id=0)  # ctx_id=0使用GPU, -1使用CPU
                
                # 检查特征维度
                dummy_input = np.zeros((112, 112, 3), dtype=np.uint8)
                dummy_feature = self.rec_model.get_feat([dummy_input])[0]
                self.feature_dim = dummy_feature.shape[0]
                
            # 方案B：使用insightface的buffalo_m包（包含MobileFaceNet）
            else:
                print("  使用insightface buffalo_m包（内置MobileFaceNet）")
                from insightface.app import FaceAnalysis
                self.app = FaceAnalysis(name='buffalo_sc', providers=providers)  # buffalo_m包含轻量化模型
                self.app.prepare(ctx_id=0, det_size=(640, 640))
                self.rec_model = self.app.models['recognition']
                
                # 检查特征维度
                dummy_input = np.zeros((112, 112, 3), dtype=np.uint8)
                dummy_feature = self.rec_model.get_feat([dummy_input])[0]
                self.feature_dim = dummy_feature.shape[0]
            
            print("  ✓ ArcFace-MobileNetV3 模型加载成功")
            print(f"  特征维度: {self.feature_dim}")
            
        except Exception as e:
            raise Exception(f"模型加载失败: {e}")
    
    def extract(self, face_img):
        """提取特征向量 - 适配MobileNetV3"""
        try:
            # 1. 确保是uint8类型
            if face_img.dtype != np.uint8:
                face_img = (face_img * 255).clip(0, 255).astype(np.uint8)
            
            # 2. 确保是3通道彩色图像
            if len(face_img.shape) == 2:  # 如果是灰度图
                face_img = cv2.cvtColor(face_img, cv2.COLOR_GRAY2BGR)
            
            # 3. BGR转RGB
            if len(face_img.shape) == 3 and face_img.shape[2] == 3:
                face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
            else:
                face_rgb = face_img
            
            # 4. 缩放到112x112
            face_112 = cv2.resize(face_rgb, (112, 112), interpolation=cv2.INTER_LINEAR)
            
            # 5. 提取特征
            feature = self.rec_model.get_feat([face_112])[0]
            
            # 6. L2归一化
            norm = np.linalg.norm(feature)
            if norm > 0:
                return feature / norm
            
            print("  警告: 特征向量为零")
            return None
            
        except Exception as e:
            print(f"  特征提取失败: {type(e).__name__}: {e}")
            print(f"  输入形状: {face_img.shape}, 类型: {face_img.dtype}")
            return None
```



##### 2.1.2 测试模型效果

- **核心特性**
  1. **全流程自动化**：从人脸检测、特征提取、数据库比对到结果可视化，无需手动干预；
  2. **轻量高效架构**：YOLOv8-face（快速人脸检测）+ MobileFaceNetV3（轻量化特征提取），兼顾速度与精度；
  3. **智能数据库管理**：自动遍历指定目录构建人脸库，支持多图平均特征（提升识别稳定性），自动过滤无效图片 / 特征；
  4. **灵活配置**：支持自定义相似度阈值、YOLO 模型路径、本地 ArcFace 模型路径，适配不同场景需求；
  5. **可视化友好**：检测框 + 身份标签 + 相似度显示，支持窗口缩放，结果自动保存；
  6. **完善的异常处理**：图片读取失败、人脸未检测到、特征维度不匹配等场景均有日志提示，便于问题排查；
  7. **安全兼容**：自动处理非法文件名，支持多种图片格式（JPG/JPEG/PNG/BMP），跨平台适配 CUDA/CPU。

- **初始化流程**
  1. **加载 YOLOv8-face 检测模型**：通过 `ultralytics.YOLO` 加载指定路径的模型，用于后续人脸区域检测；
  2. **设备自动适配**：检测 CUDA 可用性，自动选择 `cuda` 或 `cpu` 作为运行设备；
  3. **加载特征提取器**：初始化 `MobileFaceNetExtractor`，传入本地模型路径（可选）和硬件适配的执行提供者；
  4. **配置相似度阈值**：存储用户指定的阈值，用于后续身份匹配判断；
  5. **构建人脸数据库**：调用 `_build_face_database` 方法，遍历 `face_db_dir` 生成特征库；
  6. **输出初始化总结**：打印数据库人数、阈值、特征维度、设备等关键信息，便于验证配置。

```python
class FaceRecognitionSystem:
    def __init__(self, yolo_model_path, face_db_dir, similarity_threshold=0.4, 
                 arcface_model_path=None):
        print("\n===== 初始化人脸识别系统 =====")
        # 1. 加载YOLO
        print("\n1. 加载 YOLOv8-face 检测模型...")
        self.yolo_detector = YOLO(yolo_model_path)
        print(f"   ✓ YOLO 模型加载成功")
        
        # 2. 设备配置
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"\n2. 设备配置: {self.device}")
        
        # 3. 加载ArcFace-MobileNetV3
        print("\n3. 加载 ArcFace-MobileNetV3 特征提取模型...")
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
        self.arcface_extractor = MobileFaceNetExtractor(
            model_path=arcface_model_path,  # 可传入本地模型路径
            providers=providers
        )
        
        # 4. 相似度阈值
        self.similarity_threshold = similarity_threshold
        
        # 5. 构建人脸数据库
        print("\n4. 构建人脸特征数据库...")
        self.face_db = {}
        self.db_features = []
        self.db_names = []
        self._build_face_database(face_db_dir)
        
        # 打印总结信息
        print(f"\n===== 系统初始化完成 =====")
        print(f"数据库共有 {len(self.db_names)} 个人")
        print(f"相似度阈值: {similarity_threshold}")
        print(f"特征维度: {self.arcface_extractor.feature_dim}")
        print(f"设备: {self.device}")
        print("=" * 40)
    
    def _build_face_database(self, face_db_dir):
        """构建人脸特征数据库"""
        face_db_path = Path(face_db_dir)
        
        if not face_db_path.exists():
            raise Exception(f"人脸数据库目录不存在: {face_db_dir}")
        
        if not any(face_db_path.iterdir()):
            print(f"   警告: {face_db_dir} 目录为空！")
            print(f"   请按以下结构组织人脸图片:")
            print(f"   {face_db_dir}\\")
            print(f"   ├── 张三\\")
            print(f"   │   ├── img1.jpg")
            print(f"   │   └── img2.jpg")
            print(f"   └── 李四\\")
            print(f"       └── photo.jpg")
            return
        
        total_people = 0
        total_images = 0
        
        # 支持的图片格式
        img_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        
        # 遍历每个人的文件夹
        for person_dir in face_db_path.iterdir():
            if person_dir.is_dir():
                person_name = person_dir.name
                print(f"   处理 {person_name}...")
                
                features_list = []
                person_images = 0
                
                # 收集所有图片
                image_paths = []
                for ext in img_extensions:
                    image_paths.extend(person_dir.glob(ext))
                
                # 遍历该人的所有图片
                for img_path in image_paths:
                    try:
                        face_feature = self._extract_face_feature(str(img_path))
                        if face_feature is not None:
                            # 验证特征维度
                            if face_feature.ndim == 1 and face_feature.shape[0] == self.arcface_extractor.feature_dim:
                                features_list.append(face_feature)
                                person_images += 1
                                total_images += 1
                            else:
                                print(f"     警告: 特征维度异常 {face_feature.shape}，跳过")
                    except Exception as e:
                        print(f"     警告: 处理 {img_path.name} 失败: {e}")
                
                if features_list:
                    # 平均该人的所有特征作为代表
                    avg_feature = np.mean(features_list, axis=0)
                    
                    # 验证特征
                    if avg_feature.ndim == 1 and avg_feature.shape[0] == self.arcface_extractor.feature_dim:
                        self.face_db[person_name] = avg_feature
                        total_people += 1
                        print(f"     ✓ 提取 {person_images} 张图片的特征 (维度: {avg_feature.shape[0]})")
                    else:
                        print(f"     警告: 特征维度异常 {avg_feature.shape}")
                else:
                    print(f"     警告: {person_name} 没有有效的人脸图片")
        
        # 准备用于比对的数组
        self.db_names = list(self.face_db.keys())
        if self.db_names:
            self.db_features = np.array([self.face_db[name] for name in self.db_names])
            print(f"   数据库特征矩阵形状: {self.db_features.shape}")
        else:
            self.db_features = np.array([])
        
        if not self.db_names:
            print("   警告: 人脸数据库为空，所有人将被识别为陌生人！")
        else:
            print(f"   ✓ 成功构建 {total_people} 个人的数据库，共 {total_images} 张图片")
    
    def _extract_face_feature(self, image_path):
        """从单张图片提取人脸特征"""
        img = cv2.imread(image_path)
        if img is None:
            print(f"     警告: 无法读取图片 {image_path}")
            return None
        
        # 使用YOLO检测人脸
        results = self.yolo_detector(img, verbose=False)
        
        if len(results[0].boxes) == 0:
            print(f"     警告: 未检测到人脸 {Path(image_path).name}")
            return None
        
        # 获取最大的人脸框
        boxes = results[0].boxes.xyxy.cpu().numpy()
        areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
        largest_idx = np.argmax(areas)
        
        x1, y1, x2, y2 = boxes[largest_idx].astype(int)
        face_img = img[y1:y2, x1:x2]
        
        return self._get_arcface_embedding(face_img)
    
    def _get_arcface_embedding(self, face_img):
        """使用 ArcFace 提取特征向量"""
        try:
            print(f"   人脸区域形状: {face_img.shape}")
            feature = self.arcface_extractor.extract(face_img)
            
            if feature is not None:
                print(f"   成功提取特征 (维度: {feature.shape})")
            
            return feature
        except Exception as e:
            print(f"   特征提取失败: {type(e).__name__}: {e}")
            return None
    
    def detect_and_recognize(self, image):
        """
        检测并识别图片中的人脸
        Returns:
            list: 每个检测到的人脸信息
        """
        # YOLOv8人脸检测
        results = self.yolo_detector(image, verbose=False)
        
        # 调试信息
        if len(results[0].boxes) == 0:
            print("   信息: YOLO未检测到任何人脸")
        else:
            print(f"   信息: YOLO检测到 {len(results[0].boxes)} 个人脸")
        
        detections = []
        
        if len(results[0].boxes) == 0:
            return detections
        
        boxes = results[0].boxes.xyxy.cpu().numpy()
        confidences = results[0].boxes.conf.cpu().numpy()
        
        for i, (box, conf) in enumerate(zip(boxes, confidences)):
            x1, y1, x2, y2 = box.astype(int)
            
            # 提取人脸区域
            face_img = image[y1:y2, x1:x2]
            if face_img.size == 0:
                print(f"   警告: 人脸区域为空 {i}")
                continue
            
            # 提取特征
            feature = self._get_arcface_embedding(face_img)
            if feature is None:
                print(f"   警告: 无法提取人脸 {i} 的特征")
                continue
            
            # 验证特征维度
            if feature.shape[0] != self.arcface_extractor.feature_dim:
                print(f"   警告: 特征维度不匹配! 预期: {self.arcface_extractor.feature_dim}, 实际: {feature.shape[0]}")
                continue
            
            # 与数据库比对
            name, similarity = self._identify_person(feature)
            
            detections.append({
                'bbox': (x1, y1, x2, y2),
                'confidence': float(conf),
                'name': name,
                'similarity': similarity
            })
        
        return detections
    
    def _identify_person(self, feature):
        """识别最相似的人"""
        if len(self.db_features) == 0:
            return "陌生人", 0.0
        
        # 检查维度匹配
        if feature.shape[0] != self.db_features.shape[1]:
            print(f"   警告: 特征维度不匹配! 输入: {feature.shape[0]}, 数据库: {self.db_features.shape[1]}")
            return "陌生人", 0.0
        
        # 计算相似度
        feature = feature.reshape(1, -1)
        similarities = cosine_similarity(feature, self.db_features)[0]
        
        # 找到最相似的
        best_idx = np.argmax(similarities)
        max_similarity = similarities[best_idx]
        
        if max_similarity >= self.similarity_threshold:
            return self.db_names[best_idx], float(max_similarity)
        else:
            return "陌生人", float(max_similarity)
    
    def visualize(self, image, detections):
        """可视化结果"""
        vis_img = image.copy()
        
        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            name = det['name']
            similarity = det['similarity']
            conf = det['confidence']
            
            # 画框
            color = (0, 255, 0) if name != "陌生人" else (0, 0, 255)
            cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, 2)
            
            # 画标签
            label = f"{name} ({similarity:.2f})"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            
            cv2.rectangle(vis_img, (x1, y1 - label_size[1] - 10), 
                         (x1 + label_size[0], y1), color, -1)
            cv2.putText(vis_img, label, (x1, y1 - 5), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return vis_img


def process_image(image_path, yolo_model_path, face_db_dir, output_dir="results", 
                similarity_threshold=0.4,  # MobileNetV3建议阈值0.35-0.45
                show_result=False, max_window_size=(800, 600)):
    """
    处理单张图片
    similarity_threshold: MobileNetV3模型建议设置在0.35-0.45
    """
    print(f"\n{'='*60}")
    print(f"开始处理图片: {Path(image_path).name}")
    print(f"使用ArcFace-MobileNetV3，相似度阈值: {similarity_threshold}")
    print(f"{'='*60}")
    
    try:
        # 1. 验证输入文件
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"图片文件不存在: {image_path}")
        
        if not os.path.exists(yolo_model_path):
            raise FileNotFoundError(f"YOLO模型不存在: {yolo_model_path}")
        
        # 2. 读取图片
        print(f"\n[1/4] 读取图片...")
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"无法读取图片文件: {image_path}")
        
        h, w = image.shape[:2]
        print(f"   ✓ 成功读取: {w}x{h} 像素")
        
        # 3. 初始化系统
        print(f"\n[2/4] 初始化人脸识别系统...")
        system = FaceRecognitionSystem(
            yolo_model_path=yolo_model_path,
            face_db_dir=face_db_dir,
            similarity_threshold=similarity_threshold
        )
        print(f"   ✓ 系统初始化完成")
        
        # 4. 执行检测识别
        print(f"\n[3/4] 检测并识别人脸...")
        detections = system.detect_and_recognize(image)
        
        if not detections:
            print("   ⚠ 警告: 未检测到任何有效人脸")
        else:
            print(f"   ✓ 检测到 {len(detections)} 个有效人脸")
        
        # 5. 可视化结果
        print(f"\n[4/4] 生成结果图像...")
        result_img = system.visualize(image, detections)
        
        # 6. 保存结果
        os.makedirs(output_dir, exist_ok=True)
        output_path = Path(output_dir) / f"result_{Path(image_path).name}"
        safe_name = "".join(c for c in output_path.name if c not in r'\/:*?"<>|')
        output_path = output_path.with_name(safe_name)
        
        cv2.imwrite(str(output_path), result_img)
        print(f"   ✓ 结果已保存至: {output_path}")
        
        # 7. 打印检测摘要
        print(f"\n{'='*60}")
        print("检测摘要:")
        print(f"{'='*60}")
        if detections:
            for i, det in enumerate(detections, 1):
                print(f"{i}. 位置: {det['bbox']}")
                print(f"   身份: {det['name']}")
                print(f"   置信度: {det['confidence']:.3f}")
                print(f"   相似度: {det['similarity']:.3f}")
        else:
            print("未检测到人脸")
        print(f"{'='*60}\n")
        
        # 8. 显示结果（可选）
        if show_result:
            try:
                display_img = result_img.copy()
                h, w = display_img.shape[:2]
                max_w, max_h = max_window_size
                
                scale = min(max_w / w, max_h / h, 1.0)
                if scale < 1.0:
                    display_img = cv2.resize(display_img, (int(w * scale), int(h * scale)))
                
                cv2.namedWindow("人脸识别结果", cv2.WINDOW_NORMAL)
                cv2.resizeWindow("人脸识别结果", display_img.shape[1], display_img.shape[0])
                cv2.imshow("人脸识别结果", display_img)
                
                print("等待按键关闭窗口...")
                while True:
                    key = cv2.waitKey(1) & 0xFF
                    if key != 255:
                        break
                
                cv2.destroyAllWindows()
                print("   ✓ 窗口已关闭")
                
            except Exception as e:
                print(f"   ⚠ 无法显示图像: {e}")
                print("   建议: pip install opencv-contrib-python")
        
        return result_img, detections
        
    except Exception as e:
        print(f"\n{'='*60}")
        print(f"❌ 处理失败: {type(e).__name__}")
        print(f"错误信息: {str(e)}")
        print(f"{'='*60}")
        
        import traceback
        print("\n详细错误信息:")
        traceback.print_exc()
        
        return None, []
```



## 三、模型整合功能包开发

功能概述：`FaceRecognitionSystem` 是一套**端到端轻量级人脸识别解决方案**，整合了 YOLOv8-face 人脸检测、ArcFace-MobileNetV3 特征提取、人脸特征数据库管理三大核心能力，支持从图像中自动检测人脸、提取特征、与数据库比对识别身份，并提供可视化结果输出。配套的 `process_image` 函数是封装后的单图处理工具，无需手动初始化类即可快速完成单张图片的人脸识别全流程，适用于快速部署和批量处理场景。该系统兼顾轻量性与实用性，优化了边缘设备兼容性（如 RK3588），支持动态阈值调整，可广泛应用于门禁考勤、身份核验、监控识别等场景。

#### 3.1 创建功能包

##### 3.1.1 ROS2 创建功能包

介绍如何创建 ROS2 功能包以及功能包的文件结构。

```bash
# 功能包创建指令
ros2 pkg create example --build-type ament_python --dependencies rclpy

# 功能包结构
├── config
│   └── face_detect.yaml		# 配置文件
├── known_faces					# 人脸数据库，存放的人脸信息包括正脸以及左右侧脸
│   ├── fangpeng				
│   │   ├── fangpeng_front_20251124_151315_238.jpg
│   │   ├── fangpeng_front_20251124_151316_444.jpg
│   │   ├── fangpeng_left_20251124_151333_064.jpg
│   │   ├── fangpeng_left_20251124_151336_064.jpg
│   │   ├── fangpeng_right_20251124_151347_271.jpg
│   │   └── fangpeng_right_20251124_151353_675.jpg
│   ├── liuyifei
│   │   ├── 侧脸1.jpg
│   │   ├── 侧脸.jpg
│   │   ├── d13b-hzrevqa0384194.jpg
│   └── └── ideal_test.jpg
├── model
│   ├── buffalo_sc
│   │   ├── det_500m.onnx
│   │   └── w600k_mbf.onnx
│   └── yolov8n-face-lindevs.onnx
├── package.xml
├── resource
│   └── robot_face_detect
├── robot_face_detect			# 功能文件
│   ├── face_detect_node.py
│   ├── face_detector.py
│   ├── face_recognizer.py
│   ├── __init__.py
│   └── robot_face_detect.launch.py
├── setup.cfg
├── setup.py
└── test
    ├── test_copyright.py
    ├── test_flake8.py
    └── test_pep257.py
```



##### 3.1.2 人像采集存储

`SimplePhotoCollector` 是一款**轻量级人脸照片采集工具**，专为构建人脸识别系统的人脸数据库设计。工具支持通过摄像头手动拍摄不同角度（正脸、左侧脸、右侧脸）的人脸照片，自动按 “人员姓名 + 拍摄角度 + 时间戳” 的规则命名，并按 “基础文件夹 / 人员姓名” 的结构组织存储，输出的照片可直接用于 `FaceRecognitionSystem` 人脸数据库的构建，无需额外整理。

```python
def capture_photos(self, person_name, person_folder):
    """手动拍摄不同角度的人脸照片"""
    photo_angles = [
        ('front', '正脸', '请正对摄像头，保持面部清晰'),
        ('left', '左侧脸', '请缓慢向左转头，显示左侧脸'),
        ('right', '右侧脸', '请缓慢向右转头，显示右侧脸')
    ]

    photos_count = 0
    target_count = 5  # 每个角度拍摄5张

    print(f"\n开始为 {person_name} 拍摄照片...")
    print("操作说明:")
    print("  - 按 's' 键: 拍摄当前画面")
    print("  - 按 'n' 键: 切换到下一个角度") 
    print("  - 按 'q' 键: 退出拍摄")

    for angle, chinese_name, instruction in photo_angles:
        print(f"\n准备拍摄 {chinese_name} 照片...")
        print(f"提示: {instruction}")
        input("按 Enter 键开始拍摄...")

        count = 0

        while count < target_count:
            ret, frame = self.camera.read()
            if not ret:
                print("无法读取摄像头画面")
                break

            # 显示拍摄信息
            height, width = frame.shape[:2]
            info_text = f"{chinese_name}: {count}/{target_count} (按's'拍摄, 按'n'下一角度)"
            cv2.putText(frame, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

            # 显示操作提示
            cv2.putText(frame, "Press 's': Capture  'n': Next  'q': Quit", 
                       (10, height-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            cv2.imshow('Face Photo Collection', frame)

            # 键盘操作 - 使用waitKey(1)确保响应及时
            key = cv2.waitKey(1) & 0xFF
            if key == ord('s'):  # 拍摄照片
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
                filename = f"{person_name}_{angle}_{timestamp}.jpg"
                filepath = os.path.join(person_folder, filename)

                # 保存原始图片
                cv2.imwrite(filepath, frame)
                print(f"已保存: {filename}")
                count += 1
                photos_count += 1

                # 短暂显示保存提示
                save_frame = frame.copy()
                cv2.putText(save_frame, "照片已保存!", (width//2-100, height//2), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.imshow('Face Photo Collection', save_frame)
                cv2.waitKey(300)  # 显示保存提示300毫秒

            elif key == ord('n'):  # 切换到下一个角度
                print(f"切换到下一个角度，当前{chinese_name}已拍摄{count}张")
                break

            elif key == ord('q'):  # 退出拍摄
                print("用户退出拍摄")
                return photos_count

        print(f"{chinese_name}拍摄完成: {count}张照片")

    return photos_count
```

- **核心特性**
  1. **标准化存储结构**：自动按 “基础文件夹→人员姓名文件夹” 组织照片，直接适配人脸数据库要求；
  2. **多角度采集**：支持正脸、左侧脸、右侧脸拍摄，每个角度可采集多张照片，提升后续识别模型的鲁棒性；
  3. **友好交互设计**：提供拍摄指引、进度提示、保存反馈，支持键盘快捷键操作（拍摄、切换角度、退出）；
  4. **自动命名规则**：照片文件名包含 “人员姓名 + 角度 + 时间戳（精确到毫秒）”，避免文件名冲突；
  5. **灵活配置**：支持自定义基础存储文件夹路径，可设置每个角度的拍摄数量；
  6. **资源自动清理**：程序退出时自动释放摄像头资源、关闭显示窗口，避免资源泄露；
  7. **兼容性强**：基于 OpenCV 实现摄像头调用，支持主流 USB 摄像头，跨 Windows、Linux 平台。

- **使用逻辑**
  1. 遍历每个拍摄角度，打印角度说明和操作提示，用户按 Enter 键开始该角度拍摄；
  2. 循环读取摄像头画面，在画面上显示当前角度、拍摄进度、操作指引；
  3. 响应键盘操作：
     - 按 `s` 键：保存当前画面到人员文件夹，打印保存提示，拍摄进度 + 1，短暂显示 “照片已保存” 反馈；
     - 按 `n` 键：中断当前角度拍摄，切换到下一个角度；
     - 按 `q` 键：退出整个拍摄流程，返回已拍摄照片数；
  4. 每个角度拍摄满目标数量（默认 5 张）后，自动切换到下一个角度；
  5. 所有角度拍摄完成后，返回总拍摄数。



##### 3.1.3 特征提取

**ArcFaceONNXExtractor** 与 **FaceRecognitionSystem** 是专为 **ROS2（机器人操作系统 2）** 优化的轻量级人脸识别解决方案。核心定位是在 ROS2 环境中实现端到端的人脸检测、特征提取与身份识别：

- `ArcFaceONNXExtractor`：基于 ONNX Runtime 直接加载 ArcFace 模型，提供高效的人脸特征提取（无需依赖 insightface 框架，轻量化适配 ROS2 嵌入式场景）；
- `FaceRecognitionSystem`：整合自定义 YOLOv8FaceDetector 人脸检测、ArcFace 特征提取、人脸数据库管理，支持 ROS2 图像话题的实时处理，输出标准化识别结果（边界框、身份、相似度），可直接适配 ROS2 消息格式发布。

- ArcFace ONNX 模型必须提供符合以下要求的 ONNX 模型：区别 YOLOv8-face 模型输入要求640x640
  - 输入：`112x112x3`（RGB 格式，归一化到 [-1, 1]）；
  - 输出：固定维度的特征向量（如 512 维）；
  - 模型输入输出名称需与 `onnxruntime` 自动解析兼容。

```python
def extract(self, face_img: np.ndarray) -> Optional[np.ndarray]:
    """提取特征向量"""
    try:
        # 1. 确保是uint8类型
        if face_img.dtype != np.uint8:
            face_img = (face_img * 255).clip(0, 255).astype(np.uint8)

        # 2. 确保是3通道
        if len(face_img.shape) == 2:
            face_img = cv2.cvtColor(face_img, cv2.COLOR_GRAY2BGR)

        # 3. BGR转RGB
        face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)

        # 4. 缩放到112x112
        face_112 = cv2.resize(face_rgb, (112, 112), interpolation=cv2.INTER_LINEAR)

        # 5. 预处理 (归一化到[-1, 1])
        input_tensor = face_112.astype(np.float32)
        input_tensor = (input_tensor - 127.5) / 127.5
        input_tensor = np.transpose(input_tensor, (2, 0, 1))
        input_tensor = np.expand_dims(input_tensor, 0)

        # 6. ONNX推理
        feature = self.session.run(None, {self.input_name: input_tensor})[0][0]

        # 7. L2归一化
        norm = np.linalg.norm(feature)
        if norm > 0:
            return feature / norm

        return None

    except Exception as e:
        return None
```



##### 3.1.4 检测代码

`FaceDetectNode` 是一款**完全适配 ROS2 生态的端到端人脸检测与识别节点**，基于之前的 `YOLOv8FaceDetector` 和 `FaceRecognitionSystem` 核心能力，专为机器人、智能终端等 ROS2 场景设计。节点支持两种输入模式（本地摄像头 / ROS2 图像话题），自动查找模型与数据库路径（适配开发 / 安装双环境），并以 ROS2 标准消息格式发布检测结果（可视化图像、检测框坐标、人员身份信息），可直接集成到 ROS2 机器人系统中，用于身份核验、人员跟踪、智能交互等场景。

- 核心特性
  1. **ROS2 深度适配**：严格遵循 ROS2 节点开发规范，输入输出均为 ROS2 标准消息（`sensor_msgs/Image`、`std_msgs/String`、`std_msgs/Float32MultiArray`），无缝融入 ROS2 生态；
  2. **智能路径查找**：支持 `auto_find_paths` 自动查找模型（YOLO/ArcFace）和人脸数据库路径，适配 ROS2 开发环境（源码编译）与安装环境（`colcon build` 后），无需手动配置复杂路径；
  3. **双输入模式**：可选择本地 USB 摄像头（自动多索引尝试）或订阅 ROS2 图像话题（如 `/camera/image_raw`），灵活适配不同硬件场景；
  4. 多维度结果发布：同时发布 3 类核心数据，满足不同下游节点需求：
     - 可视化结果图（带检测框 + 身份标签）；
     - 结构化检测框数据（坐标、置信度、相似度）；
     - 详细身份信息（JSON 格式，含时间戳、人员姓名、边界框）；
  5. **参数化配置**：所有核心参数（阈值、话题名称、路径、输入模式）支持 ROS2 参数声明与动态配置，无需修改代码即可适配不同场景；
  6. 鲁棒性优化：
     - 摄像头多索引自动尝试（避免单索引失败）；
     - 检测结果发布时的安全校验（非有限数过滤、默认值填充）；
     - 完善的错误日志与异常捕获（图像转换、模型加载、帧处理全流程）；
  7. **友好的部署支持**：提供模型下载指引、路径配置建议、故障排查日志，降低 ROS2 新手的部署难度。

```python
def publish_detection_results(self, detections: List[Dict[str, Any]]):
    """发布检测结果 - 修复版本"""
    try:
        self.get_logger().info(f"准备发布 {len(detections)} 个检测结果")

        detection_array = Float32MultiArray()

        # 设置布局信息
        layout = MultiArrayLayout()
        layout.dim.append(MultiArrayDimension())
        layout.dim[0].label = "detections"
        layout.dim[0].size = len(detections)
        layout.dim[0].stride = 6  # 每个检测有6个值

        detection_array.layout = layout

        # 安全地填充数据
        data = []
        for i, det in enumerate(detections):
            try:
                x1, y1, x2, y2 = det['bbox']
                confidence = det['confidence']
                similarity = det['similarity']

                # 验证并转换每个值
                values_to_add = []
                for j, value in enumerate([x1, y1, x2, y2, confidence, similarity]):
                    try:
                        float_val = float(value)
                        # 检查是否为有限数
                        if not np.isfinite(float_val):
                            self.get_logger().warning(f"检测 {i} 的值 {j} 为非有限数: {value}")
                            float_val = 0.0
                        values_to_add.append(float_val)
                    except (ValueError, TypeError) as e:
                        self.get_logger().warning(f"检测 {i} 的值 {j} 转换失败: {value}, 错误: {e}")
                        values_to_add.append(0.0)

                data.extend(values_to_add)
                self.get_logger().info(f"添加检测 {i}: {values_to_add}")

            except Exception as e:
                self.get_logger().error(f"处理检测 {i} 时出错: {e}")
                # 添加默认值以避免数据结构不一致
                data.extend([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])

        detection_array.data = data
        self.detections_pub.publish(detection_array)
        self.get_logger().info(f"成功发布检测结果，数据长度: {len(data)}")

    except Exception as e:
        self.get_logger().error(f"发布检测结果失败: {e}")
        import traceback
        self.get_logger().error(f"详细错误: {traceback.format_exc()}")
```

​	

#### 3.2 集成模型测试检测效果

##### 3.2.1 编译功能包

在项目文件夹下 /src  同级目录下运行指令编译功能包。

```bash
# 下载 onnxruntime 
pip install onnxruntime==1.23.2

# 编译功能包
colcon build
```



##### 3.2.2 启动功能包

在项目文件夹下 /src  同级目录下运行指令启动功能包。结构如下图所示：

```
ros2 run robot_face_detect face_detect_node
```

![运行结果](/media/yls/1T硬盘4/picture/result.jpg)



## 四、后续升级方向

目前测试添加图片至人脸数据库并启动功能包发现连续检测存在相似度变化大的问题，这会导致设置的阈值不能太高否则会频繁误测，此外当被检测人做出表情也会影响到识别效果。考虑了几种升级方案具体如下所示：

#### 4.1 密集检测造成输出频繁

##### 4.1.1 一段时间内测试一次

想法是在10秒内进行连续检测但是只选择相似度最高的与判断阈值进行比较然后将接测结果通过话题发布出来。



##### 4.1.2 设置远近两个阈值

设置两个阈值，远近各一个先判断距离远近再进行人脸识别。



##### 4.1.3 表情问题

录入的人脸图片均没有表情，但是日常使用人会有表情这时候模型的检测效果下降明显但是只在巡家模式下使用问题不大。
