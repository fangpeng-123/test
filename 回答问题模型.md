# Ubuntu 22.04 服务器 GPU 环境配置指南

## RTX 4090D 显卡驱动 + vLLM 框架 + Qwen3-8B 模型部署

---

## 目录

1. [环境准备与目标](#第一章-环境准备与目标)
2. [NVIDIA 显卡驱动安装](#第二章-nvidia-显卡驱动安装)
3. [CUDA 工具包安装](#第三章-cuda-工具包安装)
4. [vLLM 框架部署](#第四章-vllm-框架部署)
5. [Qwen3-8B 模型运行](#第五章-qwen3-8b-模型运行)
6. [问题排查与优化](#第六章-问题排查与优化)

---

## 第一章：环境准备与目标

### 1.1 配置目标

本指南将帮助您在搭载 **NVIDIA RTX 4090D** 显卡的 **Ubuntu 22.04** 虚拟机中，完成以下配置任务：

- ✅ 安装并配置 NVIDIA 显卡驱动（推荐版本 550+）
- ✅ 安装 CUDA 工具包（版本 12.x）
- ✅ 部署 vLLM 高性能推理框架
- ✅ 运行 Qwen3-8B 大语言模型
- ✅ 启动 OpenAI 兼容的 API 服务

### 1.2 硬件环境要求

| 组件 | 规格要求 | 说明 |
|------|---------|------|
| GPU | NVIDIA RTX 4090D | 24GB 显存，支持 CUDA 12.x |
| 系统 | Ubuntu 22.04 LTS | 推荐使用 64 位版本 |
| 内存 | ≥ 32GB RAM | 建议 64GB 以获得更好的性能 |
| 存储 | ≥ 100GB 可用空间 | 模型文件约 15GB，建议预留更多空间 |
| CPU | ≥ 8 核心 | 建议 16 核心以上 |

### 1.3 软件栈组成

```
┌─────────────────────────────────────────────────────────────┐
│                    应用层                                    │
│              Qwen3-8B 模型服务                               │
├─────────────────────────────────────────────────────────────┤
│                    推理框架                                  │
│                     vLLM                                    │
├─────────────────────────────────────────────────────────────┤
│                  深度学习框架                                │
│                  PyTorch                                    │
├─────────────────────────────────────────────────────────────┤
│                   GPU 计算平台                               │
│              CUDA Toolkit 12.x                              │
├─────────────────────────────────────────────────────────────┤
│                  硬件驱动                                    │
│            NVIDIA Driver 550+                               │
├─────────────────────────────────────────────────────────────┤
│                  操作系统                                    │
│              Ubuntu 22.04                                   │
└─────────────────────────────────────────────────────────────┘
```

### 1.4 预期成果

完成本指南后，您将能够：

- 在虚拟机中成功识别并使用 RTX 4090D 显卡
- 运行基于 vLLM 的高性能模型推理服务
- 通过 RESTful API 调用 Qwen3-8B 模型
- 构建基于大模型的应用程序

---

## 第二章：NVIDIA 显卡驱动安装

### 2.1 系统准备

#### 步骤 1：更新系统软件包

```bash
# 更新软件包列表
sudo apt update

# 升级所有软件包到最新版本
sudo apt upgrade -y

# 安装必要的编译工具和依赖
sudo apt install -y build-essential dkms linux-headers-$(uname -r)
```

**说明**：
- `build-essential`：包含 GCC 编译器、make 等开发工具
- `dkms`：动态内核模块支持，确保驱动在系统内核更新后自动重新编译
- `linux-headers`：内核头文件，编译驱动模块必需

#### 步骤 2：检测显卡型号

```bash
# 检测 NVIDIA 显卡信息
lspci | grep -i nvidia
```

预期输出（类似）：
```
01:00.0 VGA compatible controller: NVIDIA Corporation AD102 [GeForce RTX 4090] (rev a1)
```

```bash
# 查看推荐驱动版本
ubuntu-drivers devices
```

预期输出：
```
== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==
modalias : pci:v000010DEd00002684sv00001458sd0000403Fbc03sc00i00
vendor   : NVIDIA Corporation
model    : AD102 [GeForce RTX 4090]
driver   : nvidia-driver-550 - distro non-free recommended
```

### 2.2 禁用 Nouveau 驱动

Ubuntu 系统默认使用 Nouveau 开源驱动，该驱动与 NVIDIA 官方驱动冲突，必须先禁用。

#### 步骤 1：创建黑名单配置文件

```bash
# 创建 Nouveau 黑名单配置
sudo tee /etc/modprobe.d/blacklist-nouveau.conf << 'EOF'
blacklist nouveau
options nouveau modeset=0
EOF
```

#### 步骤 2：更新内核启动镜像

```bash
# 重新生成 initramfs
sudo update-initramfs -u
```

#### 步骤 3：验证禁用结果

```bash
# 重启系统
sudo reboot

# 重启后检查 Nouveau 是否被禁用（应无输出）
lspci | grep nouveau
```

**⚠️ 注意**：禁用 Nouveau 后，系统可能会暂时无法进入图形界面，这是正常现象。继续完成驱动安装即可恢复。

### 2.3 安装 NVIDIA 驱动

#### 方法：使用 ubuntu-drivers 自动安装（推荐）

```bash
# 自动安装推荐驱动
sudo ubuntu-drivers autoinstall
```

该命令会自动：
- 下载并安装推荐的驱动版本（通常是 nvidia-driver-550）
- 自动处理所有依赖关系
- 配置 DKMS 支持

#### 替代方法：手动指定版本

如果自动安装失败，可以手动安装：

```bash
# 安装特定版本的驱动
sudo apt install -y nvidia-driver-550
```

### 2.4 验证驱动安装

```bash
# 重启系统使驱动生效
sudo reboot

# 验证驱动是否正确加载
nvidia-smi
```

**预期输出**：
```
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.100                Driver Version: 550.100        CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  N/A |
|  0%   35C    P8              15W /  450W |       2MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
```

**关键信息说明**：
- **Driver Version**: 550.100 - 驱动版本号
- **CUDA Version**: 12.4 - 驱动支持的 CUDA 版本
- **GPU Name**: RTX 4090 - 显卡型号
- **Memory-Usage**: 2MiB / 24576MiB - 显存使用情况

如果看到类似输出，说明驱动安装成功！

---

## 第三章：CUDA 工具包安装

### 3.1 添加 NVIDIA 官方仓库

#### 步骤 1：添加 GPG 密钥

```bash
# 下载并安装 NVIDIA GPG 密钥
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
```

#### 步骤 2：添加 CUDA 仓库

```bash
# 添加 CUDA 软件源
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /"

# 更新软件包列表
sudo apt update
```

### 3.2 安装 CUDA 工具包

```bash
# 安装 CUDA 12.x 工具包
sudo apt install -y cuda-toolkit-12-4
```

**安装说明**：
- 安装包大小约 3-4GB，下载时间取决于网络速度
- 工具包包含：CUDA 编译器 (nvcc)、CUDA 库、开发工具、示例代码

### 3.3 配置环境变量

```bash
# 编辑 ~/.bashrc 文件，添加 CUDA 环境变量
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc

# 使环境变量立即生效
source ~/.bashrc
```

**环境变量说明**：
- `PATH`：将 CUDA 可执行文件目录添加到系统路径
- `LD_LIBRARY_PATH`：指定 CUDA 动态链接库搜索路径

### 3.4 验证 CUDA 安装

#### 验证 1：检查 CUDA 编译器

```bash
# 查看 CUDA 编译器版本
nvcc -V
```

预期输出：
```
Cuda compilation tools, release 12.4, V12.4.120
```

#### 验证 2：编译并运行示例程序

```bash
# 进入 CUDA 示例目录
cd /usr/local/cuda/samples/1_Utilities/deviceQuery

# 编译示例程序
sudo make

# 运行测试
./deviceQuery
```

**预期输出最后一行**：
```
Result = PASS
```

如果看到 `Result = PASS`，说明 CUDA 安装完全成功，GPU 计算功能正常！

---

## 第四章：vLLM 框架部署

### 4.1 创建 Python 虚拟环境

#### 步骤 1：安装 Python 和 pip

```bash
# 安装 Python 3.10 和 pip
sudo apt install -y python3.10 python3.10-venv python3-pip
```

#### 步骤 2：创建隔离的虚拟环境

```bash
# 创建名为 vllm-env 的虚拟环境
python3 -m venv vllm-env

# 激活虚拟环境
source vllm-env/bin/activate
```

**提示**：激活后，命令行提示符前会出现 `(vllm-env)` 前缀，表示当前处于虚拟环境中。

### 4.2 安装 PyTorch

```bash
# 在虚拟环境中安装 PyTorch（CUDA 12.1 版本）
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**验证 PyTorch 安装**：

```python
# 验证 PyTorch 是否能识别 GPU
python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')"
```

预期输出：
```
CUDA available: True
GPU: NVIDIA GeForce RTX 4090
```

### 4.3 安装 vLLM

#### 方法 1：使用 pip 安装（推荐）

```bash
# 使用国内镜像源加速安装
pip install vllm -i https://pypi.tuna.tsinghua.edu.cn/simple
```

#### 方法 2：使用 uv 安装（更快）

```bash
# 安装 uv 包管理器
pip install uv

# 使用 uv 安装 vLLM（自动选择合适的后端）
uv pip install vllm --torch-backend=auto
```

### 4.4 验证 vLLM 安装

```bash
# 验证 vLLM 是否成功安装
python -c "import vllm; print('vLLM version:', vllm.__version__)"
```

预期输出（版本号可能不同）：
```
vLLM version: 0.6.1
```

如果无报错，说明 vLLM 安装成功！

---

## 第五章：Qwen3-8B 模型运行

### 5.1 下载模型文件

#### 方法 1：使用 modelscope 下载（推荐国内用户）

```bash
# 安装 modelscope
pip install modelscope

# 下载 Qwen3-8B 模型（约 15GB）
modelscope download --model Qwen/Qwen3-8B --local_dir ./models/Qwen3-8B
```

#### 方法 2：使用 huggingface 下载

```bash
# 安装 transformers 和 huggingface_hub
pip install transformers huggingface_hub

# 下载模型
huggingface-cli download Qwen/Qwen3-8B --local-dir ./models/Qwen3-8B
```

**模型文件结构**：
```
models/Qwen3-8B/
├── config.json                 # 模型配置
├── pytorch_model.bin          # 模型权重（主文件）
├── tokenizer.json             # 分词器
├── tokenizer_config.json      # 分词器配置
└── ...
```

### 5.2 启动模型推理服务

#### 基本启动命令

```bash
# 启动 vLLM 推理服务
python -m vllm.entrypoints.openai.api_server \
    --model ./models/Qwen3-8B \
    --tensor-parallel-size 1 \
    --max-model-len 8192 \
    --port 8000 \
    --host 0.0.0.0
```

**参数说明**：
- `--model`：模型路径或 HuggingFace 模型名称
- `--tensor-parallel-size`：张量并行度（4090D 单卡设为 1）
- `--max-model-len`：最大模型长度（上下文窗口）
- `--port`：服务端口
- `--host`：监听地址（0.0.0.0 允许外部访问）

#### 高级配置（可选）

```bash
# 启用更高性能的配置
vllm serve /home/fp/.cache/modelscope/hub/models/Qwen/Qwen3-8B \
  --served-model-name Qwen3-8B \
  --host 0.0.0.0 \		
  --port 9000 \								# 端口
  --dtype half \
  --gpu-memory-utilization 0.9 \
  --max-model-len 8192 \
  --tensor-parallel-size 1 \
  --api-key "123456" \						# 指定密钥
  --enable-mixed-precision
```

### 5.3 测试 API 服务

#### 测试 1：查看 API 文档

打开浏览器访问：`http://<服务器IP>:8000/docs`

您将看到 Swagger UI 界面，可以测试各种 API 接口。

#### 测试 2：使用 curl 调用

```bash
# 发送测试请求
curl -N   -H "Content-Type: application/json"   -H "Authorization: Bearer 123456"   http://127.0.0.1:9000/v1/chat/completions   -d '{
        "model": "Qwen3-8B",
        "messages": [{"role": "user", "content": "用一句话介绍自己"}],
        "stream": true,
        "max_tokens": 4096
      }'
```

预期返回 JSON 格式的响应，包含模型的回复。

#### 测试 3：使用 Python 调用

```python
import requests

# API 端点
url = "http://localhost:8000/v1/chat/completions"

# 请求数据
payload = {
    "model": "./models/Qwen3-8B",
    "messages": [
        {"role": "user", "content": "请写一首关于春天的诗"}
    ],
    "temperature": 0.8,
    "max_tokens": 256
}

# 发送请求
response = requests.post(url, json=payload)

# 打印结果
print(response.json()["choices"][0]["message"]["content"])
```

---

## 第六章：问题排查与优化

### 6.1 常见问题及解决方案

#### 问题 1：驱动安装失败

**症状**：`nvidia-smi` 报错或显示驱动未加载

**可能原因和解决方案**：

1. **Secure Boot 问题**
   ```bash
   # 进入 BIOS 禁用 Secure Boot
   # 或安装签名驱动
   sudo apt install linux-modules-nvidia-550-generic
   ```

2. **内核版本不匹配**
   ```bash
   # 查看当前内核
   uname -r
   
   # 安装对应内核的驱动模块
   sudo apt install linux-modules-nvidia-550-$(uname -r)
   ```

3. **Nouveau 驱动未完全禁用**
   ```bash
   # 检查 Nouveau 是否仍在运行
   lsmod | grep nouveau
   
   # 如果仍有输出，重新创建黑名单并更新 initramfs
   sudo update-initramfs -u
   sudo reboot
   ```

#### 问题 2：CUDA 版本不匹配

**症状**：PyTorch 无法识别 GPU 或报 CUDA 错误

**解决方案**：

```bash
# 检查驱动支持的 CUDA 版本
nvidia-smi

# 查看 PyTorch 使用的 CUDA 版本
python -c "import torch; print(torch.version.cuda)"

# 如果不匹配，重新安装匹配的 PyTorch 版本
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

#### 问题 3：显存不足（OOM）

**症状**：模型加载时报 CUDA out of memory 错误

**解决方案**：

1. **减少批处理大小**
   ```bash
   # 使用更小的 max-num-batched-tokens
   --max-num-batched-tokens 4096
   ```

2. **降低 GPU 内存利用率**
   ```bash
   # 设置较低的 gpu-memory-utilization
   --gpu-memory-utilization 0.8
   ```

3. **使用量化模型**
   ```bash
   # 下载 8-bit 或 4-bit 量化版本
   modelscope download --model Qwen/Qwen3-8B-Int8
   ```

#### 问题 4：模型加载失败

**症状**：启动服务时提示模型文件缺失或格式错误

**解决方案**：

```bash
# 检查模型文件是否完整
ls -la models/Qwen3-8B/

# 应该包含以下文件：
# - config.json
# - pytorch_model.bin
# - tokenizer.json
# - tokenizer_config.json

# 如果文件不完整，重新下载
modelscope download --model Qwen/Qwen3-8B --local_dir ./models/Qwen3-8B
```

#### 问题 5：API 服务无法访问

**症状**：服务已启动但无法通过 API 访问

**排查步骤**：

```bash
# 1. 检查服务是否正在运行
ps aux | grep vllm

# 2. 检查端口是否监听
netstat -tulpn | grep 8000

# 3. 检查防火墙设置
sudo ufw status

# 4. 从服务器本地测试
curl http://localhost:8000/v1/models
```

### 6.2 性能优化建议

#### 1. GPU 性能优化

```bash
# 启用持久化模式（减少驱动加载开销）
sudo nvidia-smi -pm 1

# 设置 GPU 锁定频率（提高稳定性）
sudo nvidia-smi -lgc 2100,2400

# 启用 MIG（多实例 GPU，如果有多个应用）
sudo nvidia-smi -i 0 -mig 1
```

#### 2. vLLM 性能调优

```bash
# 使用更高的批处理大小提高吞吐量
--max-num-batched-tokens 16384

# 启用前缀缓存加速重复查询
--enable-prefix-caching

# 使用 chunked prefill 减少首 token 延迟
--enable-chunked-prefill
```

#### 3. 系统级优化

```bash
# 禁用交换分区（避免 GPU 内存交换到磁盘）
sudo swapoff -a

# 增加系统文件描述符限制
echo "* soft nofile 65536" | sudo tee -a /etc/security/limits.conf
echo "* hard nofile 65536" | sudo tee -a /etc/security/limits.conf

# 优化 TCP 连接数
echo "net.core.somaxconn = 1024" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

### 6.3 常用调试命令

| 命令 | 用途 |
|------|------|
| `nvidia-smi` | 查看 GPU 状态、显存使用 |
| `nvidia-smi dmon` | 实时监控 GPU 性能 |
| `nvidia-smi top` | 查看 GPU 进程 |
| `nvcc -V` | 查看 CUDA 版本 |
| `python -c "import torch; print(torch.cuda.is_available())"` | 检查 PyTorch CUDA 支持 |
| `python -c "import vllm; print(vllm.__version__)"` | 检查 vLLM 版本 |
| `curl http://localhost:8000/v1/models` | 查看可用模型 |
| `ps aux | grep vllm` | 检查 vLLM 进程 |
| `journalctl -u vllm.service` | 查看服务日志（如果使用 systemd） |

### 6.4 获取更多帮助

如果遇到本文档未涵盖的问题，可以查阅以下资源：

- **vLLM 官方文档**：https://docs.vllm.ai/
- **NVIDIA 驱动文档**：https://docs.nvidia.com/datacenter/
- **PyTorch 安装指南**：https://pytorch.org/get-started/locally/
- **Ubuntu GPU 指南**：https://ubuntu.com/server/docs/nvidia-drivers

