### 整体流程：

- 数据集下载：在[Roboflow Universe](https://universe.roboflow.com)、[Hugging Face](https://huggingface.co/)等网站寻找开源数据集，将下载好的数据集导入并配置好
- 训练模型：针对视觉模型中宠物识别模型训练，可在YOLO V8的预训练模型基础上进行微调
- 评估训练出来的模型：观察模型训练完成后的识别结果，包括是由有框选以及置信度等因素

- 转换模型并测试效果：将pytorch格式文件复制到IP地址137的本机上进行模型转换，即将pt格式模型转化为rknn格式，进行模拟部署和相关后处理，保证模型识别效果
- 模型部署到开发板上并测试：使用命令将模型和相关程序拷贝到开发板上进行最终测试



### 开发细节：

##### 数据集下载及准备：

- 明确需求：对于宠物识别来说需要搞清楚宠物识别的识别对象，具体包括狗和猫两个类别的宠物

- 准备数据：点击整体流程下数据集下载的超链接进入网站寻找合适的数据集压缩包，一般数据集的数量要超过2000张图片但不宜过多（以Roboflow Universe网页端为例数据集左侧Data/Images下显示个数），会造成训练时长增加且效果不一定更好。对于YOLO模型来说希望找到带有标签的数据集，具体的数据集结构如下图所示：

  ```
  ├── test
  │   ├── images
  │   └── labels
  ├── train
  │   ├── images
  │   └── labels
  └── valid
      ├── images
      └── labels
  ```

  - train文件夹：包括所有用于训练的图片数据集，其中images文件夹是图片数据，labels文件夹是注明对应图片是否含有需要识别内容，以及相应识别内容在图片中对应的横纵坐标。
  - test文件夹：大致内容与train文件夹相似，主要作用是验证模型识别能力。

- 数据集配置：解压完数据集后将文件夹置于训练脚本同级目录，方便脚本编写。

- 数据集配置文件：进入D:\ultralytics\ultralytics\cfg文件夹，在datasets文件夹下创建dog and cat detection.yaml，配置内容包括用于训练的数据集位置，需要识别的类型规定，如果使用预训练模型进行训练，需要注意类别顺序，否则会出现识别错误。

  ```python
  # Example usage: yolo train data=coco8.yaml
  # parent
  # ├── ultralytics
  # └── datasets
  #     └── coco8  ← downloads here (1 MB)
  
  # Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
  path: D:\fp_dataset\dog_cat_detection # dataset root dir
  train: train # train images (relative to 'path') 4 images
  val: valid # val images (relative to 'path') 4 images
  test: test # test images (optional)
  
  # Classes
  names:
    0: cat
    1: dog
  ```

  

##### 训练模型及评估：

- 模型训练：使用train.py脚本进行训练

  ```python
  from ultralytics import YOLO
  import os
  os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
  
  if __name__ == '__main__':
  
      # Load a model
      # model = YOLO("yolov8n.yaml").load("yolov8n.pt")  # build a new model from scratch
      # model = YOLO("person_firesmoke_bleed_head/train5/weights/last.pt")  # load a pretrained model (recommended for training
      model = YOLO("yolov8n.pt")  
      # load a pretrained model (recommended for training
      # Use the model
      model.train(project='dog and cat detection', data="dog and cat detection.yaml", epochs=100, imgsz=640, device=0, resume=False, batch=256, optimizer='SGD', lr0=0.01)  
      # train the model
      # metrics = model.val()  # evaluate model performance on the validation set
      results = model(["./images/dog1.jpg", "./images/dog2.jpg",
                       "./images/dog3.jpg", "./images/cat1.jpg",
                       "./images/cat2.jpg", "./images/cat3.jpg"], save=True)  
      # predict on an image
      # path= model.export(format="onnx")  # export the model to ONNX format
  ```

- 脚本参数配置：加载预训练模型，设置模型训练参数

  - model：根据算力和需求设置加载哪个版本的YOLO预训练模型

    ![image-20250616111706449](/home/yls/.config/Typora/typora-user-images/image-20250616111706449.png)

  - 模型训练参数：训练开始时默认设置，训练过程中根据模型识别效果动态调整，主要注意epoch、batch、optimizer、Ir0参数。设计GPU性能，模型拟合问题。

  - 观察模型训练结果：通常需要模型识别的结果图置信度达到0.8及以上，其次查看模型训练损失图是否平滑下降，如下图所示：

    ![损失图](/media/yls/1T硬盘/picture/results.png)

  - 导出模型：从服务器上通过共享文件夹将训练效果的pytorch文件（best.py）传入ip地址为192.168.1.137的本机进行模型转换和部署。



##### 模型转换及测试：

- 模型转换：导入[pytorch模型](/home/yls/YOLOV8-on-RK3588/rk3588/python)，创建用于转换的文件夹，结构如下：

  ```
  ├── image
  │   ├── 猫1.jpeg
  │   └── imagelist.txt
  ├── model
  │   ├── best.onnx
  │   ├── best.pt
  │   └── dog_cat_detection.rknn
  
  ```

  image文件夹中放入用于检测转换完成的模型识别效果，因为模型不能直接读取图片文件，需要创建imagelist.txt写入图片的绝对路径辅助模型识别。

  - pt模型转换为onnx模型：使用[export.py](/home/yls/YOLOV8-on-RK3588/rk3588/python/export.py)脚本转换模型

    ```python
    import cv2
    from ultralytics import YOLO
    
    pt_path = './pet_detect/model/best.pt'
    model = YOLO(pt_path)
    result = model.predict(source='/home/yls/YOLOV8-on-RK3588/rk3588/python/pet_detect/image/猫1.jpeg')
    cv2.imshow('result', result[0].plot())
    cv2.waitKey()
    cv2.destroyAllWindows()
    # model.export(format='onnx')
    model.export(format='onnx', opset=12, imgsz=(640, 640), simplify=True, dynamic=False, int8=False)
    ```

  - onnx模型转换为rknn模型：根据不同平台选择转换脚本（x86、 arm架构），[代码请见](/home/yls/YOLOV8-on-RK3588/rk3588/python)rknn_infer_x86.py、rknn_infer.py

    - arm架构下：export_rknn()函数用于将一个 ONNX 模型转换为 Rockchip NPU 支持的 `.rknn` 模型格式，适用于部署在如 RK3568、RK3588 等芯片上运行。

      1. rknn = RKNN(verbose=True)初始化对象
      2. rknn.config() 函数配置模型参数，[具体参考](/run/user/1000/gvfs/smb-share:server=192.168.1.145,share=icare,user=icare/icare/mashuaishuai/doc)
      3. load_onnx() 加载onnx模型
      4. 另设计图片归一化、nms以及模型知识等请自行查阅资料

    - x86架构下使用init_runtime()函数：用于**初始化 RKNN 模型的运行时环境**，以便后续可以进行推理（inference）。将 `.rknn` 模型加载到 Rockchip NPU（神经网络处理单元）或 CPU 上。在rk3588芯片上通常会优先使用 NPU 加速推理。

      def export_rknn():
          """onnx模型转换为rknn
          """
          rknn = RKNN(verbose=True)

      ```python
      rknn.config(
          # see:ultralytics/yolo/data/utils.py
          mean_values=[[0, 0, 0]],
          std_values=[[255, 255, 255]],
          # TODO:使用下面均值、方差后，效果更差：
          # mean_values=[[123.675, 116.28, 103.53]],  # IMAGENET_MEAN = 0.485, 0.456, 0.406
          # std_values=[[58.395, 57.12, 57.375]],  # IMAGENET_STD = 0.229, 0.224, 0.225
          # quant_img_RGB2BGR=True,
          quantized_algorithm='normal',
          quantized_method='channel',
          # optimization_level=2,
          compress_weight=False,  # 压缩模型的权值，可以减小rknn模型的大小。默认值为False。
          # single_core_mode=True,
          # model_pruning=False,  # 修剪模型以减小模型大小，默认值为False。
          target_platform='rk3588'
      )
      rknn.load_onnx(
          model=ONNX_MODEL,
          # 获取onnx模型中的以下六个输出，可打开https://netron.app查看节点
          outputs=['/model.22/cv2.0/cv2.0.2/Conv_output_0',
                   '/model.22/cv3.0/cv3.0.2/Conv_output_0',
                   '/model.22/cv2.1/cv2.1.2/Conv_output_0',
                   '/model.22/cv3.1/cv3.1.2/Conv_output_0',
                   '/model.22/cv2.2/cv2.2.2/Conv_output_0',
                   '/model.22/cv3.2/cv3.2.2/Conv_output_0'])
      rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET, rknn_batch_size=1)
      rknn.export_rknn(RKNN_MODEL)
      
      # # 精度分析
      # rknn.accuracy_analysis(
      #     inputs=['./zidane.jpg'],
      #     output_dir="./snapshot",
      #     target=None
      # )
      
      rknn.init_runtime()
      return rknn
      ```

- 模型测试：使用opencv库读取测试图片，读取文件高度和宽度，使用letterbox()进行图像预处理，主要在不失真的情况下调整输入图片的大小，框选识别物体显示置信度以及类别。

  ```python
      # 数据准备
      img_path = './image/猫1.jpeg'
      orig_img = cv2.imread(img_path)
      orig_h, orig_w = orig_img.shape[:2]
      resized_img, ratio, dw, dh = letterbox(im=orig_img, new_shape=(model_h, model_w))  # padding resize
      resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)
      input_img = np.expand_dims(resized_img, 0)
  
      # 转换模型
      rknn = export_rknn()
  
      # 推理
      outputs = rknn.inference(inputs=[input_img], data_format="nhwc")
  
      # 后处理
      boxes, scores, classes = post_process(input_data=outputs, ratio=ratio, dw=dw, dh=dh, shape=(orig_h, orig_w))
  
      # 画图
      result_img = draw_boxes(img=orig_img, boxes=boxes, scores=scores, classes=classes)
  
      # 保存结果
      cv2.imwrite('./img_result.jpg', result_img)
      cv2.imshow('result', result_img)
      cv2.waitKey()
      cv2.destroyAllWindows()
  ```



##### 模型部署：

- 创建功能包连接测试：将已实现的功能封装为ros2功能包，在ros2框架下方便通过节点方式启用/关闭检测功能，也方便与中控之间的通信。具体操作见[封装ros2功能包.md](/home/yls/YOLOV8-on-RK3588/文档/封装ros2功能包)，这里梳理大致流程以及指令

  ```python
  # 创建功能包
  ros2 pkg create example --build-type ament_python --dependencies rclpy
  
  # 目录结构
  ├── example
  │   └── __init__.py
  ├── package.xml
  ├── resource
  │   └── example_py
  ├── setup.cfg
  ├── setup.py
  └── test
      ├── test_copyright.py
      ├── test_flake8.py
      └── test_pep257.py
     
  # 功能包的迁移
  scp -r example orangepi@192.168.1.142:/home/orangepi/ros2_ws		# 终端方式
  Vscode 插件SSH远程连接方式
  
  # 编译功能包
  cd ros2_ws
  colcon build
  source install/setup.bash
  
  # 启动具体的功能节点
  ros2 run robot_visual_identity robot_behavior_recognition		# 后面两个参数分别是功能包名以及启动节点名
  ```

  





