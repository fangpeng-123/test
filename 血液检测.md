### 整体流程

本项目旨在构建一个高效、鲁棒的流血检测系统，基于改进后的YOLOv8目标检测模型，使用包含**强负样本** （hard negative samples）的自定义数据集完成模型微调。通过对血液特征的精细标注与数据增强，提升模型在复杂背景下的识别能力。

- 流程图

  ![image-20250715112031758](/home/yls/.config/Typora/typora-user-images/image-20250715112031758.png)

### 数据集准备

- 数据集搜索网站roboflow、huggingface、kaggle寻找数据集。如果需要优化模型可以考虑使用新数据集在模型基础上继续训练。
- 血液检测模型鲁棒性不高，需要尽可能考虑生活中可能会被误测的物体，解决办法是构建强负样本数据集同时增设更多的真实场景下的正样本提高模型识别准确度。可以参考[官方数据收集标注指南](https://docs.ultralytics.com/zh/guides/data-collection-and-annotation/#introduction)
  - 血液样本：项目数据集打包在145服务器 fangpeng/datasets/blood_detect.rar ，包括了训练数据集以及验证集。
  - 红色干扰物：收集5-10类常见红色物体（玫瑰花/红苹果/红衣/红砖/红酒等）
- 数据标注：使用标注工具 LabelImg（检测） / VIA（分割）对高质量图片进行标注整合成新的数据集供模型训练。



### 模型训练

##### 创建虚拟环境：

- 训练环境：搭建pytorch框架下YOLOv8模型训练环境

```shell
conda create -n env python=3.10 
```

- 开发板 RK3588 环境

```shell
# 安装必要依赖
sudo apt-get install libopencv-dev python3-opencv
pip install tflite-runtime numpy
```

- 环境部署：

  - windows环境克隆YOLO源码，安装CUDA 支持的 PyTorch（`+cu118`）版本

  ```shell
  # 克隆源码
  git clone https://github.com/ultralytics/ultralytics
  
  # 安装 GPU 支持版本
  pip install torch --index-url https://download.pytorch.org/whl/cu118
  pip install torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple
  # 或者直接运行以下命令
  pip install ultralytics torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118 
  
  # 验证是否识别 GPU
  python -c "import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.version.cuda)"
  
  # 检查有没有环境冲突
  pip check
  ```

##### 模型训练：

- 新建配置文件：设计多个数据集共同训练以及多卡联合训练

  ``` shell
  # blood.yaml配置示例
  train: ./dataset/train/images
  val: ./dataset/val/images
  nc: 6  # 类别数（血液+5类干扰物）
  names: ['blood', 'red_rose', 'red_cloth', 'red_apple', 'red_sign', 'red_liquid']
  ```

- 命令行设置多卡训练参数：

  - `--nproc_per_node` 指定要使用的 GPU 数量。在上面的例子中，是 2 个。
  - `--batch` 是总批量大小。它将平均分配给每个GPU 。在上面的例子中，每个GPU 是 128/2=64 。
  - --weights 是加载的预权重。使用官方提供的预训练权重针对已训练检测对象的模型微调效果很好。
  - --device 是设置参与训练的 GPU 序号，多卡并行训练解决的单卡显存不足以及训练缓慢的问题
  - --worker workers是指数据装载时 CPU 所使用的线程数，默认为8，但是按照默认的设置来训练往往会导致我们的CPU爆内存，会导致其他进程进行关闭（例如浏览器）

  ```shell
  # 指定 GPU 训练
  python -m torch.distributed.run --nproc_per_node 2 train.py --batch 128 --data blood.yaml --weights yolov8.pt --device 0,1 --worker 8 
  ```

- 设置必要超参数：指定配置文件依旧模型训练产生的中间文件以及模型存储位置，否则默认使用defalt.yaml中的参数通常只需要修改下面的参数。

  ```shell
  model.train(
      project='Blood',
      data="blood.yaml",
      epochs=300,
      imgsz=640,
      device=[0,1],           # 多卡
      resume=False,
      batch=128,               # 适当增大,显存溢出错误就调小
      optimizer='SGD',
      lr0=0.0025,
      lrf=0.0025,
      workers=16,             # 加大workers
      cache=True              # 开启缓存
  )
  ```

##### 模型存储：

查看模型训练结果中损失是否平滑下降，精度是否达到0.8以上。对模型进行验证测试排除欠拟合以及过拟合情况发生。最后根据模型识别情况即是否能进行有效识别以及置信度来判断模型是否达标。

- model参数：加载训练好的检测模型，一般选择权重文件夹中 weight/best.pt 。
- source参数：指向验证数据集，尽可能所有可能造成误测的图片和需要检测的正样本。

```shell
# 验证模型检测效果    
yolo predict model=path/to/model.pt source=path/to/valid
# 转换模型格式 pt2onnx
yolo export model=path/to/model.pt format=onnx
```



### 模型转化

创建模型转换文件夹：结构如下

```shell
# 模型转换文件结构
├── convert.py					# YOLO11转换脚本
├── images
│   ├── blood.jpg
│   └── imagelist.txt
├── model
│   ├── best.onnx
│   ├── best.pt
│   └── blood_detect.rknn
├── rknn_infer.py				# x86架构下YOLOv8转换脚本
├── rknn_infer_x86.py			# arm架构下YOLOv8转换脚本，用于将模型转化成可以在开发板环境下运行的模型
└── test.py
```

##### pt格式转onnx格式：

- iamge文件夹：创建模型转化需要的验证图片，注意需要将图片路径写入 imagelist.txt 内容如下

  ```shell
  # 切换环境在rknn160环境下运行
  conda activate rknn160		
  
  /home/yls/YOLOV8-on-RK3588/rk3588/python/blood_detect/images/blood.jpg
  ```

- model文件夹：存放需要转换的模型以及模型转换后存储的位置

- 转换脚本：根据YOLO模型的版本不同平台选择转换脚本

  ```python
  # 转换脚本
  import cv2
  from ultralytics import YOLO
  
  pt_path = '/home/yls/YOLOV8-on-RK3588/rk3588/python/blood_detect/model/best.pt'
  model = YOLO(pt_path)
  result = model.predict(source='/home/yls/YOLOV8-on-RK3588/rk3588/python/blood_detect/images/blood.jpg')
  cv2.imshow('result', result[0].plot())
  cv2.waitKey()
  cv2.destroyAllWindows()
  model.export(format='onnx', opset=12, imgsz=(640, 640), simplify=True, dynamic=False, int8=False)
  ```

##### onnx格式转rknn格式：

设置转换模型以及模型存储位置，设置训练类别。

```shell
# 切换环境
conda activate rknn230
```

```python
def export_rknn():
    """onnx模型转换为rknn
    """
    rknn = RKNN(verbose=True)
    rknn.config(
        mean_values=[[0, 0, 0]],
        std_values=[[255, 255, 255]],
        quantized_algorithm='normal',
        quantized_method='channel',
        compress_weight=False,  # 压缩模型的权值，可以减小rknn模型的大小。默认值为False。
        target_platform='rk3588'
    )
    rknn.load_onnx(
        model=ONNX_MODEL,
        # 获取onnx模型中的以下六个输出，可打开https://netron.app查看节点
        outputs=['/model.22/cv2.0/cv2.0.2/Conv_output_0',
                 '/model.22/cv3.0/cv3.0.2/Conv_output_0',
                 '/model.22/cv2.1/cv2.1.2/Conv_output_0',
                 '/model.22/cv3.1/cv3.1.2/Conv_output_0',
                 '/model.22/cv2.2/cv2.2.2/Conv_output_0',
                 '/model.22/cv3.2/cv3.2.2/Conv_output_0'])
    rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET, rknn_batch_size=1)
    rknn.export_rknn(RKNN_MODEL)
    rknn.init_runtime()
    return rknn
```



### 功能包部署测试

##### 创建功能包：

介绍如何创建功能包以及功能包结构

```shell
# 创建工作目录,如在/home/orangepi下创建工作目录ros2_ws
mkdir /home/orangepi/blood_detect

# blood_detect
cd blood_detect
mkdir src
cd src
ros2 pkg create blood_detect --build-type ament_python --dependencies rclpy

# ROS2功能包结构 
├── blood_detect				
│   └── src
│       └── blood_detect
│           ├── blood_detect
│           │   ├── __init__.py
│           │   ├── robot_blood_detect.py
│           │   ├── utils
│           │   │   ├── blood_detect.py
│           │   │   └── rknn_utils.py
│           │   ├── video_publisher.py
|			|	└── robot_blood_detect.py
│           ├── cfg
│           │   └── rknn_config.yaml
│           ├── model
│           │   └── blood_detect.rknn
│           ├── package.xml
│           ├── resource
│           │   └── blood_detect
│           ├── setup.cfg
│           ├── setup.py
│           └── test
│               ├── test_copyright.py
│               ├── test_flake8.py
│               └── test_pep257.py
```

##### 编写功能包节点文件：

- 模型配置文件：cfg/rknn_config.yaml

  ```shell
  rock_detect_model: '/model/blood_detect.rknn'
  
  # config
  model_w: 640
  model_h: 640
  objectThresh: 0.5
  nmsThresh: 0.65
  
  classes:
    - "流血"
  
  ```

- 节点文件：创建摄像头信息发布者节点节点以及流血检测订阅摄像发布节点，以及识别到流血后发布流血信号节点。

  - 摄像头订阅节点：

    ```python
    class VideoPublisher(Node):
        def __init__(self, cap):
            super().__init__('video_publisher')
            self.publisher_ = self.create_publisher(Image, '/image_raw', 10)
            self.bridge = CvBridge()
            self.cap = cap
            self.frame_rate = 30
            self.timer_period = 1.0 / self.frame_rate
            self.timer = self.create_timer(self.timer_period, self.timer_callback)
    
        def timer_callback(self):
            ret, frame = self.cap.read()
            if not ret:
                self.get_logger().info('End of video file reached.')
                self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # 循环播放
            else:
                msg = self.bridge.cv2_to_imgmsg(frame, "bgr8")
                self.publisher_.publish(msg)
    ```

  - 流血检测订阅发布者节点：

    ```python
    class CameraStreamsSub(Node):
        def __init__(self, name, rock_detect, CLASSES):
            super().__init__(name)
    
            self.CLASSES = CLASSES
            self.rock_detect = rock_detect
            self.msg = String()
            # 创建订阅者
            self.image_subscriber = self.create_subscription(
                Image, "/image_raw", self.image_callback, 10)
            # 创建发布者
            self._publisher = self.create_publisher(String, "blood_detect_publisher", 10)
        def image_callback(self, msg):
            cv_img = bridge.imgmsg_to_cv2(msg, "bgr8")
            boxes, scores, classes = self.rock_detect.rknn_inference(cv_img)
            # print(emotions, scores)
            if boxes is None:
                self.msg.data = '无目标'
                logger.info('未检测到目标')
            else:
                for box, score, cl in zip(boxes, scores, classes):
                    top, left, right, bottom = [int(_b) for _b in box]
                    color = (0, 255, 0)
                    # Draw the bounding box on the image
                    cv2.rectangle(cv_img, (top, left), (right, bottom), color, 2)
                    # Create the label text with class name and score
                    label = f'{self.CLASSES[cl]}: {score:.2f}'
                    # Calculate the dimensions of the label text
                    (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX,0.5, 1)
                    # Calculate the position of the label text
                    label_x = top
                    label_y = left - 10 if left - 10 > label_height else left + 10
                    # Draw a filled rectangle as the background for the label text
                    cv2.rectangle(cv_img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height),color, cv2.FILLED)
                    # Draw the label text on the image
                    cv2.putText(cv_img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX,
                                0.5, (0, 0, 0), 1, cv2.LINE_AA)
    
            # 显示图像
            cv2.imshow('Image', cv_img)
            cv2.waitKey(1)  # 等待按键事件，1毫秒
            self._publisher.publish(self.msg)
    
    ```

- 工具函数文件夹 utils ：

  - rknn_utils.py文件：完成图像预处理（缩放、填充），RKNN 模型加载与推理运行，检测结果后处理（NMS、置信度过滤）。

    1. 图像预处理：letterbox 函数对输入图像进行**缩放并填充**，以适配模型输入尺寸，同时保持图像长宽比。

       ```python
       def letterbox(im, new_shape, color=(0, 0, 0)):
           # Resize and pad image while meeting stride-multiple constraints
           shape = im.shape[:2]  # current shape [height, width]
           if isinstance(new_shape, int):
               new_shape = (new_shape, new_shape)
           # Scale ratio (new / old)
           r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
           # Compute padding
           ratio = r  # width, height ratios
           new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
           dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding
           dw /= 2  # divide padding into 2 sides
           dh /= 2
           if shape[::-1] != new_unpad:  # resize
               im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)
           top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
           left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
           im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
           return im, ratio, dw, dh
       ```

    2. 目标框过滤：filter_boxes 函数根据**置信度阈值**过滤掉低置信度的检测框，保留可能的目标。

       ```python
       def filter_boxes(boxes, box_confidences, box_class_probs, objectThresh, keypoints=None):
           box_confidences = box_confidences.reshape(-1)
           candidate, class_num = box_class_probs.shape
           class_max_score = np.max(box_class_probs, axis=-1)
           classes = np.argmax(box_class_probs, axis=-1)
           _class_pos = np.where(class_max_score * box_confidences >= objectThresh)
           scores = (class_max_score * box_confidences)[_class_pos]
           boxes = boxes[_class_pos]
           classes = classes[_class_pos]
           if keypoints is not None:
               keypoints = keypoints[_class_pos]
               return boxes, classes, scores, keypoints
           return boxes, classes, scores
       
       ```

    3. 非极大值抑制（NMS）：nms_boxes 函数去除重叠的检测框，保留最优的一个。

       ```python
       def nms_boxes(boxes, scores, nmsThresh):
           x = boxes[:, 0]
           y = boxes[:, 1]
           w = boxes[:, 2] - boxes[:, 0]
           h = boxes[:, 3] - boxes[:, 1]
           areas = w * h
           order = scores.argsort()[::-1]
           keep = []
           while order.size > 0:
               i = order[0]
               keep.append(i)
               xx1 = np.maximum(x[i], x[order[1:]])
               yy1 = np.maximum(y[i], y[order[1:]])
               xx2 = np.minimum(x[i] + w[i], x[order[1:]] + w[order[1:]])
               yy2 = np.minimum(y[i] + h[i], y[order[1:]] + h[order[1:]])
               w1 = np.maximum(0.0, xx2 - xx1 + 0.00001)
               h1 = np.maximum(0.0, yy2 - yy1 + 0.00001)
               inter = w1 * h1
               ovr = inter / (areas[i] + areas[order[1:]] - inter)
               inds = np.where(ovr <= nmsThresh)[0]
               order = order[inds + 1]
           keep = np.array(keep)
           return keep
       ```

    4. YOLOv8 解码函数：box_process 解析 YOLOv8 模型输出的边界框信息，结合网格坐标和步长，将输出转换为图像坐标。

       ```python
       def box_process(position, model_w, model_h):
           grid_h, grid_w = position.shape[2:4]
           col, row = np.meshgrid(np.arange(0, grid_w), np.arange(0, grid_h))
           col = col.reshape(1, 1, grid_h, grid_w)
           row = row.reshape(1, 1, grid_h, grid_w)
           grid = np.concatenate((col, row), axis=1)
           stride = np.array([model_h//grid_h, model_w//grid_w]).reshape(1, 2, 1, 1)
           position = dfl(position)
           box_xy = grid + 0.5 - position[:, 0:2, :, :]
           box_xy2 = grid + 0.5 + position[:, 2:4, :, :]
           xyxy = np.concatenate((box_xy*stride, box_xy2*stride), axis=1)
           return xyxy
       
       ```

- blood_detect.py文件：基于 **RKNN 模型（瑞芯微 NPU 推理平台）** 实现对输入图像的 **目标检测推理与后处理**，主要用于在嵌入式设备上运行 YOLOv8 或类似结构的目标检测模型。使用 RKNN 模型进行推理，对输出结果进行后处理（解码边界框、过滤低置信度、NMS 非极大值抑制）。最终返回检测到血液边界框、置信度和类别。

```python
class BloodDetect:
    def __init__(self, args, path_share):
        self.objectThresh = args.objectThresh
        self.nmsThresh = args.nmsThresh
        self.model_w = args.model_w
        self.model_h = args.model_h
        self.image_w = self.model_w
        self.image_h = self.model_h
        self.blood_rknn = utils.load_rknn(rknn_path=path_share+args.blood_detect_model, core_id=2)

    def rknn_inference(self, image):
        self.image_h, self.image_w = image.shape[:2]
        input_img, ratio, dw, dh = utils.letterbox(im=image, new_shape=(self.model_h, self.model_w))
        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)
        input_img = np.expand_dims(input_img, 0)

        # 推理
        outputs = self.blood_rknn.inference(inputs=[input_img], data_format="nhwc")

        # 后处理
        boxes, scores, classes = self.post_process(input_data=outputs, ratio=ratio, dw=dw, dh=dh)

        return boxes, scores, classes

    def release_rknn(self):
        self.blood_rknn.release()

    def post_process(self, input_data, ratio, dw, dh):
        boxes, scores, classes_conf = [], [], []
        for i in range(0, 6, 2):
            boxes.append(utils.box_process(input_data[i], self.model_w, self.model_h))
            classes_conf.append(utils.sigmoid(input_data[i + 1]))
            scores.append(np.ones_like(input_data[i + 1][:, :1, :, :], dtype=np.float32))

        def sp_flatten(_in):
            ch = _in.shape[1]
            _in = _in.transpose(0, 2, 3, 1)
            return _in.reshape(-1, ch)

        boxes = [sp_flatten(_v) for _v in boxes]
        classes_conf = [sp_flatten(_v) for _v in classes_conf]
        scores = [sp_flatten(_v) for _v in scores]

        boxes = np.concatenate(boxes)
        classes_conf = np.concatenate(classes_conf)
        scores = np.concatenate(scores)

        # filter according to threshold
        boxes, classes, scores = utils.filter_boxes(boxes, scores, classes_conf, self.objectThresh)

        # filter only head-class
        cls_person = np.where(classes == 0)
        boxes = boxes[cls_person]
        scores = scores[cls_person]
        classes = classes[cls_person]

        # nms
        nboxes, nclasses, nscores = [], [], []
        for c in set(classes):
            inds = np.where(classes == c)
            b = boxes[inds]
            c = classes[inds]
            s = scores[inds]
            keep = utils.nms_boxes(b, s, self.nmsThresh)

            if len(keep) != 0:
                nboxes.append(b[keep])
                nclasses.append(c[keep])
                nscores.append(s[keep])

        if not nclasses and not nscores:
            return None, None, None

        boxes = np.concatenate(nboxes)
        classes = np.concatenate(nclasses)
        scores = np.concatenate(nscores)

        # 目标框转换成在原图上的位置
        boxes -= [dw, dh, dw, dh]
        boxes /= ratio
        boxes = np.clip(boxes, 0, [self.image_w, self.image_h, self.image_w, self.image_h])

        return boxes, scores, classes
```



