# 2025年

## 五月

### 27日：

- **摄像头测试：**orangepi4连接摄像机显示屏测试什么时候会断连
  - 开机不停跳转登陆界面：ctrl + alt + f1进入终端，输入用户用户密码，reboot重启
  - 链接网络，到这个网页测试https://www.onlinemictest.com/zh/webcam-test/
  - orangepi4启用摄像头测试温度异常

### 28日：

- 输入法切换：
- **预训练过程：**数据准备阶段，准备供语言模型学习的庞大数据，数据清洗除去低质量数据，另外可输入特定领域的文本供学习。训练阶段学习人类语言，学习语法句法，拥有理解和生成上下文的能力，获得常识某专业领域知识回答问题
  - 加速预训练方案：Net2Net，StackBert，bert2BERT，LiGO，LEMON、MSG 和 Mango ，采用分布式策略
  - 预训练难点：预训练数据量庞大，需要进行数据过滤，除去低质量数据。
  - 预训练模式：常规预训练，长上下文训练即预训练结束阶段将上下文长度从4,096个数据增加到32,768个数据，使用的是“高质量、长篇数据”。

- **语言模型：**帮助系统理解并预测给定上下文中的下一个词的可能性。语言模型的准确度直接影响到语音识别的性能，尤其是在处理长句子和复杂语境时。通过评估不同词序列的概率，语言模型能够辅助声学模型，从多个可能的识别结果中选择最合理的文本输出。
  - **作用原理：**语言模型通过学习大量的文本数据，统计词的出现频率和词序列的模式，来估计词序列的概率。在语音识别中，声学模型负责将音频信号转换为一系列的音素或词的候选，而语音模型则根据这些候选生成的词序列，计算其概率，从而帮助识别系统做出最终的文本输出决策。

- **前向传播反响传播：**由输出层（layer）经隐藏层到输出层，前向传播是神经网络通过逐层线性变换与非线性激活，从输入层传递数据至输出层，以生成预测值的过程。反响传播输出层到隐藏层到输出层，反向传播通过计算损失函数对权重的梯度，并利用链式法则逐层反向传播误差，最后使用优化算法更新网络参数，以最小化损失函数。

![](/media/yls/1T硬盘/picture/整体.png)

- **前向传播：**输入数据通过网络中的权重和偏置进行线性变换，然后通过激活函数进行非线性变换，得到每一层的输出。最终，输出层的输出即为神经网络的预测值。
  - 输入层接收数据：接受来自外部的数据
  - 计算隐藏层输出：数据从输入层传递到隐藏层，隐藏层中的每个神经元都会接收来自上一层神经元的输入，并计算其加权和。加权和通过激活函数（如ReLU、Sigmoid、Tanh等）进行非线性变换，生成该神经元的输出
  - 计算输出层输出：接收来自隐藏层（或直接从输入层，如果网络没有隐藏层）的输入，并计算（weights）最终的输出

![前向传播图示](/media/yls/1T硬盘/picture/前向传播.png)

- **反向传播：**从输出层开始，逐层计算每个神经元的误差项（即损失函数对该层激活值的导数），然后利用这些误差项和前一层的激活值来计算当前层权重的梯度。最后，使用这些梯度，通过优化算法（如梯度下降）更新网络的参数，以减小损失函数的值。
  - 计算损失（损失函数）：需要一个衡量模型预测输出与真实输出之间差异的标准，这个标准就是损失函数。常见的损失函数包括均方误差（MSE）、交叉熵损失（Cross-Entropy Loss）等。
  - 计算梯度：得到损失函数后，我们需要利用链式法则（Chain Rule）将损失函数的值反向传播到网络的每一层，并计算每个权重的梯度。梯度表示了损失函数相对于每个权重的变化率，它指导我们如何调整权重以减小损失函数的值。
  - 更新参数：得到每个权重的梯度后，我们可以使用梯度下降（Gradient Descent）等优化算法来更新网络的权重和偏置。梯度下降算法的基本思想是沿着梯度的反方向更新权重，以减小损失函数的值。

![反向传播](/media/yls/1T硬盘/picture/反向传播.png)

### 29日:

- **N-gram语言模型：**一种基于统计的语言模型，它通过计算N个连续词的条件概率来预测下一个词。N-gram模型的"N"指的是词的序列长度，例如，当N = 1时，模型称之为unigram模型；当年= 2时，模型称之为bigram模型；当N = 3时，模型称之为trigram模型。实际应用中，为了处理未出现在训练集中的词对，可能会使用平滑技术来调整概率估计。

  - 概率计算公式：
    $$
    P(wi​∣wi−1​)=P(wi−1​)/P(wi−1​,wi​)​
    $$

    - *P(wi−1,wi)* 是词对 wi−1和 wi在训练数据中同时出现的概率。

    - *P(wi−1)P(wi−1)* 是词 wi−1在训练数据中出现的概率。

  - 模型概率计算举例：假设我们有如下简单的句子作为训练数据：“I like cats and I love dogs”。

    - 首先，需要将这个句子转换为bigrams，并计算每个bigram和单个词的频率

      |  Bigram   | 频率 | 单个词 | 频率 |
      | :-------: | :--: | :----: | :--: |
      |  I like   |  1   |   I    |  2   |
      | like cats |  1   |  like  |  1   |
      | cats like |  1   |  cats  |  1   |
      |   and I   |  1   |  and   |  1   |
      |  I love   |  1   |  love  |  1   |
      | love dogs |  1   |  dogs  |  1   |

      计算*P(like∣I)*和*P(cats∣like)：*
      $$
      P(like∣I)=C(I)/C(I,like)​=21​=0.5
      $$

      $$
      P(cats∣like)=C(like)/C(like,cats)​=11​=1.0
      $$

- **语言模型平滑技术：**为了解决未见词汇（OOV）和零概率n-gram是自然语言处理中的常见问题。模型在遇到训练集中未出现的词汇或n-gram时，可能会错误地估计其概率，甚至将其概率设定为零。这不仅影响了模型的预测性能，还可能导致无法计算困惑度等指标。 研究人员提出了多种平滑技术，其中包括拉普拉斯平滑（加一平滑）、加-k平滑以及Kneser-Ney平滑等。这些技术通过从高频n-gram中“借用”一些概率质量，分配给那些未见的n-gram，从而避免了零概率问题，并提高了模型的泛化能力。修正计算过程中的概率值，避免某一项概率为0导致整个句子的概率为0。

  - 拉普拉斯平滑：

    - Add-one：强制让所有的n-gram至少出现一次，只需要在分子和分母上分别做加法即可（分母上的V是n-gram句子概率的乘积项的个数，相当于加了1的总个数）。这个方法的弊端是，大部分n-gram都是没有出现过的，很容易为他们分配过多的概率空间。

      ![Add-one](/media/yls/1T硬盘/picture/Add one.png)

    - Add-k：与Add-one类似，不同点在于原本加一改为加小于一的常数KKK，缺点在于这个常数仍然需要人工确定，对于不同的语料库KKK可能不同

      ![Add-k](/media/yls/1T硬盘/picture/Add k.png)

    - Kneser-Ney平滑：通过引入一个调整因子来解决零概率问题。它的基本思想是利用n-gram的上下文信息来估计未见n-gram的概率。具体来说，Kneser-Ney平滑使用两个概率值：

      1. 补充概率（continuation probability）：在给定上下文中下一个词的概率。它通过计算给定上下文的n-gram数量和包含该n-gram的不同上下文数量之比来估计。补充概率提供了一个对未见n-gram的概率估计。
      2. 回退概率（discounted probability）：在给定上下文中下一个词的概率。它通过计算给定上下文的n-1 gram数量和包含该n-1 gram的不同上下文数量之比来估计。回退概率提供了一个对已见n-gram的概率估计。

- **kaldi_native_fbank库：**要功能是从原始音频波形中提取 filter bank 特征（fbank），这些特征是语音识别、语音合成、说话人识别等任务中常用的输入表示。支持 WAV、PCM、FLAC 等常见格式（依赖 torchaudio 或 soundfile）。

  - Filter Bank 特征模拟了人耳听觉特性（梅尔频率），通过以下步骤处理音频：

    - 加窗（Windowing）：将音频切分成帧，并对每帧加窗（如 Hamming 窗）
    - 短时傅里叶变换（STFT）：转换为频域表示

    - 梅尔滤波器组（Mel Filter Banks）：在频谱上应用一组三角滤波器，提取不同频带的能量

    - 取对数能量：对每个滤波器输出取对数，得到log-energy
    - 堆叠帧：有时会进行上下文帧拼接或加入一阶、二阶（delta、detla-delta）以捕捉动态信息
- **scipy库：**Python 中一个非常强大且广泛使用的科学计算库，是构建在 `NumPy` 基础之上的扩展库，专门用于进行数值计算、科学分析和工程问题求解。功能包括线性代数运算、优化算法、插值、积分与微分方程、统计分布于检验、信号处理、图像处理、稀疏矩阵、聚类分析、快速傅里叶变换。

### 30号：

- **梅尔频谱图与梅尔频率倒谱系数处理流程：**

  - 音频信号 -> 预处理 -> 分帧 -> 加窗 -> FFT -> 功率谱 -> 梅尔滤波器组 -> 对数压缩 -> 梅尔频谱图。
- 梅尔频谱图 -> 离散余弦变换 (DCT) -> MFCC。
  - 特征表示：

    - 梅尔频谱图是一个二维矩阵，包含频率和时间维度的频谱信息。

    - MFCC是一个较小的特征向量（通常为每帧12到13个系数），这些系数是从梅尔频谱图中提取并压缩得到的。
- **梅尔频谱图 (Mel Spectrogram)：**梅尔频谱图是将音频信号的频谱表示转换到梅尔频率标度上，并通过一组梅尔滤波器对频谱进行加权平均后得到的结果。具体步骤如下：
    - 音频信号预处理：预加重 (Pre-emphasis)、分帧 (Framing)、加窗 (Windowing)
  - 计算功率谱 (Power Spectrum)：对每一帧信号进行快速傅里叶变换（FFT），计算每一帧的功率谱
    - 应用梅尔滤波器组（Mel Filter Bank）：使用一组三角形滤波器，频率分布在梅尔频率标度上，将功率谱通过这些滤波器，得到每个滤波器的加权平均值
    - 对数压缩 (Log Compression)：对滤波器组输出的值取对数，以模拟人耳对声音强度的非线性感知，梅尔频谱图的最终输出是对数梅尔频谱值的矩阵，代表梅尔滤波器的数量，列代表时间帧
  - **梅尔频率倒谱系数 (MFCC)：**MFCC是基于梅尔频谱图进一步处理得到的一组特征系数。具体步骤如下：
    - 计算梅尔频谱图：按照上述步骤，计算音频信号的梅尔频谱图
    - 离散余弦变换（DCT）：对梅尔频谱图的每一列（即每一时间帧的梅尔频率表示）进行离散余弦变换 (DCT)，这一步的目的是将频谱压缩到更少的系数，并去除相关性，使得特征更加集中
  - 保留低阶系数：通常只保留DCT变换后的前12到13个系数，这些系数包含了主要的频谱信息





## 六月

### 3日：

##### **机器学习：**

机器学习是一种人工智能技术，通过数据学习提升系统性能。其工作流程包括数据收集、预处理、特征工程、模型选择、模型训练、模型评估和应用。特征工程是关键步骤，用于数据预处理和提取有用信息。模型评估涉及分类和回归模型的准确率、RMSE等指标。文章还讨论了监督学习、无监督学习以及模型的过拟合和欠拟合问题。

- **线性回归：**线性回归是一种基于一个或多个输入因素预测连续结果的方法。简单的说，它通过对我们所拥有的数据拟合一条直线来帮助我们找到不同的变量之间的关系。

  - 线性回归是关于合适的一组数据点的最佳直线（回归线）。这条线由以下等式表示：
    $$
    y = mx + b
    $$
    ![线性回归函数](/media/yls/1T硬盘/picture/Linear Regression.png)

    ```python
    """
    代码实现线性回归示例
    """
    import numpy as np
    from sklearn.linear_model import LinearRegression
    import matplotlib.pyplot as plt
    
    np.random.seed(0)
    X = np.random.rand(100,1)
    y = 2 + 3 * X + np.random.rand(100,1)
    
    model = LinearRegression()
    model.fit(X, y)
    
    X_test = np.array([[0], [1]])
    y_pred = model.predict(X_test)
    
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, color = 'b', label = 'Data points')
    plt.plot(X_test, y_pred, color = 'r', label = 'Regression line')
    plt.legend()
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Liner Regression Example\nimage by iCare fp')
    plt.savefig(
        fname="/media/yls/1T硬盘/picture/Linear Regression.png",   # 文件路径 + 名称（自动创建目录需手动处理）
        format="png",              # 图像格式（可省略，由后缀自动识别）
    )
    plt.show()
    
    
    print(f"Intercept: {model.intercept_[0]:.2f}")
    print(f"Coefficient: {model.coef_[0]:.2f}")
    ```

    对于多个自变量，我们使用多元线性回归：
    $$
    y = b0 + b1 * 1 + b2 * 2 + ... + bn * n
    $$
    ![多元线性回归](/media/yls/1T硬盘/picture/polynomial regression.png)
    
    ```python
    """
    多元线性回归函数实现示例
    """
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.pipeline import make_pipeline
    from sklearn.linear_model import Ridge
    import numpy as np
    import matplotlib.pyplot as plt
    
    np.random.seed(0)
    X = np.sort(5 * np.random.rand(80,1), axis = 0)
    y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])
    
    degree = 5
    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha = 1e-3))
    model.fit(X, y)
    
    X_test = np.linspace(0, 5, 100)[:, np.newaxis]
    y_pred = model.predict(X_test)
    
    plt.scatter(X, y, color = 'b', label = 'Data points')
    plt.plot(X_test, y_pred, color = 'r', label = 'Polynomial regrression')
    plt.legend()
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title("Polynomial Regression with Ridge Regularrization")
    plt.savefig('/media/yls/1T硬盘/picture/polynomial regression.png')
    plt.show()
    ```
    
    
  
  - **逻辑回归：**逻辑回归是一种分类算法，它用于预测一组给定一组自变量的二元结果。例如你想根据今天的天气预测明天是否会下雨（是 / 否）。逻辑回归会使用一种称为sigmoid函数的特殊公式，将任何输入转换为0和1之间的概率，表示发生的可能性。如果接近1表示 “ 是 ” 的可能性大则认为会下雨反之不下雨。另外有一种特殊情况当概率达到0.5时，我们从预测 “ 否 ” 切换至 “ 是 ” ，这被称之为决策边界。
  
    ![sigmoid函数](/media/yls/1T硬盘/picture/logistic.png)
  
    ```python
    import numpy as np
    from sklearn.linear_model import LogisticRegression   # Python中的机器学习库，基于Numpy和Scipy构建的开源库，提供了大量用于数据挖掘、数据分析和机器学习任务的工具
    import matplotlib.pyplot as plt
    
    hours_studied = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1) # 使用reshape()函数把一个行向量变成了列向量
    outcome = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])
    
    model = LogisticRegression()
    model.fit(hours_studied, outcome)
    
    predicted_outcome = model.predict(hours_studied)
    predicted_probabilities = model.predict_proba(hours_studied)
    
    plt.figure(figsize=(10, 6))
    plt.scatter(hours_studied, outcome, color = 'b', label = 'Data points')
    # 使用 hours_studied 作为 x 轴，第二列（正类的概率）作为 y 轴
    plt.plot(hours_studied, predicted_probabilities[:, 1], color = 'r', label = 'Logistic line')
    plt.legend()
    plt.xlabel('hours_studied')
    plt.ylabel('probility of passing')
    plt.title('Logistic Regression Example\nimage by iCare fp')
    plt.savefig(
        fname="/media/yls/1T硬盘/picture/Logistic Regression.png",   # 文件路径 + 名称（自动创建目录需手动处理）
        format="png",              # 图像格式（可省略，由后缀自动识别）
    )
    plt.show()
    
    print("Predicted Outcomes:", predicted_outcome)
    print("Predicted Probabilities", predicted_probabilities)
    ```
  
    ![逻辑回归函数示例](/media/yls/1T硬盘/picture/Logistic Regression.png)
  
  - **决策树：**决策树用于分类和回归任务。它通过创建一个模型来工作，该模型基于一系列简单的决策进行预测—例如遵循树上的路径。在这棵树上，每个问题都是一个分支，而最终的答案是一片叶子。该树根据它给出的数据 “ 学习 ” 最好的问题，使其成为决策的强大工具。
  
    ```python
    from sklearn.tree import DecisionTreeClassifier, plot_tree
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    import matplotlib.pyplot as plt
    
    iris = load_iris()
    X, y = iris.data, iris.target
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    model = DecisionTreeClassifier(max_depth=3, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=iris.target_names))
    
    plt.figure(figsize=(20,10))
    plot_tree(model, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True)
    plt.savefig(
        fname="/media/yls/1T硬盘/picture/Decision tree.png",   # 文件路径 + 名称（自动创建目录需手动处理）
        format="png",
    )
    plt.show()
    ```
  
    ![决策树](/media/yls/1T硬盘/picture/Decision tree.png)
  
  - **随即森林：**是一种集成学习方法，通过构建多个决策树并结合它们的结果来帮助我们做出更精确的预测。随即森林不只是依赖一颗决策树，而是创造许多颗树，每棵树都可以查看数据的不同部分。通过对回归任务的结果进行平均或对分类任务进行多数投票，随即森林减少了错误，是预测更加可靠。
  
    - 评估标准：
      1. 分类：准确度、精确度、召回率、F1分数
      2. 回归：均方误差（MSE），R平方
  
    ```python
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, confusion_matrix
    import seaborn as sns
    import matplotlib.pyplot as plt
    import numpy as np
    
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                               n_redundant=5, n_classes=3, random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(10,8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix \nimage by iCare fp')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10,6))
    plt.title("Feature Importances \nimage by iCare fp")
    plt.bar(range(X.shape[1]), importances[indices])
    plt.xticks(range(X.shape[1]), indices)
    plt.tight_layout()
    plt.show()
    ```
  
    ![随机森林](/media/yls/1T硬盘/picture/Random Forest1.png)
  
    ![随即森林](/media/yls/1T硬盘/picture/Random Forest2.png)
  
    模型的准确性：0.77
  
  - **K—最邻近：**K-Nearest Neighbors（KNN）是一种简单而懒惰的学习算法，用于分类和回归。它基于特征空间中k-最邻近的多数类或平均值来预测目标值。
  
    1. 首先，你决定一个数字，K，这是你想要考虑的邻居的数量
    2. 接下来，算法计算你想要预测的项目与数据中所有其他项目之间的距离
    3. 然后它对距离进行排序，并选择K个最近-你的 “ 邻居 ” 
    4. 对于分类任务，它收集最近邻居的类别
    5. 对于回归任务，它对这些邻居的值进行平均以预测一个数字
  
    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    import matplotlib.pyplot as plt
    import numpy as np
    
    # 加载数据集
    iris = load_iris()
    X, y = iris.data, iris.target
    
    # 只取前两个特征进行训练和预测
    X_subset = X[:, [0, 1]]
    
    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.3, random_state=42)
    
    # 创建并训练KNN分类器
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X_train, y_train)
    
    # 测试评估
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=iris.target_names))
    
    # 绘制决策边界函数
    def plot_decision_boundary(X, y, model, ax=None):
        h = .02
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                              np.arange(y_min, y_max, h))
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        if ax is None:
            ax = plt.gca()
        ax.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black', s=25)
        ax.set_xlabel('Sepal length')
        ax.set_ylabel('Sepal width')
        return ax
    
    # 绘图显示
    plt.figure(figsize=(10, 8))
    plot_decision_boundary(X_subset, y, model)
    plt.title('KNN Decision Boundary (k=3) using Sepal Length & Width')
    plt.savefig(
        fname="/media/yls/1T硬盘/picture/Decision tree.png",
        format="png",
    )
    plt.show()
    ```
  
    ![](/media/yls/1T硬盘/picture/Decision tree.png)
  
  - **朴素贝叶斯：**
  
    先验概率：即基于统计的概率，是基于以往历史经验和分析得到的结果，不需要依赖当前发生的条件。
  
    后验概率：则是从条件概率而来，由因推果，是基于当下发生了事件之后计算的概率，依赖于当前发生的条件。
  
    条件概率：记事件A发生的概率为P(A)，事件B发生的概率为P(B)，则在B事件发生的前提下，A事件发生的概率即为条件概率，记为P(A|B)。
    

##### **配置服务器环境：**

icefall 工具包下 wenetSpeech 数据集训练模型运行环境配置

- **Anaconda下载：**Linux环境下命令行下载，也可以在官网下载

  ```shell
  wget https://repo.anaconda.com/archive/Anaconda3-2023.07-1-Linux-x86_64.sh
  # 或者
  curl -O https://repo.anaconda.com/archive/Anaconda3-2023.07-1-Linux-x86_64.sh
  ```

- **Anaconda安装：**打开终端并导航到包含下载脚本的目录，运行以下命令并安装

  ```shell
  bash Anaconda3-2023.07-1-Linux-x86_64.sh
  ```

  - 安装后出现 ” Subject to the terms of this Agreement, Anaconda hereby grants you a non-exclusive, non-transferable license to: “解决

    看到的内容是 Anaconda 安装过程中的 **用户许可协议（EULA）**，你需要按下 **Enter 键** 来逐页阅读，或者输入 **`q`** 跳过阅读至最后一页输入**yes**

- **配置 [icefall](https://github.com/k2-fsa/icefall.git) 环境**

  ```shell
  # 使用conda创建虚拟环境
  conda create -n icefall python=3.11
  conda activate icefall
  
  # 根据自行的环境安装pytorch，下面是简单参考命令：
  # Install Pytorch
  pip install torch==2.6.0 --index-url https://download.pytorch.org/whl/cpu
  
  # Install k2
  pip install k2==1.24.4.dev20250307+cpu.torch2.6.0 -f https://k2-fsa.github.io/k2/cpu-cn.html
  
  # 还有一些库等等
  pip install torchaudio lhotse
  
  # 获取icefall源码
  git clone https://github.com/k2-fsa/icefall.git
  cd egs/librispeech/ASR
  ```

  - 解决服务器不能复制粘贴问题，执行命令行后重启：

    ```powershell
    sudo apt-get update		#更新软件源
    sudo apt-get install open-vm-tools		#安装open-vm-tools
    ```

### 4日：

##### 数据集下载：

**bug：**

- ModuleNotFoundError: No module named 'icefall'Python ，原因找不到 `icefall` 模块。这是因为 `icefall` 是一个本地开发库（不是通过 pip 安装的标准包），需要将它的根目录加入 Python 的模块搜索路径中。

```shell
cd /path/to/icefall

# 设置 PYTHONPATH
export PYTHONPATH=$(pwd):$PYTHONPATH

# 再次运行 prepare.sh 或直接运行 compute_fbank_aishell.py
cd egs/wenetspeech/ASR
./prepare.sh --stage 3
```

- Ubuntu 默认使用开源的 `nouveau` 显卡驱动，它会与 NVIDIA 官方驱动冲突。

  ```shell
  # 检查是否启用nouveau驱动
  lsmod | grep nouveau
  # 禁用nouveau：
  sudo bash -c "echo blacklist nouveau > /etc/modprobe.d/blacklist-nvidia-nouveau.conf"
  sudo bash -c "echo options nouveau modeset=0 >> /etc/modprobe.d/blacklist-nvidia-nouveau.conf"
  sudo update-initramfs -u
  # 重启
  sudo reboot
  # 更新包列表
  sudo apt update
  # 卸载旧驱动（如有）
  sudo apt purge '^nvidia-.*' || true
  sudo apt autoremove
  # 安装推荐驱动（565-server 支持 CUDA 12.1 和 A100）
  sudo apt install nvidia-utils-565-server
  ```

  

##### 深度学习：

深度学习是一种机器学习方法，它模仿人脑神经网络的结构和功能。它通过多层次的神经网络来学习和提取数据的特征，并使用这些特征进行预测和决策。深度学习在计算机视觉、自然语言处理和语音识别等领域取得了很大的成功，它能够处理大规模和复杂的数据，并从中学习到更加准确的模式和规律。

- 四种典型的深度学习算法：

  - **卷积神经网络—CNN**：

    - 人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。下面是人脑进行人脸识别的一个示例：
  
      ![人脑视觉处理](/media/yls/1T硬盘/picture/人脑视觉处理.png.webp)
  
      我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。CNN就是模仿大脑，构造多层的神经网络，较底层的识别初级的图像特征，若干底层特征组成更上一层的特征，最终作出分类。
  
    - CNN的特点：广泛应用于人脸识别、自动驾驶、美图秀秀、安防等领域
  
      1. 能够将大数据量的图片有效的降维成小数据量（并不影响结果）且不会因为降维而影响结果
  
         ![数据降维](/media/yls/1T硬盘/picture/数据降维.png.webp)
  
      2. 能够保留图片的特征，类似人的视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来类似的图像
  
         ![特征提取](/media/yls/1T硬盘/picture/特征提取.png.webp)
  
    - CNN的基本原理：将图像像素转换为矩阵表示，利用卷积核（针对不同特征进行设计）对图像矩阵进行点积运算即特征提取，图像矩阵中子矩阵与卷积核进行点积运算的矩阵称之为感受野，点积得到的值越大，我们认为与卷积核提取的特征越接近。
  
      1. 卷积层 （降低图片特征维度）— 主要作用是提取图片中的局部特征，这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。卷积层的运算过程如下图，用一个卷积核扫完整张图片：
  
         ![卷积—特征提取](/media/yls/1T硬盘/picture/卷积—特征提取.gif)
  
         在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。总的来说卷积层通过卷积核的过滤提取图片的局部特征，跟人类的特征提取类似。
  
      2. 池化层 — 主要作用是把数据降维，可以有效的避免过拟合，池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：
  
         ![池化](/media/yls/1T硬盘/picture/池化.gif)
  
         上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。
  
         1. 全连接层（又称FC层，例如线性投影层）— 根据不同任务输出我们想要的结果：全连接层会把这些特征图**展平（Flatten）**成一个向量，然后通过多层感知机（MLP）的方式，逐步映射到最终的类别输出（例如3个类别的概率）。经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。步骤如下：
  
         2. 展平：经过前面卷积层和池化层后，输出是一个形状为：(通道数 C, 高 H, 宽 W) 或 (H, W, C)（取决于框架使用 channel_first 还是 channel_last）。`7x7x512` 的特征图（即 512 个 7x7 的特征图）——> 25088 维向量。
  
         3. 全连接层其实就是一个标准的神经网络层，形式如下：
         $$
           y=Activation(W⋅x+b)
         $$
           其中：
  
           ​	x：展平后的特征向量（如 25088 维）
  
           ​	W：权重矩阵（比如 4096 x 25088）
  
           ​	b：偏置项
  
           ​	y：输出（比如 4096 维）
  
           ​	Activation：激活函数（如 ReLU）
  
         4. 最终输出：Softmax 分类：最后一层全连接层的输出维度等于你要分类的类别数（如 CIFAR-10 是 10 类），然后再接一个 Softmax 函数，输出每个类别的概率
  
           ```
           [0.1, 0.05, 0.8, ..., 0.05] → 表示最可能是第3类
           ```
  
         ![全连接](/media/yls/1T硬盘/picture/全连接.png.webp)
  
    - CNN的实际应用：
      1. 图片分类、检索
      2. 目标定位检测
      3. 目标分割
      4. 人脸识别
      5. 骨骼识别
  
  - **循环神经网络—RNN：**
  
    RNN是一种有效的处理序列数据的算法。比如：文章内容、语音音频、股票价格走势等，之所以它能处理序列数据，是因为在序列中前面的输入也会影响到后面的输出，相当于有了 “ 记忆功能 ”。但是RNN存在严重的短期记忆问题，长期的数据影响也小。 
  
    - RNN独特价值：卷积神经网络 – CNN和普通的算法大部分都是输入和输出的一一对应，也就是一个输入得到一个输出。不同的输入之间是没有联系的。需要处理「序列数据 – 一串相互依赖的数据流」的场景就需要使用 RNN 来解决了。比如文章里的文字内容、语音中的音频内容和股票市场中的价格走势等。
  
      ![RNN优势](/media/yls/1T硬盘/picture/RNN优势.png.webp)
  
    - 基本原理：神经网络的结构 ——> 输入层 – 隐藏层 – 输出层
  
      RNN 跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。如下图所示：
  
      ![rnn-1](/media/yls/1T硬盘/picture/rnn-1.gif)
  
      假如需要判断用户的说话意图（问天气、问时间、设置闹钟…），用户说了一句“what time is it？”我们需要先对这句话进行分词：按照顺序输入 RNN ，我们先将 “what”作为 RNN 的输入，得到输出「01」,然后，我们按照顺序，将“time”输入到 RNN 网络，得到输出「02」。这个过程我们可以看到，输入 “time” 的时候，前面 “what” 的输出也产生了影响（隐藏层中有一半是黑色的）。以此类推，前面所有的输入都对未来的输出产生了影响，大家可以看到圆形隐藏层中包含了前面所有的颜色。如下图所示：
  
      ![rnn_input](/media/yls/1T硬盘/picture/input.gif)
  
    - LSTM – 长短期记忆网络：由于RNN短期的记忆影响较大（如橙色区域），但是长期的记忆影响就很小（如黑色和绿色区域），这就是 RNN 存在的短期记忆问题。导致无法处理很长的输入序列，训练RNN需要投入很大成本，因此引入LSTM。
  
      ![RNN对比LSTM](/media/yls/1T硬盘/picture/LSTM.png.webp)
  
      LSTM 类似上面的划重点，**他可以保留较长序列数据中的「重要信息」，忽略不重要的信息**。这样就解决了 RNN 短期记忆的问题。
  
      ![画重点](/media/yls/1T硬盘/picture/画重点.png.webp)
  
    - 从 LSTM 到 GRU
  
      Gated Recurrent Unit – GRU 是 LSTM 的一个变体。他保留了 LSTM 划重点，遗忘不重要信息的特点，在long-term 传播的时候也不会被丢失。GRU 主要是在 LSTM 的模型上做了一些简化和调整，在训练数据集比较大的情况下可以节省很多时间。
  
      ![GRU](/media/yls/1T硬盘/picture/gru.png.webp)
  
    - 应用场景：
      1. 文本生成
      2. 语音识别
      3. 机器翻译
      4. 生成图像描述
      5. 视频标记
  
  - **生成对抗网络（GAN）：**
    - **生成器(Generator**)：通过机器生成数据（大部分情况下是图像），目的是“骗过”判别器
    - **判别器(Discriminator**)：判断这张图像是真实的还是机器生成的，目的是找出生成器做的“假数据”
  
  - **深度强化学习 - RL**：
    - 有模型学习（Model-Based）对环境有提前认知，可以提前考虑，但是缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现不好。
    - 免模型（Model-Free）放弃了模型学习，在效率上不如前者，但是这种方式更加容易，也容易在真实场景下调整到很好的状态。所以免模型学习更加受欢迎，得到了更加广泛的开发和测试。
  
  - **深度神经网络（DNN）：**
  
    DNN是一种具有多个隐藏层的神经网络模型，其核心在于其深度，即包含多个隐藏层。这些隐藏层通过非线性变换，使得模型能够捕捉到数据中的复杂关系和模式。DNN通常由输入层、隐藏层和输出层组成，每一层都包含多个神经元，神经元之间通过权重和偏置进行连接。
    
    - 训练过程：DNN的训练过程是一个权重学习和优化的过程。在训练开始时，网络中的权重和偏置是随机初始化的。然后，通过前向传播计算网络的预测输出，并与真实标签进行比较，计算损失函数。接下来，利用反向传播算法计算损失函数关于每个**权重（控制输入信号的强弱，是神经网络学习的主要对象。）**和**偏置（提供输出的平移，使网络可以拟合更复杂的数据分布。）**的梯度，并根据这些梯度更新权重和偏置，以最小化**损失函数**。在DNN的训练中，常用的**优化算法**包括梯度下降（Gradient Descent）及其变种（如批量梯度下降、随机梯度下降、小批量梯度下降）和更先进的优化算法（如Adam、RMSProp、Adagrad等）。
    
      **权重：**权重是连接神经网络不同层之间神经元的系数。每一个神经元的输出会乘以一个权重，决定了该输入对后续神经元影响的大小。反映了输入特征对最终输出的重要性。网络在训练过程中通过反向传播算法不断调整权重，使模型能够更好地拟合数据。
    
      **偏置：**偏置是加在每个神经元上的一个常数项。它不依赖于输入数据，单独存在于每个神经元。偏置用于调整激活函数的输出，使模型具有更强的表示能力。即使所有输入为0，偏置也能让神经元有非零输出。
    
      假设输入为   x ，权重为   w ，偏置为   b ，神经元的输出为   y ：
      $$
       y = f ( w ⋅ x + b ) 
      $$
      其中，  f 是激活函数（如ReLU、Sigmoid等）。
    
      1. 前向传播：DNN从输入层到输出层的信息传递过程。在前向传播过程中，输入数据通过每一层的神经元进行加权求和和激活函数变换，最终生成输出。
      2. 损失函数：用于衡量DNN预测结果与真实标签之间的差距。常见的损失函数包括均方误差（MSE）、交叉熵损失等。通过最小化损失函数，可以优化DNN的权重和偏置项，提高模型的预测性能。
      3. 反向传播：DNN训练过程中的一种算法，用于计算损失函数关于权重和偏置项的梯度。这些梯度随后用于更新权重和偏置项，以最小化损失函数。反向传播算法通过链式法则计算梯度，从输出层开始逐层向前传播，直到更新完所有层的权重和偏置项。
    
    - 应用场景：
      1. 图像识别：图像分类、目标检测、图像分割
      2. 视频分析：用于视频内容的理解，分析以及异常处理
      3. 增强现实：辅助增强现实技术，实现更精准的物体跟踪和场景重建
      4. 机器翻译
      5. 文本生成：自动生成自然语言文本，用于内容创造、语言翻译和聊天机器人
      6. 情感分析：分析文本中的情感倾向
      7. 语音识别和合成：语音识别与合成方面也有广泛应用
    
    
  
  

### 5日：

##### torch、显卡驱动cuda安装：

- **安装步骤：**

  ~~~shell
  ## 2. 创建新环境（以python3.9为例）
  ```bash
  conda create -n icefall python=3.9 -y
  conda activate icefall
  ```
  
  ## 3. 安装PyTorch与CUDA（以torch 2.1.0 + cuda 12.1为例）
  ```bash
  conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=12.1 -c pytorch -c nvidia -y
  ```
  
  ## 4. 安装k2
  ```bash
  pip install k2==1.23.4 -f https://k2-fsa.github.io/k2/html/whl/torch-2.1.0.html
  ```
  
  ## 5. 验证
  ```python
  import torch
  import k2
  print(torch.__version__)
  print(k2.__version__)
  ```
  
  ## 6. 安装icefall及其他依赖
  ```bash
  cd path/to/icefall
  pip install -e .
  ```
  ~~~

- **检测不到k2库：**

  k2是C++扩展模块，需要动态链接libpython3.9.so.1.0。在conda环境下，某些Linux发行版或conda-forge渠道未自动安装这个库，或者LD_LIBRARY_PATH没有包含它。具体步骤如下：

  - 安装libpython，激活你的icefall环境后，运行把`libpython3.9.so.1.0`放到`$CONDA_PREFIX/lib/`里。

  ```shell
  conda activate icefall
  conda install libpython
  ```

  - 如果还不行，手动设置LD_LIBRARY_PATH，可以将这行写入`~/.bashrc`或者每次激活环境后执行

  ```shell
  find $CONDA_PREFIX -name "libpython3.9.so*"   # 检查库是否存在
  # 输出（如/home/fp/anaconda3/envs/icefall/lib/libpython3.9.so.1.0），说明库已安装。
  
  export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
  ```

- **numpy版本兼容问题：**

  ```shell
  pip uninstall numpy
  pip install numpy==1.26.4
  ```

##### 模型训练：

- **prepare脚本运行：**

下载解压数据集（如 Aishell 原始 tar.gz 包）；整理音频和标注，将音频、标注文本等文件转为同一结构（如 wav.scp、text、utt2spk、spk2utt等 Kaldi/Lhotse格式）；清洗数据，去除有问题的音频、无表注的文本、无声文件等；分离训练、验证、测试集整理好各自的文件清单。

- **训练步骤：**

  ```shell
  cd icefall/egs/aishell/ASR
  
  # 1. 添加	libpython3.9.so.1.0路径
  export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
  
  # 2. 计算FBANK特征
  python local/compute_fbank_aishell.py
  
  # 3. 训练
  python zipformer/train.py
  
  # 4. 解码评估
  python zipformer/decode.py --epoch 29 --avg 10 --exp-dir ./exp-zipformer
  ```




### 8日：

##### 模型训练思路（SmolVLA）：

现代视觉模型（如 ViT、DETR、YOLO 系列）已经能高效完成这些任务，并集成到 VLA 架构中进行统一处理。VLA 可以结合 ASR 模块将语音转为文本，再由语言模型理解并生成动作指令（例如：“打电话给妈妈” → 调用通讯录接口）。它可以根据视觉和语言输入做出具体操作，比如：检测到老人摔倒 → 触发报警 → 自动拨打紧急联系人；听到“帮我记下来” → 启动语音识别 → 记录内容到指定应用

![image-20250608092420621](/home/yls/.config/Typora/typora-user-images/image-20250608092420621.png)



### 9日：

##### Mind2Web：

开发和评估能够根据语言指令在任何网站上完成复杂任务的通用智能体。

##### MMIna：

数据集旨在评估智能体在复杂互联网任务中的表现，特别是处理跨多个网站的信息提取和操作。MMInA强调了在现实环境中进行评估的重要性，并提出了新的评估协议，以更全面地衡量智能体在多跳任务中的表现。此外，数据集还引入了记忆增强方法，以提高智能体在处理复杂任务时的性能。

##### （消融实验）ablation study：

同时提出多个思路提升某个模型的时候，为了验证这几个思路分别都是有效的，做的控制变量实验的工作。

比如为了提升baseline的性能， 给它添加了模块A、B，加完后效果提升很多。为了验证A、B两个模块是不是真的对模型有提升效果则需要做ablation study。

1. 在baseline的基础上添加模块A，看效果
2. 在baseline的基础上添加模块B，查看效果
3. 在baseline的基础上同时添加模块A、B查看效果



### 10日：

##### prompt：

- **CoT（Chain-of-Thought）：**是一种引导模型进行多步推理的方法，通过给出带有中间推理步骤的示例，让模型模仿这种“思考链”来解决复杂问题。核心思想是鼓励模型输出类似人类的逐步推理过程，而不是直接给出答案。
- **ToT（Tree of Thoughts）：**不只是单一线性推理，而是将多个可能的推理路径组织成树状结构，通过搜索或评估选择最优路径。核心思想是引入评分机制，对多路径搜索进行打分排序选出最佳路径，并且支持回溯和修改决策。
- **ReAct（Reasoning + Acting）：**允许模型在推理的同时调用工具（API），适用于交互式任务或需要外部信息的场景。核心思想在推理中穿插调用工具的行为，类似人在解决问题时会使用外部资源。

##### 参数与Token关系：

参数决定了模型如何处理token，token是模型在训练和推理阶段操作的对象，模型学到的参数去理解和生成token序列。

- **模型参数：**是神经网络中通过训练学习到的权重（weights），通过大量的数据学习语言规律，调整参数以提高预测准确性，它们决定了模型如何将输入（比如一段文字）映射为输出（比如回答或预测）。
  - 作用：参数越多，模型表达能力越强，能记住和理解的语言模式就越复杂。
  - 如何理解：参数可想象成一个 “ 大脑 ” 的连接方式，每个连接的强度就是参数值。训练过程就是在不断调整这些连接的强度，让模型更准确地理解和生成语言。
  - 训练方法：梯度下降（SGD / Adam）等优化算法；反向传播不断更新参数。

- **Token：**模型处理文本时的基本单位，它可能不是一个完整的单词，可能是词的一部分、标点符号、甚至是子词（subword）。

  - 用途：输入给模型时，将文本转化为token，模型内部再对token进行处理，最后输出新的token。

  - 作用：构成训练样本（输入token + 目标token）；根据输入token预测下一个token；通过交叉熵损失函数衡量预测误差。

    1. 交叉熵损失函数：衡量两个概率分布之间差异的函数，常用于分类任务中。它广泛应用于语言模型、图像识别、语音识别等场景。使用交叉熵损失来衡量预测和真实值的差异

       - CTC Loss（Connectionist Temporal Classification）：适用于输入和输出不一致的情况，可以自动对其语音帧和字符。

       - 交叉熵损失（Cross-Entropy Loss）：当语音识别模型使用类似Transformer的结构时，通常会把输出看作一个序列分类任务，每个时间步输出一个词或子词的概率分布。
       - Sequence-to-Sequence + Attention 中使用的损失函数：输入是一段语音编码（encoder output），输出是一串token（decoder output），一般使用交叉熵损失来逐个预测每个输出token。

##### RAG和sigmoid概率问题：

Sigmoid 输出的是神经网络学到的预测概率，而 RAG 中的 chunk 相关性得分是基于向量相似度的匹配程度。虽然它们都可以表示“相关性”，但一个是模型训练出来的，一个是检索出来的；一个用于分类决策，一个用于信息筛选。两者可以结合使用，提升 RAG 系统的效果。

- sigmoid函数：一种激活函数，将任意实数映射到 [0,1] 区间，常用于二分类任务中表示概率。  
  $$
  \sigma(x) = \frac{1}{1 + e^{-x}}
  $$

  - 在一个文本分类模型中，最后一层输出是 `z = 2.0`；
  - 经过 sigmoid 得到 `p = σ(2.0) ≈ 0.88`；
  - 表示该模型认为这个句子属于类别 A 的概率为 88%；

- RAG匹配子块概率（embedding）：在 RAG 架构中，当用户输入一个 query 时，会先从向量数据库中检索出最相关的知识片段（chunk），然后把这些信息作为上下文传给生成模型。 这里的“相关性概率”其实更像是一个 **相似度得分（similarity score）** ，通常用 **余弦相似度（cosine similarity）**  来衡量。

  - 用户输入：“如何做西红柿炒鸡蛋？” 向量数据库中找到三个最相关的 chunk：

    1. chunk1: “鸡蛋打散后下锅翻炒...” → 相似度 0.92
    2. chunk2: “西红柿切片放入锅中...” → 相似度 0.85
    3. chunk3: “加盐调味...” → 相似度 0.76

    这些分数可以理解为“这些知识与用户问题的相关程度”。 



### 11日：

##### 模型转换中的模型配置问题：

在构建RKNN模型之前，需要先对模型进行通道均值、量化图片RGB2BGR转换、量化类型等的配置，这
些操作可以通过config接口进行配置。

- 模型转换参数：

  - **mean_values：**输入的均值。参数格式是一个列表，列表中包含一个或多个均值子列表，多输入模型对应多个子列表，每个子列表的长度与该输入的通道数一致，例如[[128,128,128]]，表示一个输入的三个通道的值减去128。默认值为None，表示所有的mean值为0。
  - **std_values：**输入的归一化值。参数格式是一个列表，列表中包含一个或多个归一化值子列表，多输入模型对应多个子列表，每个子列表的长度与该输入的通道数一致，例如[[128,128,128]]，表示设置一个输入的三个通道的值减去均值后再除以128。默认值为None，表示所有的std值为1。
  - **quant_img_RGB2BGR：**表示在加载量化图像时是否需要先做RGB2BGR的操作。如果有多个输入，则用列表包含起来，如[True, True, False]。默认值为False。该配置一般用在Caffe的模型上，Caffe模型训练时大多会先对数据集图像进行RGB2BGR转换，此时需将该配置设为True。另外，该配置只对量化图像格式为jpg/png/bmp有效，npy格式读取时会忽略该配置，因此当模型输入为BGR时，npy也需要为BGR格式。该配置仅用于在量化阶段（build接口）读取量化图像或量化精度分析（accuracy_analysis接口），并不会保存在最终的RKNN模型中，因此如果模型的输入为BGR，则在调用toolkit2的inference或C-API的run函数之前，需要保证传入的图像数据也为BGR格式。
  - **quantized_dtype：**量化类型，目前支持的量化类型有w8a8、w4a16、w8a16、w4a8、
    w16a16i和w16a16i_dfp。默认值为w8a8。
    - w8a8：权重为8bit非对称量化精度，激活值为8bit非对称量化精度。（RK2118不支持）
    - w4a16：权重为4bit非对称量化精度，激活值为16bit浮点精度。（仅RK3576支持）
    - w8a16：权重为8bit非对称量化精度，激活值为16bit浮点精度。（仅RK3562支持）
    - w4a8：权重为4bit非对称量化精度，激活值为8bit非对称量化精度。（暂不支持）
    - w16a16i：权重为16bit非对称量化精度，激活值为16bit非对称量化精度。（仅
    RV1103/RV1106支持）
    - w16a16i_dfp：权重为16bit动态定点量化精度，激活值为16bit动态定点量化精度。（仅
    RV1103/RV1106支持）

  - **quantized_algorithm：**计算每一层的量化参数时采用的量化算法，目前支持的量化算法
    有：normal，mmse及kl_divergence。默认值为normal。
    - normal量化算法的特点是速度较快，推荐量化数据量一般为20-100张左右，更多的数据量下精度未必会有进一步提升。
    - mmse量化算法由于采用暴力迭代的方式，速度较慢，但通常会比normal具有更高的精度，推荐量化数据量一般为20-50张左右，用户也可以根据量化时间长短对量化数据量进行适当增减。
    - kl_divergence量化算法所用时间会比normal多一些，但比mmse会少很多，在某些场景下（feature分布不均匀时）可以得到较好的改善效果，推荐量化数据量一般为20-100张左右。
  - **quantized_method：**目前支持layer或者channel。默认值为channel。
    - layer：每层的weight只有一套量化参数；
    - channel：每层的weight的每个通道都有一套量化参数，通常情况下channel会比layer精度更高。

  - **float_dtype：**用于指定非量化情况下的浮点的数据类型，目前支持的数据类型有float16。默认值为float16。
  - **target_platform：**指定RKNN模型是基于哪个目标芯片平台生成的。目前支持 “rv1103”、‘’rv1103b“、“rv1106”、“rv1106b”、“rk2118”、“rk3562”、“rk3566”、“rk3568”、“rk3576”和
    “rk3588”。该参数对大小写不敏感。默认值为None。
  - **custom_string：**添加自定义字符串信息到RKNN模型，可以在runtime时通过query查询到该信息，方便部署时根据不同的RKNN模型做特殊的处理。默认值为None。
  - **remove_weight：**去除conv等权重以生成一个RKNN的从模型，该从模型可以与带完整权重的RKNN模型共享权重以减少内存消耗。默认值为False。
  - **compress_weight：**压缩模型权重，可以减小RKNN模型的大小。默认值为False。
  - **single_core_mode：**是否仅生成单核模型，可以减小RKNN模型的大小和内存消耗。默认值为False。目前仅对RK3588 / RK3576生效。默认值为False。
  - **dynamic_input：**用于根据用户指定的多组输入shape，来模拟动态输入的功能。格式[[input0_shapeA, input1_shapeA, ...], [input0_shapeB, input1_shapeB, ...], ...]。默认值为None，实验性功能。假设原始模型只有一个输入，shape为[1,3,224,224]，或者原始模型的输入shape本身就是动态的，如shape为[1,3,height,width]或[1,3,-1,-1]，但部署的时候，需要该模型支持3种不同的输入shape，如[1,3,224,224], [1,3,192,192]和[1,3,160,160]，此时可以设置dynamic_input=[[[1,3,224,224]], [[1,3,192,192]], [[1,3,160,160]]]，转换成RKNN模型后进行推理时，需传入对应shape的输入数据。

##### funASR语音识别包下载：

- github上拉取开源包：

```shell
git clone https://github.com/alibaba/FunASR.git && cd FunASR
```



### 12日：

##### 连接时序分类器（CTC）：

一种用于序列预测问题的算法，特别适用于输入和输出序列长度不一致的情况。主要用于解决语音识别中的对齐问题。CTC允许模型直接从输入序列预测输出序列，而不需要显式的对齐信息。

- **核心概念：**
  - 序列到序列学习：CTC是一种序列到序列的学习方法，适用于诸如语音识别、手写识别等任务，在这些任务中，输入信号（如音频波形或笔迹）与输出标记（如单词或字符）之间没有固定的对齐关系。
  - 空白标签（Blank Label）：CTC引入了一个特殊的 “ 空白 ” 标签，用来表示无输出或者间隔。这个空白标签帮助模型区分连续重复的字符，并且允许模型条过某些输入步骤，这对于处理变长序列尤为重要。

- **工作原理：**CTC通过定义一个损失函数来训练模型，该损失函数考虑了所有可能的输入序列到输出序列的映射路径，并通过动态规划的计算方式计算出最有路径的概率。具体来说，CTC使用前向-后向算法（类似于隐马尔科夫模型中的算法）来高效的计算每个输出序列的概率分布。
  - 路径可能性计算：给定一个输入序列，CTC会考虑所有可能的标记序列作为潜在输出，并为每种情况分配一个概率值。
  - 合并相同连续标记：由于CTC允许在输出中插入空白标签以及重复标记，因此需要将这些冗余去除以获得最终的输出序列。
  - 最大化正确输出的概率：在训练过程中，目标是最小化真实标签序列的负对数似然，从而调整模型参数使得正确输出的概率最大化。

- **对比CTC与HMM**
  - CTC：提供了一种无需显式对齐的方式来进行序列预测，适合于端到端的学习场景。它简化了训练过程，因为不需要预先定义输入输出之间的精确对应关系。
  - HMM：需要明确的状态空间和状态转移模式，更适合于传统统计方法下的序列分析任务。不过，它要求对输入输出之间有较为严格的先验知识或者预定义的结构。

##### 单步自回归模型：

- 定义：是一种在序列生成任务中不依赖于先前输出的模型结构。具有推理速度快、资源利用率高的优势，但生成质量略逊于自回归模型。它适用于对延迟敏感、对生成质量容忍度较高的场景，并可通过知识蒸馏、迭代优化等方式提升性能。与传统的 自回归模型（Autoregressive Models, AR）  不同，NAR 模型可以在一步之内或并行地生成整个目标序列。**一次前向传播就预测出整个输出序列** ，不需要像传统模型那样一步步生成 token。



### 15日：

- funASR语音识别离线、实时demo测试，开源模型实时测试效果不理想
- 宠物识别文档归纳，简单语音指令归纳



### 16日：

- 宠物识别开发文档书写：梳理从数据集下载到模型训练、模型转换最后模型部署的全流程


- 中控通信代码查看
  - 查看已发布话题命令：ros2 topic list
  - 表情检测和摔倒检测发布者节点名

- 要在 Python 中使用 `pyaudio` 实现 **PCM 16kHz 单声道**  的音频，将录制的音频保存为 `.wav` 文件，用于后续ASR的识别。



### 17日：

##### Docker Engine检查安装：

- 检查是否安装旧版本：

  ```shell
  sudo apt-get remove docker docker-engine docker.io containerd runc
  ```

- 更新现有的包索引并安装一些必要的包：

  ```shell
  sudo apt-get update
  sudo apt-get install ca-certificates curl gnupg
  ```

- 添加Docker的官方GPG密钥：确认下载的Docker安装包的真实性和完整性，针对Docker本身

  ```shell
  sudo install -m 0755 -d /etc/apt/keyrings
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
  sudo chmod a+r /etc/apt/keyrings/docker.gpg
  ```

- 设置仓库：

  ```shell
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  ```

- 安装Docker Engine：

  ```shell
  sudo apt-get update
  sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  ```

- 验证安装：

  ```shell
  sudo docker run hello-world
  ```

- 管理 Docker 作为非 root 用户：避免每次都使用 `sudo`，可以将当前用户添加到 docker 组中

  ```shell
  sudo usermod -aG docker ${USER}		# 添加用户至docker组
  newgrp docker		# 生效
  ```

##### 音频采集模块：

用于用户语音输入指令，保存至本地供模型识别文字

##### 语音识别模块（ASR）

可使用本地部署和云端（依赖网络）

- API：百度智能云语音识别接口调通（"access_token": "24.5b626df05983e6e85b44a30a119dea96.2592000.1752722976.282335-119258328"）

  ![image-20250617110836831](/home/yls/.config/Typora/typora-user-images/image-20250617110836831.png)

##### 大模型理解与意图识别模块：

- 本地部署：脚本下载模型文件并配置相关库。
  - datsets库中data_files.py文件中确实 get_metadata_patterns 函数：
  - MODULE_SUPPORTS_METADATA缺失：



### 18日：

##### 本地部署大模型：

Ubuntu 上使用 Docker 和 Ollama 本地部署低成本大模型，并通过 VS Code 实现 Python 调用

**docker部署Ollama：**

- 拉取并运行Ollama容器：

  ```shell
  # 拉取 Ollama 镜像（支持 ARM 架构的 RK3588 等设备）
  docker pull ollama/ollama
  
  # 运行 Ollama 容器，映射端口并持久化存储模型数据
  docker run -d -p 11434:11434 -v ollama:/root/.ollama ollama/ollama
  ```

- 验证Ollama服务：

  ```shell
  # 检查容器状态
  docker ps | grep ollama
  
  # 测试 Ollama API（返回空响应表示服务正常）
  curl http://localhost:11434/api/tags
  ```

**Ollama拉取大模型：**

- 拉取轻量级大模型：通过 [Ollama 网站](https://ollama.com/library)下载低资源占用的模型，例如`deepseek-r1:1.5b`（4 位量化版，显存需求约 4GB）：

  ```shell
  # 查看 Ollama 容器状态
  docker ps | grep ollama
  
  # 进入容器终端
  docker exec -it <container_id> /bin/bash 		# <container_id>替换为容器id
  
  # 下载模型（如 llama3:8b-q4_0 或 deepseek-r1:8b-q4_0）
  ollama pull deepseek-r1:1.5b
  
  # 查看模型
  ollama list
  
  # 输出类似
  NAME            SIZE    MODIFIED
  llama3:8b-q4_0  4.0GB   1 minute ago
  
  # 关闭本地运行大模型
  docker stop <container_id>
  ```

**使用VS Code 进行Python开发：**

- 安装Python依赖：本地环境中安装ollama-python客户端库：

  ```shell
  pip install ollama
  ```

- 使用VS Code的远程开发功能

  - 安装 Dev  Containers 扩展 ：在 VS Code 中安装 `Dev Containers` 扩展，直接在 Docker 容器内开发调试 

  - 配置开发环境 ：创建 `.devcontainer/devcontainer.json` 文件，定义 Python 环境和依赖安装脚本。

- 创建调用代码：在 VS Code 中创建脚本（如 `ollama_demo.py`），调用 Ollama 的 API

  ```python
  import ollama
  
  # 调用模型生成响应
  response = ollama.generate(model="llama3:8b-q4_0", prompt="你好，请用中文回答：1+1等于几？")
  print(response["response"])
  ```

- 限制 Docker 容器资源：编辑 Docker 运行参数，限制 CPU 和内存使用（避免影响系统稳定性）：

  ```shell
  docker run -d \
    --cpus="2" \  # 限制使用 2 个 CPU 核心
    -m "4g" \    # 限制内存为 4GB
    -p 11434:11434 \
    -v ollama:/root/.ollama \
    ollama/ollama
  ```

##### 天气查看API调用：

- 心知天气API：密钥申请，API调用测试正常，涉及用户ID、密钥封装

- 大模型天气查询接口整合：终端显示错误：
  - 解决API正常调用返回网址可以正常打开且返回内容正确，但是通过API访问数据后段显示时API拒绝（400）：
  
    **问题在于使用所有参数（包括 sig）拼接 query 去做签名，但 sig 还未生成，此时 params 里没有 sig。签名算法要求：只用参与签名的参数，不含 sig。**
  
    而之后你把 sig 加到 params 里，最后 `urlencode(params)` 会把 sig/ts/public_key等全部参数加到URL里。
  
    但**如果参数顺序、参与签名的参数有问题，API会拒绝（400）。**
  
    ```python
    params['public_key'] = public_key
    params.setdefault('ts', str(int(time.time())))
    query = "&".join(f"{key}={value}" for key, value in sorted(params.items())).encode()
    params['sig'] = b64encode(hmac.new(secret_key.encode(), query, hashlib.sha1).digest()).decode()
    url = f"https://api.seniverse.com/v3/weather/now.json?{urlencode(params)}"
    response = urlopen(url)
    ```
  
    - 官方Seniverse签名流程（重点）
      1. 用所有 **参与签名**的参数（不含 sig），按key排序，拼接成查询字符串（如：`a=xxx&b=yyy&public_key=...&ts=...`）
      2. 用 secret_key 以 HMAC-SHA1 方式对上面字符串做签名
      3. base64编码后得到 sig
      4. 最终请求URL的参数是：所有参数+sig
  
    ```python
    params['public_key'] = public_key
    params.setdefault('ts', str(int(time.time())))
    # 用于签名的参数，不包含sig
    sign_params = {k: v for k, v in params.items()}
    query = "&".join(f"{key}={value}" for key, value in sorted(sign_params.items()))
    sig = b64encode(hmac.new(secret_key.encode(), query.encode(), hashlib.sha1).digest()).decode()
    params['sig'] = sig
    url = f"https://api.seniverse.com/v3/weather/now.json?{urlencode(params)}"
    response = urlopen(url)
    ```
  
  - 实际返回中根本没有`status`字段，`weather_data.get('status')` 得到的是 `None`，所以一直走“错误”分支。导致最终请求API服务失败。



### 19日：

##### 语音指令：

- 离线录音脚本、ASR语音识别模块（离线、接口）、本地大模型部署、天气查询接口测试完成

##### 24服务器视觉模型开发环境搭建：

要在Windows平台上使用VS Code运行YOLO模型，并利用Miniconda和Git等工具来部署环境，你可以按照以下步骤操作。这里以配置YOLO v5为例，其他版本的YOLO可能需要做相应调整。

###### **1. 安装必要软件**

- 安装VS Code: 访问VS Code官网下载并安装适合Windows的版本。

  - 下载插件：python

- 安装Git: 访问Git官网下载并安装Git for Windows，这将帮助你从GitHub上克隆YOLO仓库。

  - Windows操作系统下注意要选择添加系统环境路径

- 安装Miniconda: 访问Miniconda官网下载适合Windows的Miniconda安装包，并进行安装。Miniconda是Anaconda的一个轻量级版本，可以帮助你管理Python环境。

  - 添加系统环境路径：

    打开 **控制面板 > 系统 > 高级系统设置 > 环境变量**，在“系统变量”或“用户变量”中找到 `Path` 变量，点击“编辑”，添加以下路径（根据你的安装目录调整）：

    ```
    C:\Users\<你的用户名>\Miniconda3
    C:\Users\<你的用户名>\Miniconda3\Scripts
    C:\Users\<你的用户名>\Miniconda3\Library\bin
    ```

###### **2. 创建并激活Conda环境**

打开命令提示符或Anaconda Prompt，然后执行以下命令创建一个新的Conda环境（例如名为`yolov5-env`），并激活它：

```shell
初始化添加powershell
conda init powershell		# 重新打开VS Code打开终端

创建虚拟环境
conda create --name YOLO python=3.8		# 确保你的环境中已安装了Python 3.8或更高版本
conda activate YOLO
```

###### **3. 克隆YOLO v5仓库**

在你的环境中，通过Git克隆ultralytics仓库：

```shell
git clone https://github.com/ultralytics
```



### 22日：

##### 环境配置需要的包（requirement.txt）：

opencv-python、numpy、psutil、matplotlib、tqdm、request、pandas、yaml、ultralytics

- torch、cuda安装：

  ```shell
  # 安装 CUDA 支持版本（cu118）
  pip install torch --index-url https://download.pytorch.org/whl/cu118
  
  # 安装 torchvision 和 torchaudio（使用清华源加速）
  pip install torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

  - 安装过程中报错：

  ```SHELL
  WARNING: Connection timed out while downloading.
  ERROR: Could not install packages due to an OSError: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'C:\\Users\\fp\\AppData\\Local\\Temp\\pip-unpack-mwm5k1ev\\torch-2.7.1+cu118-cp39-cp39-win_amd64.whl'
  Consider using the `--user` option or check the permissions.
  ```

  报错信息说明在用pip安装torch等包时，出现了文件被占用（WinError 32），通常是由于以下原因：

  - 临时文件夹内残留旧文件（尤其是 `.whl` 文件），导致pip无法覆盖或访问。
    - 进入目录：`C:\Users\fp\AppData\Local\Temp`删除所有临时文件，或更改pip临时目录

  - 杀毒软件或Windows安全中心拦截了pip的写入操作。
    - 临时关闭杀毒软件、Windows Defender等，再尝试安装

  - 权限问题——尤其是在Miniconda环境下，有时权限设置会导致文件锁定。

  - 磁盘或网络问题——下载大文件时网络不稳定导致文件不完整或被多次尝试访问。

    - 用浏览器下载对应的 `.whl` 文件（[PyTorch官网](https://download.pytorch.org/whl/cu118/torch-2.1.2%2Bcu118-cp39-cp39-win_amd64.whl)）。也可以去[官网](https://download.pytorch.org/whl/torch/)寻找需要的版本
    - 把文件放到如 `D:\tmp` 目录。
    - 用如下命令进行本地安装：

    ```shell
    pip install D:\tmp\torch-2.1.2+cu118-cp39-cp39-win_amd64.whl
    ```

##### weight加载失败：

PyTorch 2.4+ 版本中引入的新安全机制 导致的问题。从 PyTorch 2.6 开始，`torch.load(..., weights_only=True)` 成为了默认行为，目的是为了防止加载模型时执行任意代码（如反序列化恶意代码）。PyTorch 新版本限制了这种加载方式，除非你手动将相关类加入白名单或使用 `weights_only=False` 加载。

##### YOLO数据集配置文件默认验证集名为Val

##### torchision版本问题：

```shell
ValueError: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.

torch.cuda.is_available(): False
torch.cuda.device_count(): 0
os.environ['CUDA_VISIBLE_DEVICES']: None
See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.
```

- [官网](https://download.pytorch.org/whl/cu118/torchvision/)下载指定版本的torchvision，在终端中安装。

- 查看显卡适配的torchision：已3070为例

  ```shell
  pip install torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu118
  ```

##### 数据集标签问题：

- 数据集中标签文件需要与代码中数据集种类一致，从0开始

  ```shell
  RuntimeError: CUDA error: device-side assert triggered
  CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
  For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
  ```

  解决办法：

  - 所有标签文件的类别编号必须全是 `0`。

  - 格式必须是5列，且后四列0-1之间小数。

  - 没有多余、损坏、空白行。

  - 用下面脚本批量检查所有标签文件。

    ```python
    import os
    import glob
    
    # 设置你的标签文件夹路径，支持多个子集（如train/labels, val/labels等）
    label_dirs = [
        r"C:\datasets\blood\train\labels",
        r"C:\datasets\blood\val\labels",
        r"C:\datasets\blood\test\labels"
    ]
    
    # 你希望替换成的类别编号
    new_class_id = 0
    
    def process_label_file(file_path, new_class_id):
        lines_out = []
        changed = False
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue  # 跳过空行
                parts = line.strip().split()
                if len(parts) < 5:
                    print(f"标签格式错误: {file_path} 行: {line.strip()}")
                    continue
                # 修改类别编号
                if parts[0] != str(new_class_id):
                    parts[0] = str(new_class_id)
                    changed = True
                lines_out.append(' '.join(parts))
        # 只在需要时覆盖原文件
        if changed:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines_out) + '\n')
            print(f"已修改: {file_path}")
    
    def batch_modify_labels(label_dirs, new_class_id):
        for label_dir in label_dirs:
            if not os.path.isdir(label_dir):
                print(f"目录不存在: {label_dir}")
                continue
            label_files = glob.glob(os.path.join(label_dir, "*.txt"))
            print(f"{label_dir} 共找到 {len(label_files)} 个标签文件")
            for file_path in label_files:
                process_label_file(file_path, new_class_id)
    
    if __name__ == "__main__":
        batch_modify_labels(label_dirs, new_class_id)
    ```




### 23日：

##### 模型训练OpenCV 试图读取图片时内存分配失败：

- 内存不足
- 图片损坏或格式异常，图片过大
- batch_size调小，寻找阈值

##### 使用Dify完成智能体创建：

- 使用docker[启动本地部署大模型](# 18日：)

- 使用docker compose本地部署dify：

  - docker compose工具：Linux中如果已安装 Docker Engine 20.10.13 及以上，`docker compose` 已作为子命令集成。

    ```shell
    # 查看docker版本
    docker version
    ```

  - 克隆Dify代码仓库：

    ```shell
    # 假设当前最新版本为 0.15.3
    git clone https://github.com/langgenius/dify.git --branch 0.15.3
    ```

  - 启动Dify：

    1. 进入 Dify 源代码的 Docker 目录

       ```shell
       cd dify/docker
       ```

    2. 复制环境配置文件

       ```shell
       cp .env.example .env
       ```

    3. 启动 Docker 容器，详细细节见Dify[官方文档](https://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose)

       ```shell
       docker compose up -d
       
       # 运行命令后，你应该会看到类似以下的输出，显示所有容器的状态和端口映射：
       [+] Running 11/11
        ✔ Network docker_ssrf_proxy_network  Created                                   
        ✔ Network docker_default             Created                                   
        ✔ Container docker-redis-1           Started                                   
        ✔ Container docker-ssrf_proxy-1      Started                                   
        ✔ Container docker-sandbox-1         Started                                   
        ✔ Container docker-web-1             Started                                   
        ✔ Container docker-weaviate-1        Started                                   
        ✔ Container docker-db-1              Started                                   
        ✔ Container docker-api-1             Started                                   
        ✔ Container docker-worker-1          Started                                  
        ✔ Container docker-nginx-1           Started     
        
        # 检查所有容器是不是都正常运行
        docker compose ps
       ```

  - 更新Dify：进入 dify 源代码的 docker 目录，按顺序执行以下命令：

    ```shell
    cd dify/docker
    docker compose down
    git pull origin main
    docker compose pull
    docker compose up -d
    ```

  - 访问Dify：进入下列的地址，设置帐号密码登陆就可以开始愉快的模型应用创作之旅了。

    ```shell
    # 本地环境
    http://localhost/install
    ```

- Dify从右上头像—>设置—>模型供应商—>安装模型供应商设置模型，例如使用ollama拉取deepseek-r1:1.5b

![image-20250623161146787](/home/yls/.config/Typora/typora-user-images/image-20250623161146787.png)



### 24日：

##### Dify创建agent如何添加tool问题

不要在tool那一栏下的marketplace安装，选择在右上角插件处搜索安装。

##### 模型工具适配问题

- deepseek-r1-1.5b模型不支持调用外部接口或工具，需要选择其他模型进行测试

- 如duckduckgo一样的浏览器工具存在抓取网页版结果可能会遇到速率限制问题，导致Dify Agent 已经正确调用了 DuckDuckGo tool，但DuckDuckGo接口返回了限流（RateLimit），最终无法获得结果。

- 网络工具：

  - 本地部署SearXNG：

    1. github拉取代码，使用Dify部署在本地。

    2. SearXNG部署完成但是不能使用报错：

       ```
       Forbidden
       
       You don't have the permission to access the requested resource. It is either read-protected or not readable by the server.
       ```




### 25日：

##### 解决本地部署SearXNG403错误：

需要修改`secret_key`，不允许为默认值，调整了每秒每分允许的请求数，避免在运行时出错，以及search的formats允许返回json格式（一定要加，否则报403错误）。

```yaml
# see https://docs.searxng.org/admin/settings/settings.html#settings-use-default-settings
default_doi_resolver: 'doi.org'		# 可以解决访问500错误
use_default_settings: true
server:
  # base_url is defined in the SEARXNG_BASE_URL environment variable, see .env and docker-compose.yml
  secret_key: "R+PzvVHeg97zDPWbGZKK4RfeVu+ZjZQPhkqsqysoUYU="

  limiter: false  # enable this when running the instance for a public usage on the internet
  image_proxy: true
  api:
    enabled: true
ui:
  static_use_hash: true
redis:
  url: redis://redis:6379/0
search:
  safe_search: 0
  autocomplete: ""
  default_lang: ""
  formats:
    - html
    - json
    - csv
    - rss
ratelimit:
    enabled: true
    # 调整每秒允许的请求数
    per_second: 5
    # 调整每分钟允许的请求数
    per_minute: 60

```

##### 容器端口冲突：

使用docker compose启动Dify容器和SearXNG容器发现有端口冲突，Dify中nginx容器端口为80与SearXNG中caddy容器冲突。

- 解决方案：不修改nginx端口号使用默认80端口，修改Caddy端口号修改结果如下

```yaml
# 修改Caddy配制文件Caddyfile
reverse_proxy localhost:8888 {		# 将端口号修改为8888
	# https://github.com/searx/searx-docker/issues/24
	header_up Connection "close"
}
# 修改SearXNG配置文件settings.yml
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    networks:
      - searxng
    ports:
      - "127.0.0.1:8888:8080"
```



### 26日：

##### 修改searxng配置文件

在dify中配置searxng相关内容，包括settings.yml、limitor.toml、uwsgi.ini使两个容器可以互不干扰的打开

##### 搜索引擎连接问题

dify不能连接本地部署的搜索引擎，但是searxng是可以正常使用的



### 29日：

##### 验证GPU使用

- 重新下载CUDA启动，使用官方的测试文件检查GPU是否被使用，根据NVIDIA CUDA Toolkit 自带的示例程序输出内容显示均正常。

  ```
  C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\extras\demo_suite下bandwidthTest、deviceQuery文件直接拖至cmd界面中执行
  ```

##### agent添加本地部署搜索引擎

- [dify部署](# 23日：)

- 部署searXNG搜索引擎：修改settings.yml文件，docker-compose.yaml文件。

  dify创建智能体连接searXNG填写base url：http://192.168.1.137:8081

  ```yaml
  # settings.yml文件
  use_default_settings: true
  server:
    # base_url is defined in the SEARXNG_BASE_URL environment variable, see .env and docker-compose.yml
    secret_key: "R+PzvVHeg97zDPWbGZKK4RfeVu+ZjZQPhkqsqysoUYU="  # change this!
    limiter: false  # enable this when running the instance for a public usage on the internet
    image_proxy: true
  ui:
    static_use_hash: true
  redis:
    url: redis://redis:6379/0
  search:
    formats:
      - html
      - json
      - rss
  ```

  ```yaml
  # docker-compose.yaml文件
  version: "3.7"
  
  services:
    redis:
      container_name: redis
      image: docker.io/valkey/valkey:8-alpine
      command: valkey-server --save 30 1 --loglevel warning
      restart: unless-stopped
      networks:
        - searxng
      volumes:
        - valkey-data2:/data
      cap_add:
        - SETGID
        - SETUID
        - DAC_OVERRIDE
      logging:
        driver: "json-file"
        options:
          max-size: "1m"
          max-file: "1"
  
    searxng:
      container_name: searxng
      image: docker.io/searxng/searxng:latest
      restart: unless-stopped
      networks:
        - searxng
      ports:
        - "8081:8080"
      volumes:
        - ./searxng:/etc/searxng:rw
      environment:
        - SEARXNG_BASE_URL=http://${SEARXNG_HOSTNAME:-localhost}/
        - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
        - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
      cap_add:
        - CHOWN
        - SETGID
        - SETUID
      logging:
        driver: "json-file"
        options:
          max-size: "1m"
          max-file: "1"
  
  networks:
    searxng:
  
  volumes:
    valkey-data2:
  
  ```



### 30日：

##### 显卡CPU和内存使用问题（数据管道瓶颈）：

- 首先受限于硬件的原因（RTX3070），batch不能调整的太大一般低于64导致显卡一次处理的图片不多使用率自然不高。

- 将图片加载进行预处理时使用内存和CPU这导致两者使用率较GPU更高，尤其是数据准备阶段（CPU在做数据加载、解码、增强等）。

- 双显卡训练问题：修改device参数设置

  ```python
  # 根据显卡数量和任务数量动态调整这两个参数
  model.train(..., device=[0,1], workers=16)
  ```

- 可能是cpu对数据的预处理速度供不上gpu的使用，因此gpu要等待cpu进行数据预处理，待处理好的数据传回来之后gpu再继续工作。

##### 编写agent系统提示词：



## 七月

### 1日：

##### 智能体推理缓慢问题：

使用本地部署的qwen3:8b大模型完成推理执行过程非常缓慢因为参数量太大，连续发出请求发生电脑直接断电的问题触发"过热保护"。同时解释了为什么智能体不能正常工作往往只返回json格式的原始文件而没有正确的输出总结内容

###### 使用Function Calling策略的LLM工作流程：

LLM 在此策略中不仅仅是生成自由格式的文本，而是要生成一个结构化的请求（例如，一个指明函数名称和参数的 JSON 对象），这个请求可以被 Dify 平台或工具本身准确解析并执行 。LLM 是否能够持续稳定地产生符合预定义工具规范的、格式正确的结构化输出，是 Function Calling 策略可靠性的关键。

- **用户输入 (User Query):** 用户向 Agent 发出指令或问题。
- **LLM 意图识别 (LLM Intent Recognition):** Agent 内的 LLM 分析用户输入，理解其真实意图。
- **LLM 工具选择 (LLM Tool Selection):** 根据识别到的意图，LLM 从可用的工具列表中选择最合适的工具来完成任务。
- **LLM 参数提取 (LLM Parameter Extraction):** LLM 从用户输入中提取调用所选工具所需的参数。
- **工具执行 (Tool Execution):** Dify 平台根据 LLM 提供的工具名称和参数，实际执行该工具。
- **结果返回 (Result Returned):** 工具执行的结果会返回给 LLM (在某些场景下) 或直接输出到工作流的下一步。

##### 智能助手实现思路：

###### 语音指令：

联网实现在线搜索和信息爬取。实现多轮聊天，查询天气等。

- 启动服务：通过关键词唤醒服务，具体实现考虑封装成ROS节点和主控联系，主控打开服务器网址启动agent
- 本地语音识别内容上传问题、主控开启agent后怎么做到回答的内容输出

###### 机器人语音控制：

- 启动服务：ROS节点发布指令例如（向前走、向后走、左右拐）
- 考虑精确控制小车移动、躲避障碍物以及如何让小车执行指令

##### 智能体开发：

计划流血检测模型训练完成后在24服务器上部署，本地访问测试使用效果。
将dify容器、searXNG容器开启后通过链接http://localhost/apps进入。

##### 流血检测模型开发：

新思路使用YOLO提供的分割模型yolo-seg对血液的形状进行训练，另外考虑颜色等影响。



### 2日：

##### 关键词触发功能：

创建发布者话题名为asr_exacute，实现主控获取关键词触发信息达到打开语音助手的目的以及机器人控制指令。

完成小车控制指令识别，包括向前、向后、向左、向右以及前进后退。

##### 语音指令开发文档书写



### 3日：

##### 流血检测训练集优化：

YOLOv8n-seg原始模型mAP@50通常在60%-70%区间，经过优化可提升至90%以上。修改数据集重新训练模型

重新下载数据集进行模型训练

##### 智能体怎么提升提问和回答的准确度：

```
user -> query -> embedding模型 -> token + RAG -> cross-encoder（召回） -> LLM -> 回答
 		  ↓
 	输入参数量过大时
LlamaIndex分块提取关键信息 -> 将多模态多渠道信息切块（chunk）,建立索引
								   		  ↓
								   keywords查询Node
```



### 6日：

##### 设置YOLO默认settings.yaml:

```python
from ultralytics.cfg import handle_yolo_settings
handle_yolo_settings([])
```

这会在终端输出当前的所有 Ultralytics 设置，包括 settings.yaml 路径等信息。

##### 设置缓冲区解决windows系统git下载失败问题：

```shell
git config --global http.postBuffer 524288000
```

##### 完成流血检测模型的训练：



### 7日：

##### 正向样本标签：

- 正向样本（positive sample）：指图片中包含你要检测的目标（如血液、猫、车辆等）。对于目标检测（如YOLO），每张正向样本图片都要有一个与之同名的标签文件（.txt），里面详细描述了目标的位置和类别。

  - **类别编号**：目标属于哪个类别（从0开始编号）。

  - **x_center, y_center**：目标的中心点坐标（相对于图片宽高归一化，范围0~1）。

  - **width, height**：目标的宽和高（也是相对于图片宽高归一化，范围0~1）。

  ```
  类别编号 x_center y_center width height
  
  # 举例
  0 0.454 0.550 0.255 0.497
  0 0.484 0.331 0.575 0.650
  ```

​	表示图片中有两个血液目标，分别在不同的位置和大小。

- 模型如何利用这些标签学习：开始训练后，模型读出图片的标签文件。通过表卡文件中的内容学习识别到的类别以及物体的位置信息，模型会尝试预测所有目标框，并和标签对比，优化自己的预测能力。如果**没有标签**内容（空文件），这张图片就被当作**“负样本”**，**模型学会这里没有目标**。

- 标注工具：可视化标注，生成YOLO格式

  [LebalImg](https://github.com/HumanSignal/labelImg)：单类别/多类别物体标注，轻量级需求。支持鼠标框选目标，保存即生成对应txt文件（支持类别自定义）。另外可对标签文件进行修正。

  [Roboflow Annotate](https://roboflow.com/annotate)：网页版，团队协作、自动标注、类别管理强大。团队协作、云端管理、自动标注。

  [Labelme](https://github.com/wkentaro/labelme)：主要用于分割（可导出bbox），支持多边形标注。

  [CVAT](https://github.com/opencv/cvat)：Web端，适合大项目和团队，支持丰富标注类型和批量操作。

##### 使用Labelmg生成标签：

- 安装使用：

  - pip 安装Labelmg

    ```shell
    # 安装运行Labelmg
    pip install labelimg
    labelimg
    ```

  - 源码编译安装：确保你已经安装了 Git、Python、PyQt5 和 lxml

    ```shell
    # 克隆 LabelImg 的 GitHub 仓库
    git clone https://github.com/tzutalin/labelImg.git
    
    # 安装 PyQt5 和 lxml
    pip install pyqt5 lxml
    
    # 编译并运行 LabelImg
    pyrcc5 -o libs/resources.py resources.qrc
    python labelImg.py
    ```

- 修改默认设置：清空并自定义 `predefined_classes.txt`

  - 源码运行的，路径类似：labelImg/data/predefined_classes.txt，删除预设置的类别名。

  - pip 安装的，可以使用如下命令查找路径：然后在输出的 `Location` 目录下找到 `labelImg/data/predefined_classes.txt`

    ```shell
    pip show labelimg
    ```

- 启动 LabelImg 时指定自定义标签文件

  ```shell
  # 参数分别图片文件位置 预定义标签文本位置
  python labelImg.py ./images ./my_labels.txt   
  ```

##### 制作血液检测负样本数据集：

考虑到机器人专供家庭使用并且需要和用户交互，对于负样本需要考虑在家里会出现的常见红色物体最好是抓住与人交互的状态下的照片。

- 类别考虑：红苹果、红樱桃、红辣椒、红玫瑰、红宝石、红气球、红灯笼、红椅子、红杯子、番茄酱（红番茄）、红色指甲油、红蛋糕	
- 其他因素考虑：考虑不同亮度下图片的效果，调整图片亮度；考虑图片旋转剪裁后重新制作成样本。



### 8日：

##### 准备负样本数据集：

红板凳、红墨水、红花盆

##### 测试负样本：

使用模型对负样本数据集进行测试，剔除能够识别的样本将不能识别的样本归纳制作数据集。



### 9日：

##### agent中RAG构建：

学习在智能体中实现长文本总结以及本地知识库搭建

##### 血液检测数据集准备完毕：

- 数据集增设正向数据以及强负样本并添加标签文件，开始新数据集模型训练

- 新模型的精度为0.802可以识别花朵有一定泛化能力



### 10日：

##### 血液检测模型转换测试：

- 自定义模块C3k2缺失问题：

  ```shell
  AttributeError: Can't get attribute 'C3k2' on <module 'ultralytics.nn.modules.block' from '/home/yls/miniconda3/envs/rknn160/lib/python3.8/site-packages/ultralytics/nn/modules/block.py'>
  ```

  - 解决办法：`C3k2` 是 YOLOv8 中新增的模块之一，当前安装的 `ultralytics` 版本太旧，缺少这个模块。

    ```shell
    pip install --upgrade ultralytics
    ```

##### 配置服务器Ollama                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              环境：

- 设备24服务器配置不支持WSL2，不能使用docker-desktop本地部署Ollama

- 官网下载windows版本Ollama安装包

  ```shell
  # 拉取官方支持的大模型
  ollama run <模型名>
  
  # 验证拉取是否成功
  ollama list
  ```

  

### 13日：

##### ‌安装 Infortress 服务端

访问 [Infortress 官网](https://infortress.com/) 下载对应操作系统的服务端程序（Windows/macOS/Linux），双击安装包按向导完成安装。

- 安装路径避免使用系统盘（如 C 盘）‌
- 安装完成后自动生成数据目录 `./Data`（勿手动修改）‌
- 启动并登录‌：打开 Infortress 服务端 → 使用邮箱注册账号 → 进入主界面‌

##### 配置 AnythingLLM 与 Infortress 对接

‌获取 AnythingLLM 的 API 密钥：打开 AnythingLLM → 左下角进入设置 → 找到 ‌API 密钥‌ → 点击【创建新密钥】→ 复制生成的密钥‌

关联密钥到 Infortress：在 Infortress 服务端 → 左侧菜单选择【设置】→ 找到 ‌本地知识库‌ → 粘贴复制的 API 密钥 → 点击保存‌

##### 模型部署

下载Node.js，AnythingLLM 是基于 Node.js 构建的，所以需要安装 Node.js 环境

- 安装Yarn

  ```shell
  npm install -g yarn
  
  # 验证安装
  yarn --version
  ```

  

### 14日：

##### 模型转换并测试：

- 创建rknn工具链虚拟环境：用于将onnx格式文件转化为rknn格式，可根据创建环境所指定的python选择工具链[安装包](/home/yls/rknn-toolkit2/rknn-toolkit2/rknn-toolkit2/packages/x86_64)，安装命令如下。

  ```shell
  pip install rknn_toolkit2-2.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
  ```

- 准备材料：用于转换的模型、推理图片以及转换脚本（x86架构下和arm架构下）

- 开始转换：根据yolov8或者yolo11分别选择转换脚本

  - yolov8：选择rknn_infer_x86.py脚本将onnx格式模型转换测试，rknn_infer.py脚本将脚本转换后部署在开发板测试。

  - yolo11：选择[convert.py](https://github.com/airockchip/rknn_model_zoo/blob/main/examples/yolo11/python/convert.py)脚本转换模型并部署测试

    ```shell
    # 进入/home/yls/YOLOV8-on-RK3588/rk3588/python测试转换脚本
    python convert.py path/to/model.onnx rk3588
    ```

- 模型部署测试：ros2功能包形式在开发板上测试
  - 创建功能包：放入模型，修改后处理逻辑多流血判定进行截图，发布流血话题
  - 目前存在同意对象多次检测截图，以及识别精度过低的情况

##### 整理流血检测模型数据集



### 15日：

##### 血液检测文档书写：

完成血液检测开发档案书写以及后续优化方向。

##### 服务器智能体开发：

完成服务器模型部署，准备开始智能体开发。

- Dify连接模型时存在端口占用问题：Ollama 安装脚本通常会注册一个 systemd 服务（名称为 `ollama`），使用的端口号默认为11434与docker部署冲突。

  ```shell
  curl -fsSL https://ollama.com/install.sh | sh
  
  # 停止服务
  sudo systemctl stop ollama
  
  # 使用docker启动ollama
  docker run -d -p 11434:11434 -v ollama:/root/.ollama ollama/ollama
  ```




### 16日：

##### 智能体创建：

Dify调用ollama本地部署的模型完成智能体框架的创建

##### 智能能体推理缓慢：

对于一般性问题可以正常回答，但是推理速度很慢

###### 中文输入问题：

虚拟机内浏览器不能使用中文输入，环境是 ubuntu 22.04，在火狐浏览器中。

```shell
lsb_release -a 		# 查看ubuntu版本信息
```

###### 驱动更新：

ubuntu 默认驱动 Nouveau 不能完全发挥显卡性能

1.  彻底清理旧驱动：

   ```bash
   # 卸载所有 NVIDIA 相关组件
   sudo apt purge *nvidia* *cuda* *cudnn*
   sudo apt autoremove --purge
   sudo apt autoclean
   
   # 删除残留文件
   sudo rm -rf /etc/apt/sources.list.d/cuda*
   sudo rm -rf /usr/local/cuda*
   sudo rm -rf ~/.nv/
   ```

2. 禁用 Nouveau 驱动：

   ```bash
   # 创建黑名单文件
   sudo nano /etc/modprobe.d/blacklist-nouveau.conf
   
   # 添加内容
   blacklist nouveau
   options nouveau modeset=0
   
   # 完成后执行
   sudo update-initramfs -u
   sudo reboot
   ```

3. 安装推荐驱动（带 Secure Boot 支持）

   ```bash
   # 添加官方 NVIDIA PPA
   sudo add-apt-repository ppa:graphics-drivers/ppa
   sudo apt update
   
   # 安装推荐驱动（带开源内核模块）
   sudo apt install nvidia-driver-570-server nvidia-dkms-570-server
   ```

4. 处理 Secure Boot

   ```bash
   # 启用安全启动支持
   sudo apt install mokutil
   
   # 创建 MOK 密钥（安装过程中会提示设置密码）
   sudo update-secureboot-policy --enroll-key
   ```

5. 重启并完成 MOK 注册

   ```bash
   sudo reboot
   ```

   - 重启后重要步骤：

     系统会进入 蓝色 MOK 管理界面

     选择 `Enroll MOK` > `Continue` > `Yes`

     输入之前设置的密码

     选择 `OK` 并重启

6. 验证安装：

   ```bash
   # 检查驱动加载
   nvidia-smi
   
   # 验证内核模块
   lsmod | grep nvidia
   
   # 检查安全启动状态
   mokutil --sb-state
   ```

##### docker默认不支持使用显卡问题：

模型不能只考虑需要的内存大小，参数量决定了显卡推理的速度，即使使用A100跑满依旧存在响应缓慢的问题 （17日使用A100测试deepseek-r1:32b模型）



### 17日：

##### 安装 NVIDIA 容器工具：

解决模型推理缓慢问题，docker部署的dify默认不支持使用本地 GPU ，即使安装了正确的显卡驱动也会检测不到设备，需要用 docker 部署 [NVIDIA 容器工具](https://github.com/NVIDIA/libnvidia-container)让dify 接入的 ollama 拉取的模型可以使用本地显卡进行推理。

- 检查是否安装 NVIDIA 驱动：

  ```bash
  # 检查驱动是否已安装
  nvidia-smi  # 正常输出表示驱动正常
  
  # 若未安装，按以下步骤操作：
  # 添加 NVIDIA 仓库（Ubuntu 示例）
  sudo apt update && sudo apt install -y ubuntu-drivers-common
  sudo ubuntu-drivers autoinstall
  sudo reboot
  ```

- 安装 NVIDIA 容器工具：

  ```bash
  # 获取 OS 信息
  distribution=$(. /etc/os-release; echo $ID$VERSION_ID)
  # 添加 GPG 密钥到信任环
  curl -fsSL  https://nvidia.github.io/libnvidia-container/gpgkey  | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
  # 添加仓库地址到 APT 源
  curl -s -L "https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list " | \
    sed "s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g" | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
  
  # 安装 nvidia-docker2（关键步骤）
  sudo apt update				# 注意这两条指令要同时执行，否则会报错E: 无法定位软件包 nvidia-container-toolkit
  sudo apt install -y nvidia-container-toolkit
  
  # 验证安装
  nvidia-ctk --version
  NVIDIA Container Toolkit version X.X.X		# 输出示例
  ```

  - docker测试 NVIDIA 容器时遇到

    ```shell
    # 测试指令
    sudo docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
    
    # 报错
    Unable to find image 'nvidia/cuda:12.0-base' locally
    docker: Error response from daemon: manifest for nvidia/cuda:12.0-base not found: manifest unknown: manifest unknown
    
    # 正确输出示例
    nvidia/cuda    12.2.0-runtime-ubuntu22.04   5f87b1ee9f9a   20 months ago   1.97GB
    
    # 进入已有容器查看是否能手动调用 nvidia-smi
    sudo docker run -it --rm --gpus all nvidia/cuda:12.2.0-runtime-ubuntu22.04 bash
    which nvidia-smi
    ```

    解决办法：运行以下命令查看本地已有的镜像，或者更新 NVIDIA 官方的 CUDA 镜像

    ```bash
    # 检查本地镜像
    docker images | grep nvidia/cuda
    
    # 下载官方镜像
    sudo docker run --rm --gpus all nvidia/cuda:12.4.0-base nvidia-smi
    ```

- 配置 Docker 使用 GPU：

  ```bash
  # 查看本地已有的镜像
  docker images
  
  # 使用支持 CUDA 的 Ollama Docker 镜像
  docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
  ```

  

##### 智能体网络和知识库：

###### 配置 Google 搜索引擎：

- 获取 API 密钥：支持科学上网情况下前往https://serpapi.com网页申请免费搜索引擎 API ，前期每月共有100次的免费使用额度。SerpAPI 申请免费的 API 支持调用 Google 搜索引擎。只支持内网情况下使用 duckduckgo 测试联网搜索功能。

- 配置工具密钥：

  - 下载 Google 工具：进入 Dify Marketplace 搜索 Google下载，在智能体创建界面应用将 SerpAPI 的密钥配置到密钥框中。其他工具过程类似。

  - 报错：

    1. Ollama 拉取的 deepseek-r1:14b 模型不支持“tools”功能（即不支持 Function Calling），因此无法调用 Dify 的时间工具或任何其他工具。

       ```bash
       [ollama] Error: PluginInvokeError: {"args":{"description":"[models] Error: API request failed with status code 400: {\"error\":\"registry.ollama.ai/library/deepseek-r1:14b does not support tools\"}"},"error_type":"InvokeError","message":"[models] Error: API request failed with status code 400: {\"error\":\"registry.ollama.ai/library/deepseek-r1:14b does not support tools\"}"}
       
       # 模型不支持使用工具需要修改部署的模型  尝试拉取其他支持的模型比如qwen3:14b
       ```

    2. 

###### 配置知识库：

- 运行embedding模型：选择 ollama [官网](https://ollama.com/search?c=embedding)中合适的 Embedding 模型，用于测试开发的智能体选择了 nomic-embed-text:v1.5，如下图所示：

  ```bash
  # 拉取 embedding 模型
  ollama pull nomic-embed-text:v1.5
  ```

  ![image-20250717155212260](/home/yls/.config/Typora/typora-user-images/image-20250717155212260.png)

- 准备自定义知识库：选择中控语音通话的开发文档来测试模型基于本地知识库回答问题的能力。测试文件 pjusa2基本认识.md

  ```
  顶端导航栏选择知识库	->	创建新知识库	->	导入新文件（支持 TXT、 MARKDOWN、 MDX、 PDF、 HTML、 XLSX、 XLS、 DOCX、 CSV、 VTT、 PROPERTIES、 MD、 HTM，每个文件不超过 15MB。）	->	选择 Embedding 模型根据提示对文件进行分块
  ```

  注：知识库中不能添加图片，否则模型总结知识库时会报错。

  ​	模型存在幻觉对未知的知识有胡乱回答的可能。

​		正式使用知识库时采用 md 文档形式

​		![知识库](/home/yls/图片/知识库.png)



##### 设计智能助手开场词：

- 简单自我介绍
- 默认设置可能提问的问题



### 20日：

- threshold 参数：用于判断无语音段落的阈值。

- VAD：从包含语音的一段信号中准确地确定语音的起始点和终止点，区分语音和非语音信号，它是语音处理技术中的一个重要方面

##### 语音助手理论学习：

学习语音助手语音识别以及语音输出的原理

##### 模型 API 申请

尝试连接 API 测试语音识别以及语音转录

- 申请API：登陆阿里云百炼平台注册账号并进行实名认证

- 创建 API-Key ：模型界面选择需要的模型，在左下角 API-Key 处选择创建 API 。

  ![image-20250720145618931](/home/yls/.config/Typora/typora-user-images/image-20250720145618931.png)

- 配置环境：

  - [配置API Key](https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2803795.html&renderType=iframe)到环境变量：

    1. Linux环境下：执行以下命令来将环境变量设置追加到`~/.bashrc `文件中。

       ```bash
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       echo "export DASHSCOPE_API_KEY='YOUR_DASHSCOPE_API_KEY'" >> ~/.bashrc
       
       # 执行以下命令，使变更生效。
       source ~/.bashrc
       
       # 重新打开一个终端窗口，运行以下命令检查环境变量是否生效。
       echo $DASHSCOPE_API_KEY
       ```

    2. Windows环境下：在Windows系统中，您可以通过系统属性、CMD或PowerShell配置环境变量。

       ```cmd
       # 1. 在CMD中运行以下命令。
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       setx DASHSCOPE_API_KEY "YOUR_DASHSCOPE_API_KEY"
       
       # 在新的CMD窗口运行以下命令，检查环境变量是否生效。正确结果如下图所示
       echo %DASHSCOPE_API_KEY%
       ```

       ![](/media/yls/1T硬盘7/picture/API_cmd.png)

       ```powershell
       # 2. 在PowerShell中运行以下命令。
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       [Environment]::SetEnvironmentVariable("DASHSCOPE_API_KEY", "YOUR_DASHSCOPE_API_KEY", [EnvironmentVariableTarget]::User)
       
       # 在新的PowerShell窗口运行以下命令，检查环境变量是否生效。正确结果如下图所示
       echo $env:DASHSCOPE_API_KEY
       ```

       ![](/media/yls/1T硬盘7/picture/API_PowerShell.png)

    3. Mac环境下：如果您希望API Key环境变量在当前用户的所有新会话中生效，可以添加永久性环境变量。

       ```bash
       # 1. 在终端中执行以下命令，查看默认Shell类型。
       echo $SHELL
       
       # 2. 根据默认Shell类型进行操作。
       
       a.执行以下命令来将环境变量设置追加到 ~/.bash_profile 文件中。
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       echo "export DASHSCOPE_API_KEY='YOUR_DASHSCOPE_API_KEY'" >> ~/.bash_profile
       
       # 执行以下命令，使变更生效。并重新打开一个终端检查是否设置成功
       source ~/.bash_profile
       echo $DASHSCOPE_API_KEY
       
       b.执行以下命令来将环境变量设置追加到 ~/.zshrc 文件中。
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       echo "export DASHSCOPE_API_KEY='YOUR_DASHSCOPE_API_KEY'" >> ~/.zshrc
       
       # 执行以下命令，使变更生效。并重新打开一个终端检查是否设置成功
       source ~/.bash_profile
       echo $DASHSCOPE_API_KEY
       ```

  - [配置SDK](https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2712193.html&renderType=iframe)：阿里云百炼官方提供了 Python 与 Java 编程语言的 SDK，也提供了与 OpenAI 兼容的调用方式（OpenAI 官方提供了  Python、Node.js、Java、Go 等 SDK）。本文为您介绍如何安装 OpenAI SDK 以及 DashScope SDK。

    ```shell
    # 如果运行失败，您可以将pip替换成pip3再运行
    pip install -U openai
    ```




### 21日：

##### 优化浏览器搜索

用 Dify + Ollama + DuckDuckGo 的方式做联网搜索，之所以可以搜新闻却“搜不到天气”，大概率是因为 DuckDuckGo 的搜索结果里对“天气”这类查询返回的是**卡片化/结构化数据**（例如只给出“今日天气请见 weather.com”并附一个外链），而不是一段可直接拿来当答案的文本。LLM 收到这种结果后，只能把链接抛给用户，于是看起来就像“搜索失败”。

- 解决方案：通过自定义工具形式形式访问

  1. 申请 API Key ：可以选择 高徳、和风以及彩云三家均提供免费查询额度，笔者试了和风和彩云分别提供50000以及10000额度每月其中彩云支持设置额度提醒功能好评。

     <img src="/home/yls/.config/Typora/typora-user-images/image-20250721100353532.png" alt="image-20250721100353532" style="zoom: 50%;" />

  2.  创建自定义工具：

     - 选择自定义工具新建工具根据模板修改结构，这一步可以使用 [Swagger](https://editor.swagger.io/) 或者直接让AI生成，注意不要泄漏接口密钥，生成示例如下：

       ```json
       {
         "openapi": "3.1.0",
         "info": {
           "title": "Weather Query",
           "description": "根据城市名返回实时天气",
           "version": "v1.0.0"
         },
         "servers": [{ "url": "https://wttr.in" }],
         "paths": {
           "/{location}": {
             "get": {
               "operationId": "getWeather",
               "summary": "获取天气",
               "parameters": [
                 {
                   "name": "location",
                   "in": "path",
                   "required": true,
                   "schema": { "type": "string" },
                   "description": "城市名，例如 Beijing"
                 },
                 {
                   "name": "format",
                   "in": "query",
                   "schema": { "type": "string", "default": "%l:+%c+%t+%h" },
                   "description": "返回格式"
                 }
               ],
               "responses": {
                 "200": {
                   "description": "纯文本天气信息"
                 }
               }
             }
           }
         }
       }
       ```

     - 如接口需鉴权，点击「设置鉴权」选择 API Key 通常选择 Bearer 填写 token
     - 测试接口功能是否正常：点击右下角 「测试」输入参数 `location = Shanghai`能看到返回 `Shanghai: ⛅ +33°C 湿度 59%` 即成功

- 更换浏览器工具：替换为 Google 可以做到浏览器信息查询得到正确数据。

##### 语音输入输出



### 22日：

##### 语音输入设置：

###### 接口调用

- 使用 Chat GPT 接口调用语音服务

###### 本地部署

- Xinference 本地部署语音识别以及 TTS 模型

##### 源码部署 Dify：

###### 中间件启动

- **源代码本地启动：**

  前提：设置 Docker 和 Docker Compose，在安装 Dify 之前，请确保您的设备符合。使用 Miniconda 创建 Python >3.12 虚拟环境。

  ```
      CPU >= 2 核
      RAM >= 4 GiB
  ```

  - **克隆 Dify 仓库：**运行 git 命令克隆 [Dify 仓库](https://github.com/langgenius/dify)。

    ```bash
    git clone https://github.com/langgenius/dify.git
    ```

  - **使用 Docker Compose 启动中间件**

    Dify 后端服务需要一系列用于存储（如 PostgreSQL / Redis / Weaviate（如果本地不可用））和扩展能力（如 Dify 的 [sandbox](https://github.com/langgenius/dify-sandbox) 和 [plugin-daemon](https://github.com/langgenius/dify-plugin-daemon) 服务）的中间件。通过运行以下命令使用 Docker Compose 启动中间件：

    ```bash
    cd docker
    cp middleware.env.example middleware.env
    docker compose -f docker-compose.middleware.yaml up -d
    ```

  - **设置后端服务：**

    1. API 服务：为前端服务和 API 访问提供 API 请求服务
    2. Worker 服务：为数据集处理、工作区、清理等异步任务提供服务

    - 环境准备：使用 Miniconda 创建 Python 3.12环境

      ```bash
      conda create -n env python=3.12
      ```

    - 启动 API 服务：进入 API 文件夹启动 Dify 后端服务

      ```bash
      # 切换到 api 目录
      cd dify/api
      
      # 准备环境变量配置文件
      cp .env.example .env
      
      # 生成随机密钥并替换 .env 文件中的 SECRET_KEY 值
      awk -v key="$(openssl rand -base64 42)" '/^SECRET_KEY=/ {sub(/=.*/, "=" key)} 1' .env > temp_env && mv temp_env .env
      
      # 安装依赖 使用 uv 管理依赖。 通过运行以下命令使用 uv 安装所需依赖
      pip install nv 
      uv sync 			# 对于 macOS：使用 brew install libmagic 安装 libmagic。
      
      # 执行数据库迁移 执行数据库迁移到最新版本
      uv run flask db upgrade
      
      # 启动 API 服务
      uv run flask run --host 0.0.0.0 --port=5001 --debug
      
      # 预期示例
      * Debug mode: on
      INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
       * Running on all addresses (0.0.0.0)
       * Running on http://127.0.0.1:5001
      INFO:werkzeug:Press CTRL+C to quit
      INFO:werkzeug: * Restarting with stat
      WARNING:werkzeug: * Debugger is active!
      INFO:werkzeug: * Debugger PIN: 695-801-919
      ```

  - 启动 Worker 服务：

    要从队列中消费异步任务，例如数据集文件导入和数据集文档更新，请按照以下步骤启动 Worker 服务

    ```bash
    # 对于 macOS 或 Linux
    uv run celery -A app.celery worker -P gevent -c 1 --loglevel INFO -Q dataset,generation,mail,ops_trace
    
    # 对于 Windows
    uv run celery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail,ops_trace --loglevel INFO
    
    # 预期输出
    -------------- celery@bwdeMacBook-Pro-2.local v5.4.0 (opalescent)
    --- ***** -----
    -- ******* ---- macOS-15.4.1-arm64-arm-64bit 2025-04-28 17:07:14
    - *** --- * ---
    - ** ---------- [config]
    - ** ---------- .> app:         app_factory:0x1439e8590
    - ** ---------- .> transport:   redis://:**@localhost:6379/1
    - ** ---------- .> results:     postgresql://postgres:**@localhost:5432/dify
    - *** --- * --- .> concurrency: 1 (gevent)
      -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)
      --- ***** -----
      -------------- [queues]
      .> dataset          exchange=dataset(direct) key=dataset
      .> generation       exchange=generation(direct) key=generation
      .> mail             exchange=mail(direct) key=mail
      .> ops_trace        exchange=ops_trace(direct) key=ops_trace
    
    [tasks]
    . schedule.clean_embedding_cache_task.clean_embedding_cache_task
    . schedule.clean_messages.clean_messages
    . schedule.clean_unused_datasets_task.clean_unused_datasets_task
    . schedule.create_tidb_serverless_task.create_tidb_serverless_task
    . schedule.mail_clean_document_notify_task.mail_clean_document_notify_task
    . schedule.update_tidb_serverless_status_task.update_tidb_serverless_status_task
    . tasks.add_document_to_index_task.add_document_to_index_task
    . tasks.annotation.add_annotation_to_index_task.add_annotation_to_index_task
    . tasks.annotation.batch_import_annotations_task.batch_import_annotations_task
    . tasks.annotation.delete_annotation_index_task.delete_annotation_index_task
    . tasks.annotation.disable_annotation_reply_task.disable_annotation_reply_task
    . tasks.annotation.enable_annotation_reply_task.enable_annotation_reply_task
    . tasks.annotation.update_annotation_to_index_task.update_annotation_to_index_task
    . tasks.batch_clean_document_task.batch_clean_document_task
    . tasks.batch_create_segment_to_index_task.batch_create_segment_to_index_task
    . tasks.clean_dataset_task.clean_dataset_task
    . tasks.clean_document_task.clean_document_task
    . tasks.clean_notion_document_task.clean_notion_document_task
    . tasks.deal_dataset_vector_index_task.deal_dataset_vector_index_task
    . tasks.delete_account_task.delete_account_task
    . tasks.delete_segment_from_index_task.delete_segment_from_index_task
    . tasks.disable_segment_from_index_task.disable_segment_from_index_task
    . tasks.disable_segments_from_index_task.disable_segments_from_index_task
    . tasks.document_indexing_sync_task.document_indexing_sync_task
    . tasks.document_indexing_task.document_indexing_task
    . tasks.document_indexing_update_task.document_indexing_update_task
    . tasks.duplicate_document_indexing_task.duplicate_document_indexing_task
    . tasks.enable_segments_to_index_task.enable_segments_to_index_task
    . tasks.mail_account_deletion_task.send_account_deletion_verification_code
    . tasks.mail_account_deletion_task.send_deletion_success_task
    . tasks.mail_email_code_login.send_email_code_login_mail_task
    . tasks.mail_invite_member_task.send_invite_member_mail_task
    . tasks.mail_reset_password_task.send_reset_password_mail_task
    . tasks.ops_trace_task.process_trace_tasks
    . tasks.recover_document_indexing_task.recover_document_indexing_task
    . tasks.remove_app_and_related_data_task.remove_app_and_related_data_task
    . tasks.remove_document_from_index_task.remove_document_from_index_task
    . tasks.retry_document_indexing_task.retry_document_indexing_task
    . tasks.sync_website_document_indexing_task.sync_website_document_indexing_task
    
    2025-04-28 17:07:14,681 INFO [connection.py:22]  Connected to redis://:**@localhost:6379/1
    2025-04-28 17:07:14,684 INFO [mingle.py:40]  mingle: searching for neighbors
    2025-04-28 17:07:15,704 INFO [mingle.py:49]  mingle: all alone
    2025-04-28 17:07:15,733 INFO [worker.py:175]  celery@bwdeMacBook-Pro-2.local ready.
    2025-04-28 17:07:15,742 INFO [pidbox.py:111]  pidbox: Connected to redis://:**@localhost:6379/1.
    ```

  - 设置 Web 服务：启动用于前端页面的 web 服务。

    环境准备：要启动 web 前端服务，需要 [Node.js v22 (LTS)](http://nodejs.org/) 和 [PNPM v10](https://pnpm.io/)。

    ```bash
    # 安装 nvm
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash
    source ~/.bashrc        # 重新加载 shell
    
    # 安装最新 LTS（长期支持）版本
    nvm install --lts
    nvm use --lts
    
    # 验证
    node -v     # 应输出 v20.x 或 v18.x
    npm  -v
    
    # 安装pnpm
    npm i -g pnpm
    ```

    - 启动 Web 服务：

      ```bash
      # 进入 web 目录
      cd ../web
      
      # 安装依赖
      pnpm install --frozen-lockfile
      
      # 准备环境变量配置文件 在当前目录中创建一个名为 .env.local 的文件，并从 .env.example 复制内容。根据您的需求修改这些环境变量的值
      # For production release, change this to PRODUCTION
      NEXT_PUBLIC_DEPLOY_ENV=DEVELOPMENT
      # The deployment edition, SELF_HOSTED or CLOUD
      NEXT_PUBLIC_EDITION=SELF_HOSTED
      # The base URL of console application, refers to the Console base URL of WEB service if console domain is
      # different from api or web app domain.
      # example: http://cloud.dify.ai/console/api
      NEXT_PUBLIC_API_PREFIX=http://localhost:5001/console/api
      # The URL for Web APP, refers to the Web App base URL of WEB service if web app domain is different from
      # console or api domain.
      # example: http://udify.app/api
      NEXT_PUBLIC_PUBLIC_API_PREFIX=http://localhost:5001/api
      
      # SENTRY
      NEXT_PUBLIC_SENTRY_DSN=
      NEXT_PUBLIC_SENTRY_ORG=
      NEXT_PUBLIC_SENTRY_PROJECT=
      
      # 构建 web 服务
      pnpm build
      
      # 启动 web 服务
      pnpm start
      
      # 预期输出
      ▲ Next.js 15
         - Local:        http://localhost:3000
         - Network:      http://0.0.0.0:3000
      
       ✓ Starting...
       ✓ Ready in 73ms
      ```

  - 访问 Dify：

    初次登录通过浏览器访问 [http://127.0.0.1:3000](http://127.0.0.1:3000/) 即可享受 Dify 所有激动人心的功能。后续在同一台设备上通过http://127.0.0.1:3000/apps登陆使用。



### 23日：

##### 本地部署 TTS 模型：

- 克隆源码至本地：

  ```bash
  git clone https://github.com/2noise/ChatTTS
  ```

- 本地部署 TTS 模型：

  ```bash
  conda create --name xinference python=3.10
  conda activate xinference
  pip install ChatTTS -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install xinference -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

- 安装成功后，只需要输入如下命令，就可以在服务上启动 Xinference 服务：

  ```bash
  xinference-local -H 0.0.0.0
  ```

  Xinference 默认会在本地启动服务，端口默认为 9997。因为这里配置了-H 0.0.0.0参数，非本地客户端也可以通过机器的 IP 地址来访问 Xinference 服务。启动成功后可以通过 http://0.0.0.0:9777来访问 Xinference 的 WebGUI 界面了。



### 24日：

#### 大模型超参数 Temperature ：

在生成式语言模型中，Temperature 参数控制了模型[生成文本](https://so.csdn.net/so/search?q=生成文本&spm=1001.2101.3001.7020)时的`多样性和随机性`。简单来说，Temperature 参数决定了模型在生成下一个单词时，选择概率的分布是否平滑或者更加尖锐。这个参数本质上是一个对模型[概率分布](https://so.csdn.net/so/search?q=概率分布&spm=1001.2101.3001.7020)的重新缩放因子，用来调整输出的熵值，进而影响输出的随机程度。较高的Temperature参数会使模型更具"创造性"，例如在生成散文时可能很有用。而较低的Temperature参数会让模型更具"确定性"，这在问答类应用场景中非常实用。

##### Dify 工具实现语音 IO：

使用硅基流动接口接入 FunAudioLLM/SenseVoiceSmall STT模型、FunAudioLLM/CosyVoice2-0.5B TTS 模型。

###### 模型回答问题：

- 问题描述：Dify 接入的大模型（如 DeepSeek-R1、QwQ 等）**强制开启“深度思考模式”**，导致：
  1. 响应时间过长；
  2. 思考过程被展示给用户；
  3. TTS 语音播报时从思考内容开始读起，体验极差。

- 解决方案：

  1. 使用 Qwen3 模型（推荐）：Qwen3 支持关闭思考模式，可通过以下方式实现

     | 方法           | 操作                                                    | 说明               |
     | -------------- | ------------------------------------------------------- | ------------------ |
     | **参数控制**   | 在 Dify 的模型配置中，添加参数：`enable_thinking=false` | 完全关闭思考过程   |
     | **提示词控制** | 在 prompt 中加入 `/no_think`                            | 动态关闭当前轮思考 |

  2. 使用 DeepSeek-R1 / QwQ 模型（无法关闭思考）：由于这些模型**不支持关闭思考**，需通过**后处理**屏蔽思考内容，模型仅输出结果将过滤后的文本传给 TTS 模型，**避免思考内容被语音播报**

     | 步骤                  | 操作                                        |
     | --------------------- | ------------------------------------------- |
     | **1. 添加工作流节点** | 在 Dify 工作流中，添加一个 **代码执行节点** |
     | **2. 过滤思考内容**   | 使用正则表达式删除 `<think>...</think>` 块  |

     ```python
     # 代码示例
     import re
     
     def main(text: str) -> dict:
         cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
         return {"result": cleaned.strip()}
     ```

###### 接入硅基流动 TTS 模型：

- 前往硅基流动官网申请 API 密钥，在 Dify 模型供应商中配置

- 测试 TTS 模型：

  - 获取 Dify 访问 Token：

    1. docker 直接部署：登录 Dify → 右上角用户头像 → **「设置」→「访问令牌」→ 复制 Token**。

       ```bash
       curl -X POST 'http://localhost:5001/console/api/apps/c793a193-c520-4f64-9885-3de3a0f96de3/text-to-audio' \
         -H 'Authorization: Bearer <这里粘贴控制台 token>' \
         -H 'Content-Type: application/json' \
         -d '{"text":"你好世界","voice":"zh-CN-XiaoxiaoNeural"}' \
         --output test.mp3
       ```

    2. 中间件源码本地部署：直接调用登录接口拿 token，将 token 取代<这里粘贴控制台 token>，调用成功返回一段示例音频。

       ```bash
       curl -X POST http://localhost:5001/console/api/login \
         -H 'Content-Type: application/json' \
         -d '{"email":"你的管理员邮箱","password":"你的密码"}'
         
         
       # 成功返回示例
       {
         "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
         "token_type": "Bearer"
       }
       ```

       

### 27日：

##### TTS 输出问题：

使用硅基流动密钥调用 STT、TTS 模型接口给 Dify 创建的智能体赋能，测试过程中发现在 STT 模型可以正常使用但是 TTS 模型不论是语音试听还是生成回答问题的语音均显示加载中。下面是问题排查的具体步骤：

- 测试后端网页请求：基于后端服务没有报错确定是因为前端请求后端服务时出错。

  ```bash
  curl -X POST http://127.0.0.1:5001/console/api/apps/c793a193-c520-4f64-9885-3de3a0f96de3/text-to-audio \ -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoiNzFkYTA5NzktMGNlYy00ODMwLTllNjUtYjZjM2YyMDU0ODAxIiwiZXhwIjoxNzUzMzUzMjU2LCJpc3MiOiJTRUxGX0hPU1RFRCIsInN1YiI6IkNvbnNvbGUgQVBJIFBhc3Nwb3J0In0.xkdWZLDXqEAW0xqIENYXdnm0PUOmab-ql9o6bGCvu1E" \
    -H "Content-Type: application/json" \
    -d '{"text":"你好世界","voice":"zh-CN-XiaoxiaoNeural"}' \
    --output ok.mp3
  ```

- 前端网页请求问题：OPTIONS 请求没有返回 → CORS 预检被后端直接拦掉，浏览器拿不到 `Access-Control-Allow-*` 头，于是后续真正的 POST 也被阻断。

  ```bash
  # 在 Flask 版 Dify 里，只要给 /console/api/* 加上 CORS 中间件即可
  pip install flask-cors
  ```

  ```python
  from flask_cors import CORS
  ...
  CORS(app, resources={
      r"/console/api/*": {
          "origins": ["http://localhost:3000"],
          "allow_headers": ["Authorization", "Content-Type"],
          "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
      }
  })
  ```

- 浏览器音频拦截：Firefox + NextJS 前端 对 MP3 通过 MediaSource 播放 的支持不完整所致。

  ```bash
  # 网页日志报错内容
  MediaSource.addSourceBuffer: Type not supported in MediaSource  
  Cannot play media. No decoders for requested formats: audio/mpeg
  ```

  解决办法：启动后端服务使用 Google 浏览器打开 http://127.0.0.1:3000/apps 进入开发界面。

##### 调用本地部署 TTS 模型

本地部署 xinference 平台为 Dify 提供模型接口，可以使用 docker 部署或者源码部署，所需空间很大保证挂载的空间大于100G。

- docker 部署[参考](https://blog.csdn.net/zero_forever_lzm/article/details/147305634?ops_request_misc=&request_id=&biz_id=102&utm_term=ubuntu%20xinference%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2%20%E5%B9%B6%E4%BD%BF%E7%94%A8xinfe&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-147305634.142^v102^control&spm=1018.2226.3001.4187)

- 源码部署：可以通过 pip 来安装，如果需要使用 Xinference 进行模型推理，可以根据不同的模型指定不同的引擎，参考[官方文档](https://inference.readthedocs.io/zh-cn/latest/getting_started/installation.html#installation)：

  ```bash
  # 安装需要的依赖
  pip install "xinference[all]"
  
  # 启动服务
  nohup xinference-local --host 0.0.0.0 --port 8892 & > output.log
  ```

  - 下载模型失败：[address=0.0.0.0:43795, pid=832165] The ChatTTS model is not correct: /home/yls/.xinference/cache/v2/ChatTTS



### 28日：

##### 完善语音助手

完成服务器上智能语音助手开发包括：多论对话问答、知识库检索、语音输入输出功能。其中 LLM 使用本地部署 Qwen3:8b、Embedding 模型使用本地部署 nomic-embed-text:v1.5、STT 、TTS 模型采用接口形式实现。

##### 后端实现音频模型调用

使用 Langchain 代码实现接口或本地模型给dify提供音频模型服务。

##### 智能体数字人构建



### 29日：

##### 构建智能聊天机器人：

前端使用gradio构建，gradio是一个开源的Python库，旨在快速构建机器学习模型的交互式网页界面。它允许用户通过简单的几行代码创建可视化的机器学习模型演示项目。希望后续项目可以拓展为 LangChain + Gradio 实现的语音智能机器人，模型依赖接口也可以选择本地部署接入为中控调用提供接口。

###### 开发过程中遇到的 bug：

```
ValueError: When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.
```

- 问题描述：前端部署依赖 gradio Pyton开源库，由于虚拟环境中配置的 python 版本为3.9，该版本支持的库最高版本为 4.44.1，此版本不支持聊天机器人的前端部署。需要升级 python版本。[详情见](https://pypi.org/project/gradio/)

  ![image-20250729164738179](/home/yls/.config/Typora/typora-user-images/image-20250729164738179.png)

  ```bash
  # miniconda 创建的虚拟环境中的 python 版本支持升级，可能需要重新安装某些包
  conda install python=3.10
  ```

##### 数字人开发：



### 30日：

##### 智能体应用：

######  1. 基于大语言模型（LLM）的任务规划 Agent 设计 

- **提示工程** ：利用 Chain-of-Thought（思维链）等提示工程技术，引导 LLM 将复杂任务分解为多个子步骤，逐步推理解决问题的路径，提升任务规划的逻辑性和可行性。
- **外部工具调用** ：集成函数 API 等外部工具，使 Agent 能够调用数据库查询、搜索引擎搜索等功能，获取外部信息辅助任务规划，拓展其知识边界和规划能力。
- **记忆管理** ：借助向量数据库（如 Pinecone）存储长期上下文信息，让 Agent 能够在任务规划过程中参考历史经验、长期目标等，实现更连贯、更全面的任务规划，避免因记忆有限导致的规划失误。

###### 2. 客服 Agent 的对话管理系统设计 

- **NLU 模块（意图识别 + 槽位填充）** ：利用 BERT 等预训练语言模型进行意图识别，判断用户当前的意图，如咨询产品、投诉问题等；同时进行槽位填充，提取用户表达中的关键实体信息，例如产品名称、问题描述等细节内容。
- **对话状态跟踪（DST）** ：实时维护用户的目标、上下文信息以及对话所处的阶段等状态信息，确保对话具有连贯性和针对性，让客服 Agent 能准确理解用户需求并做出合理的回应。
- **策略模块** ：可以基于规则（采用有限状态机来定义不同对话场景下的固定应对策略），或者采用强化学习（如 Deep Q-Networks）方法，让客服 Agent 根据对话状态和历史信息，动态选择最优的回应策略，提升对话效果。

###### 3. 在 ROS 中实现 Agent 导航模块的核心组件 

- **SLAM（如 Gmapping、Cartographer）** ：用于构建环境地图，同时对 Agent 自身在地图中的位置进行定位，为后续的路径规划提供基础地图信息和自身位姿信息。
- **路径规划（A***/D* Lite 全局规划，TEB 局部避障）** ：全局规划算法如 A*、D* Lite 等负责从起点到终点生成一条可行的全局路径，而局部避障算法 TEB 则针对实时出现的障碍物等局部情况，调整局部路径，确保 Agent 能够安全、高效地沿着规划路径行进。
- **控制（MoveBase 集成 PID 或 MPC）** ：MoveBase 作为控制模块的核心，集成 PID 控制或 MPC 控制方法，根据路径规划结果实时控制 Agent 的运动，实现精准的轨迹跟踪和避障操作。

###### 4. AI Agent 在具身智能（Embodied AI）中的关键技术 

- **多模态感知** ：融合视觉（如通过摄像头获取图像信息）、触觉（感知物体硬度、纹理等）、听觉（接收声音信号）等多种感知模态，使 Agent 能更全面、准确地感知物理世界，就像人类通过多种感官认知环境一样。
- **物理交互建模** ：利用刚体动力学仿真工具（如 PyBullet）对物理交互过程进行建模，让 Agent 能提前预判与物体碰撞、抓取等操作的结果，优化交互策略，提升在物理世界中的操作能力。
- **仿真到真实（Sim2Real）** ：采用域随机化等技术，在仿真环境中对各种环境因素（如光照、物体摩擦系数等）进行随机扰动，提升 Agent 从仿真环境迁移到真实环境时的适应能力，确保其在真实场景中也能稳定、有效地运行。

##### RAG-Fusion：

通过生成多个用户查询并重新排序结果，利用逆向排名融合和自定义向量评分加权进行综合、准确的搜索。RAG-Fusion旨在弥合用户明确询问与意图询问之间的差距，更接近于发现通常隐藏的变革性知识。常用向量数据库（Elasticsearch、Pinecone）

![RAG-Fusion图解](/media/yls/1T硬盘3/picture/RAG-Fusion.png)

##### RRF（逆向排序融合）：

$$
\text{RRFscore}(d) = \sum_{i=1}^{n} \frac{1}{k + \text{rank}_i(d)}
$$

------

###### 公式解释

1. **逆向排名贡献**
2. **平滑常数 k**
   - 作用：避免排名为1时分母过小。
   - 经验值：k=60 能平衡排名差异的敏感度，实际可调整。
3. **缺失文档处理**
4. **得分融合**
   文档的最终得分是其在所有系统中贡献值的求和。得分越高，融合后排名越靠前。

------

###### 示例说明

假设两个排序系统（n=2）和 k=60：

- 系统A 结果：`[doc1, doc2, doc3]` → 排名：`doc1=1, doc2=2, doc3=3`
- 系统B 结果：`[doc2, doc4, doc1]` → 排名：`doc2=1, doc4=2, doc1=3`

计算 RRF 得分：

| 文档 | 系统A排名 | 系统B排名 | 计算过程                             | RRFscore |
| ---- | --------- | --------- | ------------------------------------ | -------- |
| doc1 | 1         | 3         | 1/(60+1) + 1/(60+3) = 1/61 + 1/63$   | ≈ 0.0325 |
| doc2 | 2         | 1         | 1/(60+2) + 1/(60+1) = 1/62 + 1/61$   | ≈ 0.0325 |
| doc3 | 3         | 未出现    | 1/(60+3) + 1/(60+61) = 1/63 + 1/121$ | ≈ 0.0241 |
| doc4 | 未出现    | 2         | 1/(60+61) + 1/(60+2) = 1/121 + 1/62$ | ≈ 0.0241 |

**融合后排序**：`doc1 ≈ doc2` > `doc3 ≈ doc4`（同分时可进一步按原始排名细化）。

------

###### 特点与优势

- 无需分数归一化：直接使用排名，避免不同系统分数尺度差异问题。
- 强调头部文档：排名靠前的文档贡献显著，符合信息检索的典型需求。
- 鲁棒性：对噪声和部分系统缺失不敏感。
- 应用场景：搜索引擎结果融合、多召回模型排序、交叉验证模型集成等。

##### 模型微调：

###### 1. CPT（Continued Pre-Training，继续预训练）

通过无标注数据进行无监督继续预训练，强化或新增模型特定能力。

数据要求：需要大量文本数据(通常几GB到几十GB)数据质量要高，最好是你目标领域的专业内容。

适用场景：

- 让模型学习特定领域的知识，比如医学、法律、金融

- 增强模型对某种语言或方言的理解

- 让模型熟悉你所在行业的专业术语

###### 2. SFT(Supervised Fine-Tuning)监督微调

通常需要几千到几万条高质量的问答对，答案要准确、风格统一。

适用场景：

- 训练客服机器人

- 创建特定任务的助手(比如代码助手、写作助手)

- 让模型学会特定的对话风格

###### 3. DPO(Direct Preference Optimization)偏好训练

引入负反馈，降低幻觉，使得模型输出更符合人类偏好

工作原理：给模型同一个问题的两个不同答案，告诉模型哪个答案更好，让模型学会倾向于生成更好的答案。

![DPO](/media/yls/1T硬盘3/picture/DPO.png)

适用场景：

- 让模型的回答更符合人类偏好

- 减少有害内容的生成

- 提高回答的质量和安全性

##### RAG ：

**经典 Naive RAG**：以文本处理为核心，遵循 “索引 - 检索 - 生成” 的标准流程；

**Modular RAG**：相较于 Naive RAG，知识整合与检索策略更为灵活。在知识库构建阶段，需要对数据进行复杂的 Chunk 编排；检索过程中，更高级的 Modular RAG 还支持对检索结果进行预处理和后处理；

**Agentic RAG**：依托智能体架构，不仅能够高效管理私域数据检索，还配备了一套专门的工具链，极大增强了知识检索能力。

![RAG](/media/yls/1T硬盘3/picture/RAG.png)

##### Prompt Engineering 和 Context Engineering：

###### 提示词工程：

提升单次交互质量的系统性方法。通过结构化、优化和迭代提示词提高AI在特定任务上的输出质量。

![](/media/yls/1T硬盘3/picture/prompt engineering.png)

###### 上下文工程：

通过管理多维度信息（如历史对话、外部数据、工具调用），为AI提供更全面的背景，是构建智能AI系统的核心。

![](/media/yls/1T硬盘3/picture/context engineering.png)



### 31日：

##### 协程函数 async def ：

- 什么是协程函数：协程一种比传统函数更高级的控制结构，由程序员手动控制其切换，线程在操作系统级别进行调度，可能导致频繁的上下文切换开销。协程则由 Python 解释器调度，开销更低且不会发生竞争资源的问题。协程可以在程序的多个点之间切换（在一个地方暂停并在程序的另一个点恢复），从而实现并发执行达到不需要多线程以及多进程的开销。
- 什么是 async def () ：用于定义一个协程函数。
- 什么是 async for () ：用于异步迭代可等待对象的异步迭代器。类似普通 for 循环，但是可以在异步环境中使用。
- 什么是 yield ：yield 用于定义生成器函数，生成器函数在每次 yield 语句处暂停。

##### 数字人构建：

目前开源可接入智能体的数字人不满足需求。

##### 语音助手工具创建：

设置读取命令并记录到本地的 MCP ，考虑怎么样可以把它集成到语音助手中。



## 八月

### 3日：

##### 本地部署模型推理缓慢分析：

笔者是在一台虚拟机中下载了支持在虚拟机上使用的显卡驱动镜像，使用的 ollama 进行本地部署没有使用源码+权重文件部署。验证了模型运作时的显卡使用率是正常的。部署配置为CPU Intel(R) Xeon(R) Gold 6254 CPU @  3.10GHz，运行内存可用118G，显卡A100本地部署 qwen3-20b，理论上配置是足够的，但是实际运行起来推理的速度很慢。以下是优化建议：

| 优先级 |              操作项               |             说明             |
| :----: | :-------------------------------: | :--------------------------: |
|   高   |        换用 **vLLM** 部署         | 显著提升吞吐，避免Ollama瓶颈 |
|   高   |      在**宿主机裸机**上测试       |    排除虚拟机I/O延迟问题     |
|   中   |   检查CPU是否满载，考虑CPU瓶颈    |    Xeon 6254单核性能较弱     |
|   中   | 尝试更高精度量化（如Q8\_0或FP16） |    减少量化带来的计算损失    |
|   低   |    关闭思考模式（`/no_think`）    |       减少冗余推理步骤       |

###### 1. 虚拟机部署的隐藏瓶颈：PCIe直通与I/O延迟

即使GPU显存占用正常，虚拟机环境下的PCIe直通（passthrough）可能存在带宽瓶颈或延迟过高的问题，这会严重影响推理速度，尤其是像Qwen3-20B这样的大模型对数据传输延迟非常敏感。

**建议：**

- 在宿主机上直接部署测试一次，排除虚拟化带来的性能损耗。
- 检查是否启用了IOMMU、NUMA绑定、CPU pinning等优化。

######  2. Ollama默认未开启高效推理后端（如vLLM）

Ollama虽然方便，但默认使用的是transformers推理路径，并未启用像vLLM、TensorRT-LLM这类专为高吞吐优化的引擎

。这会导致：

- KV缓存未优化；
- 多卡并行效率低；
- GPU利用率虽高，但实际吞吐极低。

**建议：**

- 改用 vLLM + Docker 部署 Qwen3-20B，支持1D tensor parallelism，可显著提升推理效率

- 示例命令（单卡）：

  ```bash
  python -m vllm.entrypoints.openai.api_server \
    --model qwen3-20b \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 4096
  ```

###### 3. CPU性能瓶颈：Xeon Gold 6254 单核性能较弱

虽然有118G内存，但Qwen3-20B在推理时仍需要大量CPU参与（如tokenization、调度、KV缓存管理）。Xeon Gold 6254 是服务器级CPU，单核性能不强，主频偏低（3.1GHz），在Ollama这类非GPU极致优化的框架下容易成为瓶颈。

**建议：**

- 使用 `htop` 或 `perf top` 查看推理时CPU是否满载。
- 若CPU瓶颈明显，可尝试使用 vLLM + CUDA graph优化，减少CPU参与。



##### 提示词技巧：

| Element                   | Description                                                  | Examples                                                   | Tips               |
| ------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------ |
| Instruction 指令词        | a specific task or instruction you want the model to perform想要模型执行的特定任务或指令。 | 简述，解释，翻译，总结，生成代码 ...                       | clear and specific |
| Context 背景              | external information or additional context that can steer the model to better responses包含外部信息或额外的上下文信息，引导语言模型更好地响应。 | 我是一个小学生；你是苏格拉底...                            | Act as 扮演        |
| Input Data 输入           | the input or question that we are interested to find a response for用户输入的内容或问题。 | 总结时提供的文本；编写SQL代码时提供的数据库/表结构信息 ... | use ### or """"    |
| Output Indicator 输出要求 | the type or format of the output.指定输出的类型或格式        | 50字；4句话；以 JSON格式输出                               |                    |

1. 先提问推理模型获得结果（这一步可以多重复几次，选取出现次数最多的结果作为大模型的回答）

2. 上述方法无效，加入样本提示并尝试 1 - 5 个样本

3. 样本方式无效，尝试零样本思维链

   ```
   添加提示词  “让我们逐步思考”
   ```

4. 零样本思维链无效，尝试加入思维链中间过程

   ```
   prompt="""
   Q：我去超市买了5块蛋糕，吃掉两块，然后又买了5块，还剩多少块蛋糕？
   A：8块
   Q：我去超市买了5块蛋糕，给了邻居1块蛋糕和修理工1块。然后我去买了8块，我还剩下多少块蛋糕？
   A：11块
   Q：我去超市买了5根香蕉，丢了3根，然后吃了1根，还剩下多少根？
   A：1根
   Q：我去市场卖了10根苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了一个。我还剩下多少个苹果？
   """
   ```

   

##### 数字人创建：

**Trulience 数字人形象创建：**

密钥：实时获取

6524277332510068677

**Agora 实时语音通信参数**

- 报错 AGORA_APP_ID invalid

App ID 

c52a43192d344e27a7685790b2ce847c

Primary Certificate

5d84eac329c2487aa20058312894a3cd



### 4日：

##### 智能客服搭建：

后端实现工具调用，考虑嵌入智能客服框架中。

##### 数字人搭建：

尝试解决连接工具时出现错误。



### 5日：

##### docker 环境安全删除：

删除 GitHub 源码只是删了“启动脚本”，真正占用空间的是 Docker 的镜像、容器和数据卷。要彻底清理指定环境，必须进入其项目目录执行 docker compose down，并手动删除相关卷和镜像，才能真正释放磁盘空间。

```bash
# 1. 进入项目目录
cd /path/to/

# 2. 停止并删除容器和网络
docker compose down

# 3. 删除数据卷（关键！）
docker volume ls		# 查看有哪些卷
docker volume inspect <卷名>		# 检查每个卷是否被任何容器引用
docker volume rm <VOLUME NAME>		# 删除指定卷

# 4. 删除镜像（可选）
docker image ls		# 检查有哪些镜像，按需删除
docker rmi <REPOSITORY/TAG:ACTIVE>		# 例如docker rmi langgenius/dify-sandbox:0.2.12

# 5. 清理系统资源
docker system prune --all --force
```



##### Ten-Agent 部署数字人嵌入失败：

测试网络原因对 Ten-Agent 部署影响，发现在具备科学上网能力的电脑上部署可以正确时识别 AGORA_APP_ID 和AGORA_APP_CERTIFICATE 但是在只能访问国内网站的服务器上不能导致不能部署。如下图所示：

![image-20250805111134524](/home/yls/.config/Typora/typora-user-images/image-20250805111134524.png)



### 6日：

##### Ten-Agent Room connected 连接失败问题：

排查了核心容器是否正常工作，确认 Playground 是否正确连接 Agent 服务，测试了 Agent 在端口 8080 运行。现排查清除是网络连接问题。

- 申请 Deepgram 密钥：目前申请个人账户免费使用，商用版需要付费

  ```
  a2765fe4df4bf221f87623c908198045ec85ab9c
  ```

##### Ten-Agent 网络连接失败问题：

[官方 Github](https://github.com/TEN-framework/ten-framework/issues/372) 给出的修改意见是重新配置环境 



### 7日：

##### 解决Ten-Agent 网络连接失败问题：

修改配置文件爱你，更换拓展文件下的节点属性重新启动容器并构建 Ten-Agent，重新启动前端界面。修改 agora   

鉴权并重新启动连接成功

##### 语音助手回答问题：

语音助手存在回答问题时连同思考过程一并回答 



### 10日：

##### 小车语音助手测试：

- 设置 Insecure origins treated as secure：板子上麦克风识别不到，外接麦克风然后按如下设置重启浏览器可以正常使用语音输入功能。

使用浏览器打开下面链接，chrome://flags/#unsafely-treat-insecure-origin-as-secure

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/efb5dfacc66d4789b60bc0bc90b50f57.png)

- 语音输出功能测试：

  现在已经使用的模型

![image-20250810173737404](/home/yls/.config/Typora/typora-user-images/image-20250810173737404.png)



### 11日：

##### 服务器模型部署：

首先需要区分官方的更新驱动是对于本机的应用更新而非 docker 的镜像，其次更新 Ollama 会保留上一个版本的配置所以可以放心更新。

- Ollama 镜像更新：拉取最新的 Docker 镜像，这会把官方最新的镜像下载下来，确保容器里跑的也是新版 Ollama。

  ```bash
  sudo docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 ollama/ollamadocker pull ollama/ollama:latest
  
  # 关闭正在运行的 Ollama 镜像
  docker ps -a  # 找到旧容器ID
  docker stop <container_id>
  docker rm <container_id>
  
  # 重新启动镜像
  sudo docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
  
  # 验证容器版本
  docker exec -it <container_id> ollama --version
  ```

  

- 运行容器报错：failed to bind host port for 0.0.0.0:11434... address already in use，明确报错是由于端口已经被占用导致使用如下指令查看是哪个进程占用。

  ```bash
  sudo lsof -i :11434
  
  # 杀死进程
  sudo kill -9 PID
  ```

  1. 如果是由于非 Ollama 占用则杀死进程或更换 镜像端口号解决。

  2. 如果是因为 Ollama 占用分两种情况：

     - 均为容器打开的 Ollama 则只需要关闭之前打开的容器就好；

     - 若一个是本机打开的应用，一个是借由 Docker 打开的镜像则需要决定使用哪一种方式，笔者使用 Docker 打开。则需要进行如下设置：

       ```bash
       sudo systemctl stop ollama
       sudo systemctl disable ollama  # 可选：防止开机自启
       
       # 重新运行 Docker 指令
       sudo docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
       ```

       

##### 智能助手测试：

- 输出规范化：现有智能体对规范输出不敏感，进行 TTS 转化时将思考过程一并转化，思考如何规避这一步骤。根据 Github 官方 [Issues](https://github.com/langgenius/dify/issues/14191) 确认是由于模型供应商一般将模型的思考过程以及生成结果一并打包输出导致 TTS 转化时会读出思考过程。官方也没有进行优化。

- 测试接口调用形式：API 的未来调用应包含此 `conversation_id`，以确保与 Dify 机器人的对话连续性。但是会报错 The requested URL was not found on the server.

  ```bash
  curl -X POST 'http://192.168.1.28/v1/chat-messages' \
  --header 'Authorization: Bearer app-kPdQWVvNqI6b7zoJ2OVGUQFB' \
  --header 'Content-Type: application/json' \
  --data-raw '{
      "inputs": {},
      "query": "What are the specs of the iPhone 13 Pro Max?",
      "response_mode": "streaming",
      "conversation_id": "",
      "user": "abc-123",
      "files": [
        {
          "type": "image",
          "transfer_method": "remote_url",
          "url": "https://cloud.dify.ai/logo/logo-site.png"
        }
      ]
  }'
  ```
  
  

### 12日：

##### 智能客服 API 测试：

以下采用 python 脚本的形式测试调用 dify 接口。但是报错 {"code": "unauthorized", "message": "Access token is invalid", "status": 401}

```python
import requests
import json

url = "https://api.dify.ai/v1/completion-messages"

headers = {
    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',
    'Content-Type': 'application/json',
}

data = {
    "inputs": {"text": 'Hello, how are you?'},
    "response_mode": "streaming",
    "user": "abc-123"
}

response = requests.post(url, headers=headers, data=json.dumps(data))

print(response.text)
```



##### 开发板测试：

通过修改浏览器默认配置，将服务器地址修改为可信任地址使麦克风的权限打开并可以正常使用，但是扬声器不论是开发板内置还是外接设备都不能正常使用。



### 13日：

##### 语音助手优化：

- 使用自定义回答来规避回答包含思考过程，注意在系统提示词中添加请直接回答问题，不要展示推理过程 **/no_think** 

  ```text
  用户提问：查询合肥天气
  
  标准回答：合肥今天天气多云，气温33°C，湿度63%。建议注意防晒和补充水分哦！
  
  用户提问：可以和我聊一聊庐剧吗
  
  标准回答：庐剧是安徽省的传统戏曲剧种，起源于清代，距今已有200多年历史，被誉为“黄梅戏的前身”。它融合了民间小调、地方语言和民俗文化，具有独特的艺术魅力。
  ```

- 修改大模型关闭思考模式：使用硅基流动的调用 Qwen3-14B 大模型可以关闭思考模式回答十分简短解决了语音播报有思考过程的问题，但是可能回答的不够智能。ollama 本地部署同款模型没有看到这个选项。
- 开发板测试：目前问题输入后回答问题很快且没有输出思考过程。



##### 语音助手接口测试：

返回内容不正确，以下是部分返回值

![image-20250813173802046](/home/yls/.config/Typora/typora-user-images/image-20250813173802046.png)



### 14日：

##### 语音助手指令测试：

通过将代码移植到主控电脑的并运行智能助手识别程序做到基于关键词的指令识别，目前有机器人端以及服务器端两种指令。机器人端有方向控制以及前进、后退两种，通过以前的功能包已实现。服务器端做到部署了语音助手可以通过特定的唤醒词进行使用。



##### 智能体接口测试：

使用智能体地址 + API key 的形式安全访问服务器的目的。

- 显示拒绝连接：curl: (7) Failed to connect to 192.168.1.28 port 443 after 11 ms: 连接被拒绝
  - 解决办法：确认 Dify 服务是不是用 HTTPS（443），还是 HTTP（80），如果浏览器里用的是 `http://` 而不是 `https://`，那应该用 80 端口。

- 接口访问正常可以检查到正确访问的日志，但是目前都是流式访问获取到的结果不够直观。官方文档中注明 Agent 模式下不能进行阻塞模式访问。

  ![image-20250814161859866](/home/yls/.config/Typora/typora-user-images/image-20250814161859866.png)



- python 智能助手调用脚本：采用流式输出，然后使用 python 官方的 json 包对输出结果进行解析输出。

  ```python
  import requests
  import json
  
  url = "http://192.168.1.28/v1/chat-messages"
  headers = {
      "Authorization": "Bearer app-kPdQWVvNqI6b7zoJ2OVGUQFB",  # 替换为你的API Key
      "Content-Type": "application/json"
  }
  payload = {
      "inputs": {},
      "query": "今天杭州的天气如何？",
      "response_mode": "streaming",
      "conversation_id": "",
      "user": "decade",
      "files": [
          {
              "type": "image",
              "transfer_method": "remote_url",
              "url": "https://cloud.dify.ai/logo/logo-site.png"
          }
      ]
  }
  
  # 发起流式请求
  response = requests.post(url, headers=headers, json=payload, stream=True)
  
  answer_parts = []
  for line in response.iter_lines():
      if line:
          text = line.decode('utf-8')
          if text.startswith("data: "):
              try:
                  event = json.loads(text[6:])
                  # 提取流式返回的自然语言部分
                  if event.get("event") in ["agent_message", "message"]:
                      ans = event.get("answer", "")
                      if ans:
                          answer_parts.append(ans)
              except Exception:
                  continue
  
  # 拼接完整答案
  final_answer = ''.join(answer_parts)
  print("Agent answer:", final_answer)
  
  ```

- 现在用户输入问题需要手动：考虑做一个一直运行的程序调用语音识别以及语音输出接口来辅助 Dify 创建的智能体做到一个智能语音助手。

  

##### 语音控制打电话：

- 基于 Linphone 支持拨打电话的功能，想法是将这个功能通过 MCP 的方式接入 Dify 创建的智能体，来实现智能体语音控制打电话的功能。



### 17日：

##### 整合语音助手：

通过关键词唤醒的方式打开语音助手，解决无用语音的转录，节省 Token 支出。关键词唤醒后打开带有自动语音检测和文本转语音功能的脚本，通过访问服务器获取回答并以语音形式返回给用户。

##### 语音功能接口测试：

- 函数 create_async_playwright_browser 和函数 create_sync_playwright_browser 区别：
  1. create_async_playwright_browser 函数
     - **异步初始化：**这个函数是异步的，返回一个 async 浏览器实例。它适用于异步环境中，例如在 asyncio 事件循环中运行的代码。
     - **使用方式：**需要在 async 函数中使用 await 调用。
     - **适用场景：**当你需要在异步环境中操作 Playwright 时，例如在异步的 Web 应用程序或异步脚本中。
  2. create_sync_playwright_browser  函数
     - **同步初始化**：这个函数是同步的，返回一个同步的浏览器实例。它适用于同步环境中，例如在普通的 Python 脚本或同步函数中。
     - **使用方式**：直接调用，不需要 `await`。
     - **适用场景**：当你需要在同步环境中操作 Playwright 时，例如在普通的 Python 脚本或同步的 Web 应用程序中。

- 创建 .ipynb 文件时报错：

  ```bash
  It looks like you are using Playwright Sync API inside the asyncio loop.Please use the Async API instead
  # 考虑是因为异步环境（如 Jupyter Notebook 或其他支持 asyncio 的环境）中使用了 同步 Playwright API。Jupyter Notebook 默认运行在一个异步事件循环中，而 create_sync_playwright_browser() 是同步的，不兼容异步环境。需要改用异步版本的 Playwright 浏览器初始化函数 create_async_playwright_browser()，并确保整个代码运行在异步环境中。
  ```

- 使用 python  脚本进行服务器请成功：

  ```python
  from pathlib import Path
  from openai import OpenAI
  
  speech_file_path = Path(__file__).parent / "siliconcloud-generated-speech.mp3"
  
  client = OpenAI(
      api_key="api-key", # 从 https://cloud.siliconflow.cn/account/ak 获取
      base_url="https://api.siliconflow.cn/v1"
  )
  
  with client.audio.speech.with_streaming_response.create(
    model="FunAudioLLM/CosyVoice2-0.5B", # 支持 fishaudio / GPT-SoVITS / CosyVoice2-0.5B 系列模型
    voice="FunAudioLLM/CosyVoice2-0.5B:claire", # 系统预置音色
    # 用户输入信息
    input="合肥是安徽省的省会，位于中国东部，是一个历史悠久且发展迅速的城市。",
    response_format="mp3" # 支持 mp3, wav, pcm, opus 格式	
  ) as response:
      response.stream_to_file(speech_file_path)
  ```



### 18日：

##### 语音回复功能优化：

现在集成回复自动转语音并播放，播放后自动删除音频功能，完成 TTS 模块开发。

##### 语音识别功能开发：

借助阿里云百炼平台的接口实现自动语音识别并转化为文字，需要考虑什么是后启动程序，又在什么中断识别让大模型回答问题并返回结果。

- 启动程序：将程序做为关键词识别程序的后处理操作，即关键词触发后（基于 sherpa-onnx 的关键词识别技术）执行语音识别和后续操作。
- 识别中断：考虑设置单词识别时长，避免用户短时间内提出过多问题，或者长时间说话导致一直在执行语音识别从而不断发送给模型识别的情况。
- 开启提示：考虑设置语音助手开启的提示音。这一点可以参考阿里云百炼开启识别时输出的日志。
- VAD（静音检测）：VAD 的核心任务是**鉴别音频信号中的语音出现（speech presence）和语音消失（speech absence）**，也就是区分语音和非语音（或静音）部分。想象一下，在一个有背景噪音的环境中，VAD 就像一个智能“守门人”，它能准确识别什么时候有人在说话，什么时候是纯粹的环境噪音或沉默。



### 19日：

LangSmith API KEY ：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

LangSmith project name:  pr-cold-structure-88

阿里云 ASR 模型计费标准：[官网](https://help.aliyun.com/zh/isi/product-overview/billing-10)

![image-20250819163154752](/home/yls/.config/Typora/typora-user-images/image-20250819163154752.png) 



##### 静音检测开发：

使用两种方法尝试实现静音检测，分别是 webrtcvad 开源包检测以及基于音频能量的简易VAD。两种方法都报错没有做到语音识别，设备检测和采样率均有问题



##### 整合语音助手：

使用设置定是录制转换的检测算法作为服务器端模型输入，实现ASR自动检测。



##### 语音助手实现策略：

- ASR 检测定时发送给大模型输出答案。
- 手动录制音频到本地然后检测离线音频作为模型的输入。
- VAD 静音检测实现对于什么时候检测是么时候上传问题给大模型。



### 20日：

##### VAD 相关内容阅读：

阅读文章尝试寻找新方法实现语音识别的静音检测。



##### 语音助手整合：

- 报错：识别语音后按照设计每五秒检测并发送一次这条功能是正确的，但是请求发送给智能体这一步显示请求失败测试了将参数 timeout 依旧报错。使用 curl 请求排查发现是服务器大模型没有正确输出导致的，需要使用 docker 重新器启动容器，解决了请求超时的问题。

  ```bash
  请您通过麦克风讲话，每5秒的语音内容会作为输入发送到指定API
  [DEBUG] 新增识别内容：今天
  [DEBUG] 新增识别内容：今天，合肥天气。
  [DEBUG] 新增识别内容：今天，合肥天气。
  [DEBUG] 新增识别内容：今天，合肥天气。
  [DEBUG] 拼接后数据：[{'sentence_id': 0, 'text': '今天', 'timestamp': 1755670951.2646205}, {'sentence_id': 0, 'text': '今天，合肥天气。', 'timestamp': 1755670952.0628905}, {'sentence_id': 0, 'text': '今天，合肥天气。', 'timestamp': 1755670952.6690311}, {'sentence_id': 0, 'text': '今天，合肥天气。', 'timestamp': 1755670952.8709922}]
  [DEBUG] 拼接后的 query : 今天 今天，合肥天气。 今天，合肥天气。 今天，合肥天气。
  [ERROR] 请求失败：HTTPConnectionPool(host='192.168.1.28', port=80): Read timed out. (read timeout=10)
  [DEBUG] 新增识别内容：合肥天
  [DEBUG] 拼接后数据：[{'sentence_id': 1, 'text': '合肥天', 'timestamp': 1755670994.0657747}]
  [DEBUG] 拼接后的 query : 合肥天
  [ERROR] 请求失败：HTTPConnectionPool(host='192.168.1.28', port=80): Read timed out. (read timeout=10)
  [DEBUG] 新增识别内容：合肥天气。
  [DEBUG] 新增识别内容：合肥天气。
  [DEBUG] 新增识别内容：合肥天气。
  [DEBUG] 拼接后数据：[{'sentence_id': 1, 'text': '合肥天气。', 'timestamp': 1755671004.4271932}, {'sentence_id': 1, 'text': '合肥天气。', 'timestamp': 1755671004.4272685}, {'sentence_id': 1, 'text': '合肥天气。', 'timestamp': 1755671004.4273171}]
  [DEBUG] 拼接后的 query : 合肥天气。 合肥天气。 合肥天气。
  [ERROR] 请求失败：HTTPConnectionPool(host='192.168.1.28', port=80): Read timed out. (read timeout=10)
  ```

- 流式输出结构解析：识别的问题发送给模型回答返回的内容是流式输出，需要使用 json 包对输出的答案进行提取。

  ```python
  answer_parts = []  # 存储流式返回的 answer 片段
                  for line in response.iter_lines():
                      if line:
                          text = line.decode('utf-8')
                          if text.startswith("data: "):
                              try:
                                  event = json.loads(text[6:])  # 去除 "data: " 前缀
                                  if event.get("event") in ["agent_message", "message"]:
                                      ans = event.get("answer", "")
                                      if ans:
                                          answer_parts.append(ans)
                                          print(f"[INFO] 流式回答片段: {ans}")
                                  elif event.get("event") == "message_end":
                                      print(f"[INFO] 完整回答: {''.join(answer_parts)}")
                              except json.JSONDecodeError as e:
                                  print(f"[ERROR] JSON 解析失败: {e}")
                                  continue
              except Exception as e:
                  print(f"[ERROR] 请求失败：{e}")
  ```

- 修改回复提取逻辑以及问题生成逻辑：现有的回答均建立在持续5s的语音识别拼接基础上，涉及重复文字的拼接问题，需要改良；生成的答案由于没有完全去除思考过程需要对流式输出的内容做限定。具体如下图：

  ![image-20250820163005836](/home/yls/.config/Typora/typora-user-images/image-20250820163005836.png)

  ![image-20250820163120478](/home/yls/.config/Typora/typora-user-images/image-20250820163120478.png)



### 21日：

##### VAD 开发以及模型部署：

可以尝试使用 Webrtc-VAD、Silero-VAD、FSMN-VAD 三个开源包做静音检测，模型部署方面可以考虑本地部署 TTS、STT 模型然后借助 dify 工作流的方式使用接口调用。

##### 修复语音助手问题：

整体流程：

```
[关键词唤醒] → [启动语音识别] → [5秒后自动停止] → [发送请求] → [生成回答] → [语音播报] → [停止识别] → [退出程序]
```



- 拼接重复文字：在实时语音识别场景中，ASR（自动语音识别）系统通常会不断更新识别结果（即“中间结果”），导致短时间内出现重复或递增的文本片段（如“今天” → “今天合” → “今天合肥” → “今天合肥天气”）。而需求是**只将完整的最终识别结果发送给服务器**，避免重复和冗余。

  - 解决办法：阿里云 `dashscope` 的 `TranslationRecognizerCallback` 会为每个**完整句子**分配一个唯一的 `sentence_id`。**只有当一个句子被确认为完整时，才会生成新的 `sentence_id`**。我们可以通过判断 `sentence_id` 是否变化，来识别“完整句子”。

    ```python
            current_time = time.time()
            
            # 只保留最近5秒内的识别结果
            filtered = [item for item in self.transcriptions
                        if current_time - item["timestamp"] <= 5]
            
            if not filtered:
                return 
    
            # 核心修改：按 sentence_id 分组，取每个句子的最新文本
            sentence_dict = {}
            for item in filtered:
                sid = item["sentence_id"]
                # 只保留该 sentence_id 的最新文本（最长或最完整）
                if sid not in sentence_dict or len(item["text"]) > len(sentence_dict[sid]["text"]):
                    sentence_dict[sid] = item
    
            # 取出所有完整句子的文本（sentence_id 存在即为完整句）
            # 可以选择只发送最后一个完整句子（最符合“最终结果”）
            complete_sentences = [item["text"].strip() for item in sentence_dict.values() if item["text"].strip()]
    
            if not complete_sentences:
                return
    
            # 方案1：只发送最后一个完整句子（推荐）
            query = complete_sentences[-1]  # 只取最后一个完整句子
    ```

- 错误识别问题：模型生成回答通过接给播放音频时语音识别功能也会同步进行，这个时候会造成语音错误识别，考虑使用一次关键词唤醒只回答一个问题的方法避免这个情况的发生。

  

### 24日：

##### 回复内容优化：

- 语音回复：经过语音合成的回复会被语音识别接口错误识别然后传给模型作为新问题，考虑使用设置语音输出时间限制的方式来规避这种错误。实测虽然在设置的时间内造成问题组装的错误但是最终问题的回复上不会有问题。

  

- 回复内容：现有部署的模型 Qwen3: 14B 使用 prompt 去除思考过程不完全，会返回 <think> </think> 两个无用字段且使用 TTS 转语音识别会对这两个字段进行播报，考虑在进行流式输出解析时过滤掉这两个字段。

  - 使用通义接口使用 Qwen2: 14B 时有关闭 Thinking 模式的选项但是使用 Ollama 本地部署没有。

  ```python
  def _remove_think_tags(self, text):
    """去除文本中的<think>标签及其内容"""
    # 使用正则表达式匹配和移除<think>...</think>标签及其内容
    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
  ```

  

##### 模型剪枝：

旨在通过减少神经网络中不必要的参数和连接，来优化模型的效率和性能。剪枝可以很好地衡量模型轻量化程度与精度的关系，是替换轻量化结构完全没办法比的，大部分轻量化模块都是由时间换空间，而且精度还会下降得比较多，但是剪枝可以很好地避免这个问题。



##### 更新关键词文档：

设计语音助手唤醒词以及开场白，使用 sherpa-onnx 库添加识别打电话给儿子、女儿的关键字。

```bash
sherpa-onnx-cli text2token --tokens keyword_tokens.txt --tokens-type ppinyin keywords_raw.txt keywords.txt
```



### 25日：

##### 本地部署 Kitten TTS ：

部署完测试发现只支持英语的 Text2Speech。



##### ChatTTS 本地部署：

- 使用 requirements.txt 安装包：

  ```bash
  pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

  

### 26日：

##### 本地部署 ChatTTS：

部署环境 ubuntu22.04 ，使用 git 克隆官方代码，创建虚拟环境，安装必要的库。下载模型文件编写测试脚本。

```bash
# 克隆原始项目
git clone https://github.com/2noise/ChatTTS 	# 这一步如果有外网的网络条件可以选择 lfs 下载，注意使用时需要提前安装

# 安装指令
sudo apt-get install git-lfs
git lfs install

# 创建虚拟环境
conda ceate -n chattts python=3.10 -y
conda activate chattts

# 按照 requirements.txt 下载需要的库，国内可选择清华源或者阿里源
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 下载模型文件，注意这里不是下载项目源码，地址一定选择 huggingface 不是 github
git clone https://huggingface.co/2Noise/ChatTTS

# 测试指令
python tts_test.py --text "ChatTTS 是一个开源的文本转语音大模型，支持中文和英文。" --seed 1234 --speed 1.1 --out ./demo.wav
```

```python
# 使用最简单的测试脚本
import torch
import ChatTTS
from IPython.display import Audio

# 初始化ChatTTS
chat = ChatTTS.Chat()
chat.download_models()

# 定义要转换为语音的文本
texts = ["你好，欢迎使用ChatTTS！"]

# 生成语音
wavs = chat.infer(texts, use_decoder=True)

# 播放生成的音频
Audio(wavs[0], rate=24_000, autoplay=True)
```



##### RAG 后端实现：

`Streamlit`前端界面，结合`LangChain`框架`retriever`工具与`DashScope`向量模型服务、`DeepSeek`大模型服务，从0到1实现了轻量化的RAG知识库系统，支持上传多个PDF文档，系统将自动完成文本提取、分块、向量化，并构建基于 FAISS 的检索数据库。用户随后可以在页面中输入任意问题，系统会调用大语言模型（如 DeepSeek-Chat）对 PDF 内容进行语义理解和回答生成。



### 27日：

##### 实时因子（RTF）：

通常用实时率(Real Time Factor, RTF)来衡量识别的速度，实时率等于识别花的时间除以语音本身的时间。实时率为1就表示用户一说完话结果就能出来(前提是忽略假设录音实时的传给语音识别系统，时间情况很多时候是在服务器端进行解码的，因此会有网络的延迟)。实时率大于1就表示话说完了，系统还得再处理一段时间。而实时率小于1表示识别速度比说话速度快，这样万一有网络延迟，它还能追上来。

解释 RTF 值：

- RTF < 1：实时或更快的生成。生成语音所需时间少于语音时长。比如 RTF = 0.5 意味着生成一分钟语音只需要 30 秒。
- RTF > 1：非实时。生成语音所需时间长于语音时长。比如 RTF = 2 意味着生成一分钟语音需要 2 分钟。

RTF 越低，意味着 TTS 系统生成语音的速度越快。实时因子低的 TTS 系统在需要实时响应的应用场景（如语音助手）中尤为关键。



##### ChatTTS 本地部署：

尝试使用不同版本的源码以及手动下载和脚本下载模型文件的方式进行模型部署，最后要么卡在下载失败（切换下载源解决），要么卡在函数缺失（检查发现是官方在类中的函数命名已经修改但是官方文档中中文说明文档没有说明而且官方示例也没有使用新的函数名），要么卡在一些看不懂的错误上。



### 28日：

##### ChatTTS 本地部署文档书写：

##### 创建语音转文字服务：

- 创建访问密钥：

  ```bash
  export API_KEYS="iCare20250828,iCare20250831"
  ```

  

### 31日：

##### 库名与文件名重复问题：

文件的名字跟真正的 `langchain` 三方库同名，导致 `import` 时 Python 先找到并加载了 `langchain.py`，于是找不到真正的 `langchain.agents` 模块了。这是一个常见的python陷阱，其中本地文件名与第三方软件包的名称相撞。

```
File "/media/yls/1T硬盘4/code/Agent/LangChain-MCP/langchain.py", line 3, in <module>
    from langchain.agents import create_tool_calling_agent, AgentExecutor
ModuleNotFoundError: No module named 'langchain.agents'; 'langchain' is not a package
```

- 解决办法：直接修改文件名保证与第三方库名不相同，删除缓存文件重新打开 vscode 运行即可。



##### API 服务搭建：

- 测试适用本地服务：在 postman 发出的请求问题一直出在缺少各种不同的字段。

  ![image-20250831143813385](/home/yls/.config/Typora/typora-user-images/image-20250831143813385.png)

  

  main.py 脚本如下：

  ```python
  from fastapi import FastAPI, HTTPException
  from fastapi.responses import FileResponse
  from pydantic import BaseModel
  import numpy as np
  from wave import Wave_write
  import os
  from types import SimpleNamespace
  
  import ChatTTS
  chat = ChatTTS.Chat()
  chat.load(compile=False)
  
  app = FastAPI()
  
  class AudioRequest(BaseModel):
      text: str
      params_refine_text: dict
  
  @app.post("/generate-audio/")
  def generate_audio(request: AudioRequest):
      try:
          # 修正：将 dict 转为对象
          params_refine_text_obj = SimpleNamespace(**request.params_refine_text)
          wavs = chat.infer(request.text, params_refine_text=params_refine_text_obj)[0]
          sample_rate = 24000
          audio_data_rescaled = (wavs * 28000).astype(np.int16).flatten()
  
          os.makedirs('audio_files', exist_ok=True)
          file_path = 'audio_files/test.wav'
  
          with Wave_write(file_path) as wave_file:
              wave_file.setparams((1, 2, sample_rate, len(audio_data_rescaled), 'NONE', 'not compressed'))
              wave_file.writeframes(audio_data_rescaled.tobytes())
  
          return FileResponse(path=file_path, filename="test.wav", media_type='audio/wav')
      except Exception as e:
          raise HTTPException(status_code=500, detail=str(e))
  
  if __name__ == "__main__":
      import uvicorn
      uvicorn.run(app, host="0.0.0.0", port=8000)
  ```

  

- 官方示例接口：在项目源码中 examples/api 文件夹下有示例接口声明文件，文件目录结构如下：

  ```
  ├── client.py
  ├── main.py
  ├── openai_api.py
  ├── postScript.py
  ├── README.md
  └── requirements.txt
  ```

  

## 九月

### 1日：

##### 模型加载运行：

虚拟机内网络环境差需要自行下载模型文件并放入指定文件夹内 ~/.cache/huggingface/hub/models--2Noise--ChatTTS 中再重新启动接口服务。具体模型放置目录结构如下：

```bash
├── blobs
│   ├── 12d59e7d0af9ccfd5deb4ec01b4db3855f3d7314
│   ├── 5ea569e3431b0ed2aa1c699461017c7174d2f56d
│   ├── 8df13367906f6cd6b1f88b3cc6f1f15599b19e94
│   ├── 9c7b3d09af3f9fea19072d4a35aecee15779f51c
│   ├── b62fb7fbd3c9b91498b869b32343642d03a25fc0
│   └── be32c1231832c60ddad7e0c2e8bd027f51a183b2
├── refs
│   └── main
└── snapshots			# 用于存放模型文件
    └── 1a3c04a8b0651689bd9242fbb55b1f4b5a9aef84
        ├── asset
        │   ├── Decoder.pt
        │   ├── Decoder.safetensors
        │   ├── DVAE_full.pt
        │   ├── DVAE.pt
        │   ├── DVAE.safetensors
        │   ├── Embed.safetensors
        │   ├── gpt
        │   │   ├── config.json
        │   │   └── model.safetensors
        │   ├── GPT.pt
        │   ├── spk_stat.pt
        │   ├── tokenizer
        │   │   ├── special_tokens_map.json
        │   │   ├── tokenizer_config.json -> ../../../../blobs/b62fb7fbd3c9b91498b869b32343642d03a25fc0
        │   │   └── tokenizer.json
        │   ├── tokenizer.pt
        │   ├── Vocos.pt
        │   └── Vocos.safetensors
        └── config
            ├── decoder.yaml -> ../../../blobs/9c7b3d09af3f9fea19072d4a35aecee15779f51c
            ├── dvae.yaml -> ../../../blobs/8df13367906f6cd6b1f88b3cc6f1f15599b19e94
            ├── gpt.yaml -> ../../../blobs/be32c1231832c60ddad7e0c2e8bd027f51a183b2
            ├── path.yaml -> ../../../blobs/5ea569e3431b0ed2aa1c699461017c7174d2f56d
            └── vocos.yaml -> ../../../blobs/12d59e7d0af9ccfd5deb4ec01b4db3855f3d7314
```



在项目源码找到 FastAPI 启动程序 main.py 启动接口服务，具体步骤如下：

```bash
# 首先检查模型文件是否正确
ls ~/.cache/huggingface/hub/models--2Noise--ChatTTS/snapshots/1a3c04a8b0651689bd9242fbb55b1f4b5a9aef84/asset/Vocos.safetensors

# 如果输出文件存在运行
fastapi dev main.py
```



##### 模型接口测试：

使用官方给出的测试脚本发现一段 9s 的生成时间差不多需要1分钟，测试脚本如下：

```python
import datetime
import os
import zipfile
from io import BytesIO

import requests

chattts_service_host = os.environ.get("CHATTTS_SERVICE_HOST", "localhost")
chattts_service_port = os.environ.get("CHATTTS_SERVICE_PORT", "8000")

CHATTTS_URL = f"http://{chattts_service_host}:{chattts_service_port}/generate_voice"


# main infer params
body = {
    "text": [
        "四川美食确实以辣闻名，但也有不辣的选择。",
        "比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。",
    ],
    "stream": False,
    "lang": None,
    "skip_refine_text": True,
    "refine_text_only": False,
    "use_decoder": True,
    "audio_seed": 12345678,
    "text_seed": 87654321,
    "do_text_normalization": True,
    "do_homophone_replacement": False,
}

# refine text params
params_refine_text = {
    "prompt": "",
    "top_P": 0.7,
    "top_K": 20,
    "temperature": 0.7,
    "repetition_penalty": 1,
    "max_new_token": 384,
    "min_new_token": 0,
    "show_tqdm": True,
    "ensure_non_empty": True,
    "stream_batch": 24,
}
body["params_refine_text"] = params_refine_text

# infer code params
params_infer_code = {
    "prompt": "[speed_5]",
    "top_P": 0.1,
    "top_K": 20,
    "temperature": 0.3,
    "repetition_penalty": 1.05,
    "max_new_token": 2048,
    "min_new_token": 0,
    "show_tqdm": True,
    "ensure_non_empty": True,
    "stream_batch": True,
    "spk_emb": None,
}
body["params_infer_code"] = params_infer_code


try:
    response = requests.post(CHATTTS_URL, json=body)
    response.raise_for_status()
    with zipfile.ZipFile(BytesIO(response.content), "r") as zip_ref:
        # save files for each request in a different folder
        dt = datetime.datetime.now()
        ts = int(dt.timestamp())
        tgt = f"./output/{ts}/"
        os.makedirs(tgt, 0o755)
        zip_ref.extractall(tgt)
        print("Extracted files into", tgt)

except requests.exceptions.RequestException as e:
    print(f"Request Error: {e}")

```



### 2日：

##### 紧急呼救关键词：

在开发板上测试了常见的一些呼救词，目前实现识别并发送呼叫信息。现有指令内容如下：

```
# 语音助手唤醒词
小明你好

# 小车控制指令
向前
向后
向左
向右
前进
后退
跟我走

# 语音控制打电话
打给儿子
打给女儿

# 紧急呼救类
救命
着火
有人需要帮助
快报警
我不行了快叫救护车
```



##### 优化 ChatTTS 本地部署文档：

将 FastAPI 主程序启动方式以及测试脚本测试结果写入文档。	



##### 关键词唤醒于语音助手整合测试：

- 音频设备检测报错：外界的音频设备没有检测到，导致程序运行是没有问题输入。通过强制指定设备解决输入问题，但是生成的音频不能播放。具体报错信息以及暂时解决方案如下：

  ```bash
  TranslationRecognizerCallback open.
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.front
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround40
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround41
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround50
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround51
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround71
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958
  ALSA lib confmisc.c:1369:(snd_func_refer) Unable to find definition 'cards.0.pcm.hdmi.0:CARD=0,AES0=4,AES1=130,AES2=0,AES3=2'
  ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory
  ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM hdmi
  ALSA lib confmisc.c:1369:(snd_func_refer) Unable to find definition 'cards.0.pcm.hdmi.0:CARD=0,AES0=4,AES1=130,AES2=0,AES3=2'
  ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory
  ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM hdmi
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline
  ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp
  ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp
  ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card
  ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'
  ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card
  ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'
  
  
  # 查看设备状态
  arecord -l
  
  # 示例输出
  card 3: rockchipi2s [rockchip-i2s], device 0: HiFi audio-i2s-0 []  # 关注 card 后面的数字修改参数
    Subdevices: 1/1
    Subdevice #0: subdevice #0
  ```

  ```python
  # 参数修改
  def on_open(self) -> None:
      global mic, stream
      print("TranslationRecognizerCallback open.")
      mic = pyaudio.PyAudio()
      # 根据 arecord -l 的输出调整设备索引
      stream = mic.open(
          format=pyaudio.paInt16,
          channels=1,
          rate=16000,
          input=True,
          input_device_index=3  # 替换为实际设备索引
      )
  ```




### 3日：

##### 关键词唤醒于语音助手整合测试：

完成语音助手开发板测试，中间遇到音频生成但是播放不成功。可能使用是因为声卡驱动问题也可能是设备未被检测到或是声音太小导致播放音频没有声音。下面列出常见的错误排查方法：

1. 先确认最简单的：系统里到底有没有声卡，以及当前用户有没有权限

   ```bash
   # 终端执行
   aplay -l
   ```

   - 如果提示 “no soundcards found…” → 说明系统根本没识别出声卡，先解决驱动/硬件问题，后面的步骤都不用看了。

   - 正常会列出类似：

     ![image-20250903094014405](/home/yls/.config/Typora/typora-user-images/image-20250903094014405.png)

     

     1.2 再确认当前用户是否在 `audio` 组，执行完成后重新登录或重启。

     ```
     groups $USER
     
     # 如果没有 audio，执行
     sudo usermod -a -G audio $USER
     ```



2. 安装/切换播放后端（99% 的 Linux 桌面系统已经自带 PulseAudio 或 PipeWire）

   pydub 默认会依次尝试：
   simpleaudio → pyaudio → ffplay → avplay。
   在 Linux 上最稳、最不需要折腾的是让 pydub 直接走 PulseAudio（或 PipeWire），而不是裸 ALSA。

   - 装 PulseAudio 命令行工具（很多发行版默认已装）

     ```bash
     sudo apt install pulseaudio pulseaudio-utils
     ```

   - 让 pydub 强制使用“pulse”设备（最简单的方法：加两行环境变量）
     在运行脚本前临时加：

     ```bash
     export AUDIODEV=pulse
     export AUDIODRIVER=pulse
     python your_script.py
     ```

     或者写在 Python 里（放在 import 之前）：

     ```python
     import os
     os.environ["AUDIODEV"] = "pulse"
     os.environ["AUDIODRIVER"] = "pulse"
     ```

     

3. 终极兜底：让 pydub 用 ffplay / ffprobe（完全绕开 ALSA）

   - 与上面安装 PulseAudio 工具一样，Ubuntu/Debian 版本安装命令如下：

     ```
     sudo apt install ffmpeg
     ```

   - 测试脚本：

     ```python
     from pydub import AudioSegment
     from pydub.playback import _play_with_ffplay
     
     speech_file_path = "temp_speech_1756804721.mp3"
     audio = AudioSegment.from_mp3(speech_file_path)
     _play_with_ffplay(audio)
     
     # 或者直接用 subprocess 调用 ffplay
     import subprocess, os
     subprocess.run(["ffplay", "-nodisp", "-autoexit", "temp_speech_1756804721.mp3"])
     ```



4. 如果不是以上配置问题，检查是不是因为系统混音器声音设置偏低，或者宿主机屏幕带有声卡而输出与输出选择的声卡不匹配导致的音频在播放但是没有声音。具体检查方法如下：

   - 系统混音器是不是静音了

     ```
     # 终端执行
     alsamixer          # 键盘方向键左右选通道，m 键取消 Mute，↑ 调大音量
     
     # 如果用的是 PulseAudio，就是图形化界面管理
     pavucontrol        # 图形界面，把 Playback / Output 都拉到 100%
     ```

   - 确认 ffplay 实际走的是哪个声卡：有的机器有多块声卡（HDMI、主板集成声卡、USB 耳机等），ffplay 可能默认把声音输出到 HDMI，而你的显示器又没接音箱，于是听不到。

     ```bash
     # 这一条指令可以显示设备所有的声卡，但是具体切换不会
     ffplay -nodisp -autoexit -devices
     ```

     

##### 紧急呼救关键词：

使用 sherpa-onnx 进行关键词识别



##### 语音助手关键词唤醒：

整体实现逻辑是在关键词检测时，当检测到特定的语音助手唤醒词后启动外部语音助手服务。由于关键词检测脚本持续启动而虽然语音服务每一次只进行一次对话，但是只要用户希望对话直接叫机器人"名字"就可以实现持续对话。

```python
# 在检测到关键词后，启动外部语音助手脚本。将以下代码插入到 audio_callback 中：
def audio_callback(self, msg):
    self.frames = self.frames[-32000:]
    self.frames.extend(msg.data)
    try:
        s = self.keyword_spotter.create_stream()
        s.accept_waveform(16000, np.array(self.frames, dtype=np.float16))
        s.input_finished()
        while True:
            if not self.keyword_spotter.is_ready(s):
                r = self.keyword_spotter.get_result(s)
                if r:
                    self.get_logger().info(f"{r} is detect")
                break
            r = self.keyword_spotter.get_result(s)
            if r:
                self.get_logger().info(f"Detection Keyword:{r}")
                # 创建 String 类型消息并发布
                msg_out = String()
                msg_out.data = r
                self.publisher.publish(msg_out)

                # 启动外部语音助手脚本
                self._start_assistant_script()

                break
            self.keyword_spotter.decode_streams([s])

    except Exception as e:
        logger.info(f"stopping by user: {e}")
        
        
# 在 KeywordSpotter 类中新增方法，用于启动外部脚本：
def _start_assistant_script(self):
    """启动外部语音助手脚本"""
    import subprocess
    import threading

    def run_script():
        try:
            # 替换为实际路径
            script_path = "/path/to/assistant_script.py"
            subprocess.Popen(["python3", script_path])
            self.get_logger().info("已启动语音助手脚本")
        except Exception as e:
            self.get_logger().error(f"启动语音助手脚本失败: {e}")

    # 使用线程避免阻塞 ROS2 主线程
    thread = threading.Thread(target=run_script, daemon=True)
    thread.start()
```



### 4日：

##### 语音打电话功能实现：

暂时商量的是使用 "打电话给儿子， 打电话给女儿" 两个词来测试，后面可以考虑用户自定义关键词来作为语音控制打电话的对象。



##### 紧急呼救关键词开发完成：

针对常见呼救关键词进行识别并在开发板上完场测试。



### 7日：

##### 下载火焰识别数据集：

寻找 YOLO 支持的数据集并根据demo查看模型的检测效果。了解其他可以在嵌入式开发板上实现并部署的火焰检测方案。



##### 火焰检测模型训练：

一轮训练结束，考虑对模型进行目标测试，设计易混物体比如灭火器。



### 8日：

##### 火焰识别模型测试：

测试结果显示一般，检测出的火焰目标存在置信度偏低的情况。另外测试图片中有一张完全没有检测到，但是没有对灭火器进行误检。



##### 功能包封装测试：

完成模型转换并将火焰检测模型放进开发板测试，但是出现所有检测对象的置信度均为0.5的 bug ，怀疑是因为模型的识别精度不够导致触发设定的最低重新检查模型检测逻辑。现在基本确定是由于检测到的物体置信度太低导致的。模型转换是损失太多精度导致开发板上模型的识别效率低，置信度低。

- 配置文件：

  ```yaml
  fire_detect_model: '/rknn_model/fire_best.rknn'
  
  model_w: 640
  model_h: 640 
  objectThresh: 0.1		# 置信度阈值，当把这个阈值调高以后输出结果默认置信度为0.5
  nmsThresh: 0.65
  
  classes:
    - "fire"
  ```

- 功能包下 utils 文件夹下 rknn_utils.py 文件中对低置信度目标设置的过滤机制，简单的来说就是当模型检测的物体置信度低于配置文件中设置的 objectThresh 则自动视为未检测到目标。

  ```python
  def filter_boxes(boxes, box_confidences, box_class_probs, objectThresh, keypoints=None):
      """Filter boxes with object threshold.
      """
      box_confidences = box_confidences.reshape(-1)
      candidate, class_num = box_class_probs.shape
  
      class_max_score = np.max(box_class_probs, axis=-1)
      classes = np.argmax(box_class_probs, axis=-1)
  
      _class_pos = np.where(class_max_score * box_confidences >= objectThresh)
      scores = (class_max_score * box_confidences)[_class_pos]
  
      boxes = boxes[_class_pos]
      classes = classes[_class_pos]
      if keypoints is not None:
          keypoints = keypoints[_class_pos]
          return boxes, classes, scores, keypoints
  
      return boxes, classes, scores
  ```

  

- ROS2 节点文件：文件中设置了目标检测输出内容以及对于低置信度目标的处理办法，即强制设置为0.5.

  ```python
      def image_callback(self, msg):
          # 图像大小640*480（w*h）
          # ros2消息类型(imgmsg)转换为np.array
          cv_img = bridge.imgmsg_to_cv2(msg, "bgr8")
          boxes, scores, classes = self.fire_detect.rknn_inference(cv_img)
          # print(emotions, scores)
          if boxes is None:
              self.msg.data = '无目标'
              logger.info('未检测到目标')
          else:
  
              for box, score, cl in zip(boxes, scores, classes):
                  top, left, right, bottom = [int(_b) for _b in box]
  
                  color = (0, 255, 0)
                  # color = (0, 255, 127)
  
                  # Draw the bounding box on the image
                  cv2.rectangle(cv_img, (top, left), (right, bottom), color, 2)
  
                  # Create the label text with class name and score
                  label = f'{self.CLASSES[cl]}: {score:.2f}'
  
                  # Calculate the dimensions of the label text
                  (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX,
                                                                   0.5, 1)
  
                  # Calculate the position of the label text
                  label_x = top
                  label_y = left - 10 if left - 10 > label_height else left + 10
  
                  # Draw a filled rectangle as the background for the label text
                  cv2.rectangle(cv_img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height),
                                color, cv2.FILLED)
  
                  # Draw the label text on the image
                  cv2.putText(cv_img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX,
                              0.5, (0, 0, 0), 1, cv2.LINE_AA)
  
          # 显示图像
          cv2.imshow('Image', cv_img)
          cv2.waitKey(1)  # 等待按键事件，1毫秒
          self._publisher.publish(self.msg)
  ```

  修改为下面的代码验证上述的错误分析:

  ```python
  def image_callback(self, msg):
          # 图像大小640*480（w*h）
          # ros2消息类型(imgmsg)转换为np.array
          cv_img = bridge.imgmsg_to_cv2(msg, "bgr8")
          boxes, scores, classes = self.fire_detect.rknn_inference(cv_img)
  
          # 添加调试打印
          print(f"DEBUG: boxes={boxes}, scores={scores}, classes={classes}")
  
          # 初始化消息为无目标
          self.msg.data = '无目标'
          has_high_confidence_target = False
      
          # 即使boxes不为None，也可能都是低置信度的框
          if boxes is not None and len(boxes) > 0:
              for box, score, cl in zip(boxes, scores, classes):
                  top, left, right, bottom = [int(_b) for _b in box]
              
                  # ！！！新增：设置一个高置信度阈值来判断是否真的是目标！！！
                  high_confidence_threshold = 0.5  # 您可以调整这个值
              
                  if score >= high_confidence_threshold:
                      has_high_confidence_target = True
                      # 只有高置信度目标才更新消息
                      self.msg.data = f'检测到{self.CLASSES[cl]}, 置信度:{score:.2f}'
  
                  # 无论置信度高低，都画框（这是您原来的逻辑）
                  color = (0, 255, 0) if score >= high_confidence_threshold else (0, 0, 255)  # 高置信度绿色，低置信度红色
              
                  # Draw the bounding box on the image
                  cv2.rectangle(cv_img, (top, left), (right, bottom), color, 2)
              
                  # Create the label text with class name and score
                  label = f'{self.CLASSES[cl]}: {score:.2f}'
              
                  # Calculate the dimensions of the label text
                  (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX,
                                                               0.5, 1)
              
                  # Calculate the position of the label text
                  label_x = top
                  label_y = left - 10 if left - 10 > label_height else left + 10
              
                  # Draw a filled rectangle as the background for the label text
                  cv2.rectangle(cv_img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height),
                            color, cv2.FILLED)
              
                  # Draw the label text on the image
                  cv2.putText(cv_img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX,
                          0.5, (0, 0, 0), 1, cv2.LINE_AA)
      
          # 根据是否有高置信度目标来记录日志
          if has_high_confidence_target:
              logger.info(f'检测到目标: {self.msg.data}')
          else:
              logger.info('未检测到高置信度目标')
      
          # 显示图像
          cv2.imshow('Image', cv_img)
          cv2.waitKey(1)  # 等待按键事件，1毫秒
          self._publisher.publish(self.msg)
  ```



### 9日：

##### 重新训练模型：

学习调整训练参数，替换数据集重新训练模型验证模型的检测精度。

- lr0：该参数设置优化器的初始学习率。指定学习率为 0.0001，这是许多优化任务的常见起始值。这一点针对小模型例如 YOLOv8n 设置的初始学习率调小。

- lrf：该参数指定最终学习率，其计算方式为初始学习率乘以lrf。它有助于在训练过程中逐渐降低学习率以稳定学习过程。（`最终学习率 = lr0 × lrf`）

  ```python
  # 采用余弦退火法并修改了 lr0 和 lrf
  lr0=0.1,
  lrf=0.001,
  cos_lr=True,
  
  # 结果此轮调整收益很高但是，精度最终也只在0.7左右
  ```

  

https://github.com/Username378/Fire_smoke_monitoring_system/blob/master/models/new.pt



##### 优化模型识别脚本：

ros2 节点文件中设置的检测逻辑是初始开启检测到物体终端无返回值但是如果初始为检测到物体就会一直显示未检测到目标，检查发布节点发布内容发现不受影响所以未修改这部分逻辑，在发布者节点中检查了发送内容不符合阅读逻辑，将发送内容修改为（类别 + 数量）分别显示。具体修改如下：

```python
def image_callback(self, msg):
    cv_img = bridge.imgmsg_to_cv2(msg, "bgr8")
    boxes, scores, classes = self.pet_detect.rknn_inference(cv_img)

    if boxes is None:
        self.msg.data = '无目标'
        logger.info('未检测到目标')
    else:
        # 统计每个类别的数量
        from collections import Counter
        class_counts = Counter([self.CLASSES[cl] for cl in classes])
        
        # 构建详细的消息
        detection_info = []
        for class_name, count in class_counts.items():
            detection_info.append(f"{class_name}:{count}个")
        
        self.msg.data = f'检测到目标: {", ".join(detection_info)}'

        for box, score, cl in zip(boxes, scores, classes):
            top, left, right, bottom = [int(_b) for _b in box]
            color = (0, 255, 0)
            cv2.rectangle(cv_img, (top, left), (right, bottom), color, 2)
            label = f'{self.CLASSES[cl]}: {score:.2f}'
            # ...（其余绘制代码）

    cv2.imshow('Image', cv_img)
    cv2.waitKey(1)
    self._publisher.publish(self.msg)
```

 

##### 配置新板子运行环境：

```bash
pip install PyAudio==0.2.14 -i https://pypi.tuna.tsinghua.edu.cn/simple 
Defaulting to user installation because normal site-packages is not writeable
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple 
Collecting PyAudio==0.2.14
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/26/1d/8878c7752febb0f6716a7e1a52cb92ac98871c5aa522cba181878091607c/PyAudio-0.2.14.tar.gz  (47 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Building wheels for collected packages: PyAudio
  Building wheel for PyAudio (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Building wheel for PyAudio (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [17 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.linux-aarch64-3.10
      creating build/lib.linux-aarch64-3.10/pyaudio
      copying src/pyaudio/__init__.py -> build/lib.linux-aarch64-3.10/pyaudio
      running build_ext
      creating build/temp.linux-aarch64-3.10
      creating build/temp.linux-aarch64-3.10/src
      creating build/temp.linux-aarch64-3.10/src/pyaudio
      aarch64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/include -I/usr/include -I/usr/include/python3.10 -c src/pyaudio/device_api.c -o build/temp.linux-aarch64-3.10/src/pyaudio/device_api.o
      src/pyaudio/device_api.c:9:10: fatal error: portaudio.h: No such file or directory
          9 | #include "portaudio.h"
            |          ^~~~~~~~~~~~~
      compilation terminated.
      error: command '/usr/bin/aarch64-linux-gnu-gcc' failed with exit code 1
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for PyAudio
Failed to build PyAudio
ERROR: Could not build wheels for PyAudio, which is required to install pyproject.toml-based projects
```

说明 PyAudio 编译时找不到 `portaudio.h` 头文件，这是因为系统中没有安装 PortAudio 的开发库。

- 解决办法：

  ```bash
  # 对于 Ubuntu
  sudo apt update
  sudo apt install portaudio19-dev
  
  # 重新安装 PyAudio
  pip install PyAudio==0.2.14 -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

  

### 10日：

##### rknn 工具链安装：

注意要看安装的工具链版本，首先是看系统是不是 aarch64 架构，其次查看 python 版本号是否匹配。

```bash
# 确认系统架构和 Python 版本
uname -m
python3 --version

# 安装工具链
pip install rknn_toolkit_lite2-2.3.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl
```



##### 功能包测试：

在新开发板上进行了关于视觉识别的测试，其中由于没有显示器人体跟踪无法测试。音频检测缺少显示器也无法测试。

```bash
qt.qpa.xcb: could not connect to display :0
qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "/home/ubt/.local/lib/python3.10/site-packages/cv2/qt/plugins" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: xcb.
```



现有完成测试如下：

- 流血检测：功能正常但是检测精度低，需要持续获取真实数据然后再训练模型检测。

  ![image-20250910102247487](/home/yls/.config/Typora/typora-user-images/image-20250910102247487.png)

- 宠物识别：正常
- 表情识别：正常



##### 火焰识别模型训练：

调整了模型训练参数但是模型识别的精度依旧只有0.7左右。



### 11日：

##### 模型训练脚本问题：

训练脚本和指令存在严重的冲突和配置问题，这直接导致了训练失效。关键在于300轮训练每一轮的损失都很大，一般来说模型训练50轮左右损失会大幅降低。

同时使用了两种配置方式，导致了冲突：

- **Python脚本**：在 `model.train()` 中设置了大量参数（如 `batch=64`, `data=...`, `device=...`）。

- **命令行指令**：又通过命令行传递了另一套参数（如 `--batch 64`, `--data ...`, `--device 0,1`）。

**最关键的是**，您的命令行指令使用了 `-m torch.distributed.run` 进行分布式训练，但您的Python脚本**并没有为分布式训练做任何准备**。这几乎是导致您训练失败的首要原因。



##### 模型注意力机制优化：

通过整合Ghost模块[[](https://www.hanspub.org/journal/paperinformation?paperid=92626#ref2)[2](https://www.hanspub.org/journal/paperinformation?paperid=92626#ref2)]和卷积块注意力机制(CBAM) [[](https://www.hanspub.org/journal/paperinformation?paperid=92626#ref3)[3](https://www.hanspub.org/journal/paperinformation?paperid=92626#ref3)]，对YOLOv8s模型进行了优化。Ghost模块通过分组卷积和线性变换的方式生成更多特征，从而在降低计算复杂度的同时保持模型的检测性能。卷积块注意力机制(CBAM)则通过引入空间和通道注意力，有效增强了模型对火灾特征的捕捉能力。



##### 损失问题、精度问题：

- 在初始训练中损失一直很大且一直降不下来。智能观察模型训练期间有没有过拟合表现，如果有就要停止训练然后再修改配置或者更换数据集重新训练。
  1. **box_loss** (边界框损失)： 衡量预测框与真实框之间的位置和形状差异（通常使用CIoU、DIoU等）。值越高，说明模型定位不准。
  2. **cls_loss** (分类损失)： 衡量预测的类别概率与真实标签之间的差异（通常使用交叉熵）。值越高，说明模型分类不准。
  3. **dfl_loss** (分布焦点损失)： 这是YOLOv8引入的改进。它不再直接回归框的宽高，而是回归一个分布，让模型更专注于学习边界附近难以区分的样本。这个值通常与其他损失在一个量级上。

- 模型经过格式转换和量化后精度太低了以至于触发了配置文件中最低置信度。修改配置文件中的置信度阈值发现对于置信度本身小于阈值的检测目标时可以正常识别的且误检率很低，但是对于高于阈值的目标置信度都是阈值归咎于模型的识别精度不够。



##### 智能体构建问题：

- 大模型回答问题缓慢：ollama部署模型导致模型运行效率不佳，这个可以考虑使用 vLLM 本地部署大模型。
- 语音回复问题：ollama不支持关闭模型思考模式导致智能体语音回复时播报思考内容，通过正则表达式过滤调带有 </think><think>标签的内容，另一方面可以在智能体的系统提示词上手动设置不显示思考内容。

​	
