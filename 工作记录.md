# 2025年

## 五月

### 27日：

- **摄像头测试：**orangepi4连接摄像机显示屏测试什么时候会断连
  - 开机不停跳转登陆界面：ctrl + alt + f1进入终端，输入用户用户密码，reboot重启
  - 链接网络，到这个网页测试https://www.onlinemictest.com/zh/webcam-test/
  - orangepi4启用摄像头测试温度异常

### 28日：

- 输入法切换：
- **预训练过程：**数据准备阶段，准备供语言模型学习的庞大数据，数据清洗除去低质量数据，另外可输入特定领域的文本供学习。训练阶段学习人类语言，学习语法句法，拥有理解和生成上下文的能力，获得常识某专业领域知识回答问题
  - 加速预训练方案：Net2Net，StackBert，bert2BERT，LiGO，LEMON、MSG 和 Mango ，采用分布式策略
  - 预训练难点：预训练数据量庞大，需要进行数据过滤，除去低质量数据。
  - 预训练模式：常规预训练，长上下文训练即预训练结束阶段将上下文长度从4,096个数据增加到32,768个数据，使用的是“高质量、长篇数据”。

- **语言模型：**帮助系统理解并预测给定上下文中的下一个词的可能性。语言模型的准确度直接影响到语音识别的性能，尤其是在处理长句子和复杂语境时。通过评估不同词序列的概率，语言模型能够辅助声学模型，从多个可能的识别结果中选择最合理的文本输出。
  - **作用原理：**语言模型通过学习大量的文本数据，统计词的出现频率和词序列的模式，来估计词序列的概率。在语音识别中，声学模型负责将音频信号转换为一系列的音素或词的候选，而语音模型则根据这些候选生成的词序列，计算其概率，从而帮助识别系统做出最终的文本输出决策。

- **前向传播反响传播：**由输出层（layer）经隐藏层到输出层，前向传播是神经网络通过逐层线性变换与非线性激活，从输入层传递数据至输出层，以生成预测值的过程。反响传播输出层到隐藏层到输出层，反向传播通过计算损失函数对权重的梯度，并利用链式法则逐层反向传播误差，最后使用优化算法更新网络参数，以最小化损失函数。

![](/media/yls/1T硬盘4/picture/整体.png)

- **前向传播：**输入数据通过网络中的权重和偏置进行线性变换，然后通过激活函数进行非线性变换，得到每一层的输出。最终，输出层的输出即为神经网络的预测值。
  - 输入层接收数据：接受来自外部的数据
  - 计算隐藏层输出：数据从输入层传递到隐藏层，隐藏层中的每个神经元都会接收来自上一层神经元的输入，并计算其加权和。加权和通过激活函数（如ReLU、Sigmoid、Tanh等）进行非线性变换，生成该神经元的输出
  - 计算输出层输出：接收来自隐藏层（或直接从输入层，如果网络没有隐藏层）的输入，并计算（weights）最终的输出

![前向传播图示](/media/yls/1T硬盘4/picture/前向传播.png)

- **反向传播：**从输出层开始，逐层计算每个神经元的误差项（即损失函数对该层激活值的导数），然后利用这些误差项和前一层的激活值来计算当前层权重的梯度。最后，使用这些梯度，通过优化算法（如梯度下降）更新网络的参数，以减小损失函数的值。
  - 计算损失（损失函数）：需要一个衡量模型预测输出与真实输出之间差异的标准，这个标准就是损失函数。常见的损失函数包括均方误差（MSE）、交叉熵损失（Cross-Entropy Loss）等。
  - 计算梯度：得到损失函数后，我们需要利用链式法则（Chain Rule）将损失函数的值反向传播到网络的每一层，并计算每个权重的梯度。梯度表示了损失函数相对于每个权重的变化率，它指导我们如何调整权重以减小损失函数的值。
  - 更新参数：得到每个权重的梯度后，我们可以使用梯度下降（Gradient Descent）等优化算法来更新网络的权重和偏置。梯度下降算法的基本思想是沿着梯度的反方向更新权重，以减小损失函数的值。

![反向传播](/media/yls/1T硬盘4/picture/反向传播.png)

### 29日:

- **N-gram语言模型：**一种基于统计的语言模型，它通过计算N个连续词的条件概率来预测下一个词。N-gram模型的"N"指的是词的序列长度，例如，当N = 1时，模型称之为unigram模型；当年= 2时，模型称之为bigram模型；当N = 3时，模型称之为trigram模型。实际应用中，为了处理未出现在训练集中的词对，可能会使用平滑技术来调整概率估计。

  - 概率计算公式：
    $$
    P(wi​∣wi−1​)=P(wi−1​)/P(wi−1​,wi​)​
    $$

    - *P(wi−1,wi)* 是词对 wi−1和 wi在训练数据中同时出现的概率。

    - *P(wi−1)P(wi−1)* 是词 wi−1在训练数据中出现的概率。

  - 模型概率计算举例：假设我们有如下简单的句子作为训练数据：“I like cats and I love dogs”。

    - 首先，需要将这个句子转换为bigrams，并计算每个bigram和单个词的频率

      |  Bigram   | 频率 | 单个词 | 频率 |
      | :-------: | :--: | :----: | :--: |
      |  I like   |  1   |   I    |  2   |
      | like cats |  1   |  like  |  1   |
      | cats like |  1   |  cats  |  1   |
      |   and I   |  1   |  and   |  1   |
      |  I love   |  1   |  love  |  1   |
      | love dogs |  1   |  dogs  |  1   |

      计算*P(like∣I)*和*P(cats∣like)：*
      $$
      P(like∣I)=C(I)/C(I,like)​=21​=0.5
      $$

      $$
      P(cats∣like)=C(like)/C(like,cats)​=11​=1.0
      $$

- **语言模型平滑技术：**为了解决未见词汇（OOV）和零概率n-gram是自然语言处理中的常见问题。模型在遇到训练集中未出现的词汇或n-gram时，可能会错误地估计其概率，甚至将其概率设定为零。这不仅影响了模型的预测性能，还可能导致无法计算困惑度等指标。 研究人员提出了多种平滑技术，其中包括拉普拉斯平滑（加一平滑）、加-k平滑以及Kneser-Ney平滑等。这些技术通过从高频n-gram中“借用”一些概率质量，分配给那些未见的n-gram，从而避免了零概率问题，并提高了模型的泛化能力。修正计算过程中的概率值，避免某一项概率为0导致整个句子的概率为0。

  - 拉普拉斯平滑：

    - Add-one：强制让所有的n-gram至少出现一次，只需要在分子和分母上分别做加法即可（分母上的V是n-gram句子概率的乘积项的个数，相当于加了1的总个数）。这个方法的弊端是，大部分n-gram都是没有出现过的，很容易为他们分配过多的概率空间。

      ![Add-one](/media/yls/1T硬盘4/picture/Add one.png)

    - Add-k：与Add-one类似，不同点在于原本加一改为加小于一的常数KKK，缺点在于这个常数仍然需要人工确定，对于不同的语料库KKK可能不同

      ![Add-k](/media/yls/1T硬盘4/picture/Add k.png)

    - Kneser-Ney平滑：通过引入一个调整因子来解决零概率问题。它的基本思想是利用n-gram的上下文信息来估计未见n-gram的概率。具体来说，Kneser-Ney平滑使用两个概率值：

      1. 补充概率（continuation probability）：在给定上下文中下一个词的概率。它通过计算给定上下文的n-gram数量和包含该n-gram的不同上下文数量之比来估计。补充概率提供了一个对未见n-gram的概率估计。
      2. 回退概率（discounted probability）：在给定上下文中下一个词的概率。它通过计算给定上下文的n-1 gram数量和包含该n-1 gram的不同上下文数量之比来估计。回退概率提供了一个对已见n-gram的概率估计。

- **kaldi_native_fbank库：**要功能是从原始音频波形中提取 filter bank 特征（fbank），这些特征是语音识别、语音合成、说话人识别等任务中常用的输入表示。支持 WAV、PCM、FLAC 等常见格式（依赖 torchaudio 或 soundfile）。

  - Filter Bank 特征模拟了人耳听觉特性（梅尔频率），通过以下步骤处理音频：

    - 加窗（Windowing）：将音频切分成帧，并对每帧加窗（如 Hamming 窗）
    - 短时傅里叶变换（STFT）：转换为频域表示

    - 梅尔滤波器组（Mel Filter Banks）：在频谱上应用一组三角滤波器，提取不同频带的能量

    - 取对数能量：对每个滤波器输出取对数，得到log-energy
    - 堆叠帧：有时会进行上下文帧拼接或加入一阶、二阶（delta、detla-delta）以捕捉动态信息
- **scipy库：**Python 中一个非常强大且广泛使用的科学计算库，是构建在 `NumPy` 基础之上的扩展库，专门用于进行数值计算、科学分析和工程问题求解。功能包括线性代数运算、优化算法、插值、积分与微分方程、统计分布于检验、信号处理、图像处理、稀疏矩阵、聚类分析、快速傅里叶变换。

### 30号：

- **梅尔频谱图与梅尔频率倒谱系数处理流程：**

  - 音频信号 -> 预处理 -> 分帧 -> 加窗 -> FFT -> 功率谱 -> 梅尔滤波器组 -> 对数压缩 -> 梅尔频谱图。
- 梅尔频谱图 -> 离散余弦变换 (DCT) -> MFCC。
  - 特征表示：

    - 梅尔频谱图是一个二维矩阵，包含频率和时间维度的频谱信息。

    - MFCC是一个较小的特征向量（通常为每帧12到13个系数），这些系数是从梅尔频谱图中提取并压缩得到的。
- **梅尔频谱图 (Mel Spectrogram)：**梅尔频谱图是将音频信号的频谱表示转换到梅尔频率标度上，并通过一组梅尔滤波器对频谱进行加权平均后得到的结果。具体步骤如下：
    - 音频信号预处理：预加重 (Pre-emphasis)、分帧 (Framing)、加窗 (Windowing)
  - 计算功率谱 (Power Spectrum)：对每一帧信号进行快速傅里叶变换（FFT），计算每一帧的功率谱
    - 应用梅尔滤波器组（Mel Filter Bank）：使用一组三角形滤波器，频率分布在梅尔频率标度上，将功率谱通过这些滤波器，得到每个滤波器的加权平均值
    - 对数压缩 (Log Compression)：对滤波器组输出的值取对数，以模拟人耳对声音强度的非线性感知，梅尔频谱图的最终输出是对数梅尔频谱值的矩阵，代表梅尔滤波器的数量，列代表时间帧
  - **梅尔频率倒谱系数 (MFCC)：**MFCC是基于梅尔频谱图进一步处理得到的一组特征系数。具体步骤如下：
    - 计算梅尔频谱图：按照上述步骤，计算音频信号的梅尔频谱图
    - 离散余弦变换（DCT）：对梅尔频谱图的每一列（即每一时间帧的梅尔频率表示）进行离散余弦变换 (DCT)，这一步的目的是将频谱压缩到更少的系数，并去除相关性，使得特征更加集中
  - 保留低阶系数：通常只保留DCT变换后的前12到13个系数，这些系数包含了主要的频谱信息





## 六月

### 3日：

##### **机器学习：**

机器学习是一种人工智能技术，通过数据学习提升系统性能。其工作流程包括数据收集、预处理、特征工程、模型选择、模型训练、模型评估和应用。特征工程是关键步骤，用于数据预处理和提取有用信息。模型评估涉及分类和回归模型的准确率、RMSE等指标。文章还讨论了监督学习、无监督学习以及模型的过拟合和欠拟合问题。

- **线性回归：**线性回归是一种基于一个或多个输入因素预测连续结果的方法。简单的说，它通过对我们所拥有的数据拟合一条直线来帮助我们找到不同的变量之间的关系。

  - 线性回归是关于合适的一组数据点的最佳直线（回归线）。这条线由以下等式表示：
    $$
    y = mx + b
    $$
    ![线性回归函数](/media/yls/1T硬盘4/picture/Linear Regression.png)

    ```python
    """
    代码实现线性回归示例
    """
    import numpy as np
    from sklearn.linear_model import LinearRegression
    import matplotlib.pyplot as plt
    
    np.random.seed(0)
    X = np.random.rand(100,1)
    y = 2 + 3 * X + np.random.rand(100,1)
    
    model = LinearRegression()
    model.fit(X, y)
    
    X_test = np.array([[0], [1]])
    y_pred = model.predict(X_test)
    
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, color = 'b', label = 'Data points')
    plt.plot(X_test, y_pred, color = 'r', label = 'Regression line')
    plt.legend()
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Liner Regression Example\nimage by iCare fp')
    plt.savefig(
        fname="/media/yls/1T硬盘/picture/Linear Regression.png",   # 文件路径 + 名称（自动创建目录需手动处理）
        format="png",              # 图像格式（可省略，由后缀自动识别）
    )
    plt.show()
    
    
    print(f"Intercept: {model.intercept_[0]:.2f}")
    print(f"Coefficient: {model.coef_[0]:.2f}")
    ```

    对于多个自变量，我们使用多元线性回归：
    $$
    y = b0 + b1 * 1 + b2 * 2 + ... + bn * n
    $$
    ![多元线性回归](/media/yls/1T硬盘4/picture/polynomial regression.png)
    
    ```python
    """
    多元线性回归函数实现示例
    """
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.pipeline import make_pipeline
    from sklearn.linear_model import Ridge
    import numpy as np
    import matplotlib.pyplot as plt
    
    np.random.seed(0)
    X = np.sort(5 * np.random.rand(80,1), axis = 0)
    y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])
    
    degree = 5
    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha = 1e-3))
    model.fit(X, y)
    
    X_test = np.linspace(0, 5, 100)[:, np.newaxis]
    y_pred = model.predict(X_test)
    
    plt.scatter(X, y, color = 'b', label = 'Data points')
    plt.plot(X_test, y_pred, color = 'r', label = 'Polynomial regrression')
    plt.legend()
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title("Polynomial Regression with Ridge Regularrization")
    plt.savefig('/media/yls/1T硬盘/picture/polynomial regression.png')
    plt.show()
    ```
    
    
  
  - **逻辑回归：**逻辑回归是一种分类算法，它用于预测一组给定一组自变量的二元结果。例如你想根据今天的天气预测明天是否会下雨（是 / 否）。逻辑回归会使用一种称为sigmoid函数的特殊公式，将任何输入转换为0和1之间的概率，表示发生的可能性。如果接近1表示 “ 是 ” 的可能性大则认为会下雨反之不下雨。另外有一种特殊情况当概率达到0.5时，我们从预测 “ 否 ” 切换至 “ 是 ” ，这被称之为决策边界。
  
    ![sigmoid函数](/media/yls/1T硬盘4/picture/logistic.png)
  
    ```python
    import numpy as np
    from sklearn.linear_model import LogisticRegression   # Python中的机器学习库，基于Numpy和Scipy构建的开源库，提供了大量用于数据挖掘、数据分析和机器学习任务的工具
    import matplotlib.pyplot as plt
    
    hours_studied = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1) # 使用reshape()函数把一个行向量变成了列向量
    outcome = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])
    
    model = LogisticRegression()
    model.fit(hours_studied, outcome)
    
    predicted_outcome = model.predict(hours_studied)
    predicted_probabilities = model.predict_proba(hours_studied)
    
    plt.figure(figsize=(10, 6))
    plt.scatter(hours_studied, outcome, color = 'b', label = 'Data points')
    # 使用 hours_studied 作为 x 轴，第二列（正类的概率）作为 y 轴
    plt.plot(hours_studied, predicted_probabilities[:, 1], color = 'r', label = 'Logistic line')
    plt.legend()
    plt.xlabel('hours_studied')
    plt.ylabel('probility of passing')
    plt.title('Logistic Regression Example\nimage by iCare fp')
    plt.savefig(
        fname="/media/yls/1T硬盘/picture/Logistic Regression.png",   # 文件路径 + 名称（自动创建目录需手动处理）
        format="png",              # 图像格式（可省略，由后缀自动识别）
    )
    plt.show()
    
    print("Predicted Outcomes:", predicted_outcome)
    print("Predicted Probabilities", predicted_probabilities)
    ```
  
    ![逻辑回归函数示例](/media/yls/1T硬盘4/picture/Logistic Regression.png)
  
  - **决策树：**决策树用于分类和回归任务。它通过创建一个模型来工作，该模型基于一系列简单的决策进行预测—例如遵循树上的路径。在这棵树上，每个问题都是一个分支，而最终的答案是一片叶子。该树根据它给出的数据 “ 学习 ” 最好的问题，使其成为决策的强大工具。
  
    ```python
    from sklearn.tree import DecisionTreeClassifier, plot_tree
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    import matplotlib.pyplot as plt
    
    iris = load_iris()
    X, y = iris.data, iris.target
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    model = DecisionTreeClassifier(max_depth=3, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=iris.target_names))
    
    plt.figure(figsize=(20,10))
    plot_tree(model, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True)
    plt.savefig(
        fname="/media/yls/1T硬盘/picture/Decision tree.png",   # 文件路径 + 名称（自动创建目录需手动处理）
        format="png",
    )
    plt.show()
    ```
  
    ![决策树](/media/yls/1T硬盘4/picture/Decision tree.png)
  
  - **随即森林：**是一种集成学习方法，通过构建多个决策树并结合它们的结果来帮助我们做出更精确的预测。随即森林不只是依赖一颗决策树，而是创造许多颗树，每棵树都可以查看数据的不同部分。通过对回归任务的结果进行平均或对分类任务进行多数投票，随即森林减少了错误，是预测更加可靠。
  
    - 评估标准：
      1. 分类：准确度、精确度、召回率、F1分数
      2. 回归：均方误差（MSE），R平方
  
    ```python
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, confusion_matrix
    import seaborn as sns
    import matplotlib.pyplot as plt
    import numpy as np
    
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                               n_redundant=5, n_classes=3, random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(10,8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix \nimage by iCare fp')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10,6))
    plt.title("Feature Importances \nimage by iCare fp")
    plt.bar(range(X.shape[1]), importances[indices])
    plt.xticks(range(X.shape[1]), indices)
    plt.tight_layout()
    plt.show()
    ```
  
    ![随机森林](/media/yls/1T硬盘4/picture/Random Forest1.png)
  
    ![随即森林](/media/yls/1T硬盘4/picture/Random Forest2.png)
  
    模型的准确性：0.77
  
  - **K—最邻近：**K-Nearest Neighbors（KNN）是一种简单而懒惰的学习算法，用于分类和回归。它基于特征空间中k-最邻近的多数类或平均值来预测目标值。
  
    1. 首先，你决定一个数字，K，这是你想要考虑的邻居的数量
    2. 接下来，算法计算你想要预测的项目与数据中所有其他项目之间的距离
    3. 然后它对距离进行排序，并选择K个最近-你的 “ 邻居 ” 
    4. 对于分类任务，它收集最近邻居的类别
    5. 对于回归任务，它对这些邻居的值进行平均以预测一个数字
  
    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    import matplotlib.pyplot as plt
    import numpy as np
    
    # 加载数据集
    iris = load_iris()
    X, y = iris.data, iris.target
    
    # 只取前两个特征进行训练和预测
    X_subset = X[:, [0, 1]]
    
    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.3, random_state=42)
    
    # 创建并训练KNN分类器
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X_train, y_train)
    
    # 测试评估
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=iris.target_names))
    
    # 绘制决策边界函数
    def plot_decision_boundary(X, y, model, ax=None):
        h = .02
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                              np.arange(y_min, y_max, h))
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        if ax is None:
            ax = plt.gca()
        ax.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black', s=25)
        ax.set_xlabel('Sepal length')
        ax.set_ylabel('Sepal width')
        return ax
    
    # 绘图显示
    plt.figure(figsize=(10, 8))
    plot_decision_boundary(X_subset, y, model)
    plt.title('KNN Decision Boundary (k=3) using Sepal Length & Width')
    plt.savefig(
        fname="/media/yls/1T硬盘/picture/Decision tree.png",
        format="png",
    )
    plt.show()
    ```
  
    ![](/media/yls/1T硬盘4/picture/Decision tree.png)
  
  - **朴素贝叶斯：**
  
    先验概率：即基于统计的概率，是基于以往历史经验和分析得到的结果，不需要依赖当前发生的条件。
  
    后验概率：则是从条件概率而来，由因推果，是基于当下发生了事件之后计算的概率，依赖于当前发生的条件。
  
    条件概率：记事件A发生的概率为P(A)，事件B发生的概率为P(B)，则在B事件发生的前提下，A事件发生的概率即为条件概率，记为P(A|B)。
    

##### **配置服务器环境：**

icefall 工具包下 wenetSpeech 数据集训练模型运行环境配置

- **Anaconda下载：**Linux环境下命令行下载，也可以在官网下载

  ```shell
  wget https://repo.anaconda.com/archive/Anaconda3-2023.07-1-Linux-x86_64.sh
  # 或者
  curl -O https://repo.anaconda.com/archive/Anaconda3-2023.07-1-Linux-x86_64.sh
  ```

- **Anaconda安装：**打开终端并导航到包含下载脚本的目录，运行以下命令并安装

  ```shell
  bash Anaconda3-2023.07-1-Linux-x86_64.sh
  ```

  - 安装后出现 ” Subject to the terms of this Agreement, Anaconda hereby grants you a non-exclusive, non-transferable license to: “解决

    看到的内容是 Anaconda 安装过程中的 **用户许可协议（EULA）**，你需要按下 **Enter 键** 来逐页阅读，或者输入 **`q`** 跳过阅读至最后一页输入**yes**

- **配置 [icefall](https://github.com/k2-fsa/icefall.git) 环境**

  ```shell
  # 使用conda创建虚拟环境
  conda create -n icefall python=3.11
  conda activate icefall
  
  # 根据自行的环境安装pytorch，下面是简单参考命令：
  # Install Pytorch
  pip install torch==2.6.0 --index-url https://download.pytorch.org/whl/cpu
  
  # Install k2
  pip install k2==1.24.4.dev20250307+cpu.torch2.6.0 -f https://k2-fsa.github.io/k2/cpu-cn.html
  
  # 还有一些库等等
  pip install torchaudio lhotse
  
  # 获取icefall源码
  git clone https://github.com/k2-fsa/icefall.git
  cd egs/librispeech/ASR
  ```

  - 解决服务器不能复制粘贴问题，执行命令行后重启：

    ```powershell
    sudo apt-get update		#更新软件源
    sudo apt-get install open-vm-tools		#安装open-vm-tools
    ```

### 4日：

##### 数据集下载：

**bug：**

- ModuleNotFoundError: No module named 'icefall'Python ，原因找不到 `icefall` 模块。这是因为 `icefall` 是一个本地开发库（不是通过 pip 安装的标准包），需要将它的根目录加入 Python 的模块搜索路径中。

```shell
cd /path/to/icefall

# 设置 PYTHONPATH
export PYTHONPATH=$(pwd):$PYTHONPATH

# 再次运行 prepare.sh 或直接运行 compute_fbank_aishell.py
cd egs/wenetspeech/ASR
./prepare.sh --stage 3
```

- Ubuntu 默认使用开源的 `nouveau` 显卡驱动，它会与 NVIDIA 官方驱动冲突。

  ```shell
  # 检查是否启用nouveau驱动
  lsmod | grep nouveau
  # 禁用nouveau：
  sudo bash -c "echo blacklist nouveau > /etc/modprobe.d/blacklist-nvidia-nouveau.conf"
  sudo bash -c "echo options nouveau modeset=0 >> /etc/modprobe.d/blacklist-nvidia-nouveau.conf"
  sudo update-initramfs -u
  # 重启
  sudo reboot
  # 更新包列表
  sudo apt update
  # 卸载旧驱动（如有）
  sudo apt purge '^nvidia-.*' || true
  sudo apt autoremove
  # 安装推荐驱动（565-server 支持 CUDA 12.1 和 A100）
  sudo apt install nvidia-utils-565-server
  ```

  

##### 深度学习：

深度学习是一种机器学习方法，它模仿人脑神经网络的结构和功能。它通过多层次的神经网络来学习和提取数据的特征，并使用这些特征进行预测和决策。深度学习在计算机视觉、自然语言处理和语音识别等领域取得了很大的成功，它能够处理大规模和复杂的数据，并从中学习到更加准确的模式和规律。

- 四种典型的深度学习算法：

  - **卷积神经网络—CNN**：

    - 人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。下面是人脑进行人脸识别的一个示例：
  
      ![人脑视觉处理](/media/yls/1T硬盘4/picture/人脑视觉处理.png.webp)
  
      我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。CNN就是模仿大脑，构造多层的神经网络，较底层的识别初级的图像特征，若干底层特征组成更上一层的特征，最终作出分类。
  
    - CNN的特点：广泛应用于人脸识别、自动驾驶、美图秀秀、安防等领域
  
      1. 能够将大数据量的图片有效的降维成小数据量（并不影响结果）且不会因为降维而影响结果
  
         ![数据降维](/media/yls/1T硬盘4/picture/数据降维.png.webp)
  
      2. 能够保留图片的特征，类似人的视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来类似的图像
  
         ![特征提取](/media/yls/1T硬盘4/picture/特征提取.png.webp)
  
    - CNN的基本原理：将图像像素转换为矩阵表示，利用卷积核（针对不同特征进行设计）对图像矩阵进行点积运算即特征提取，图像矩阵中子矩阵与卷积核进行点积运算的矩阵称之为感受野，点积得到的值越大，我们认为与卷积核提取的特征越接近。
  
      1. 卷积层 （降低图片特征维度）— 主要作用是提取图片中的局部特征，这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。卷积层的运算过程如下图，用一个卷积核扫完整张图片：
  
         ![卷积—特征提取](/media/yls/1T硬盘4/picture/卷积—特征提取.gif)
  
         在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。总的来说卷积层通过卷积核的过滤提取图片的局部特征，跟人类的特征提取类似。
  
      2. 池化层 — 主要作用是把数据降维，可以有效的避免过拟合，池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：
  
         ![池化](/media/yls/1T硬盘4/picture/池化.gif)
  
         上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。
  
         1. 全连接层（又称FC层，例如线性投影层）— 根据不同任务输出我们想要的结果：全连接层会把这些特征图**展平（Flatten）**成一个向量，然后通过多层感知机（MLP）的方式，逐步映射到最终的类别输出（例如3个类别的概率）。经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。步骤如下：
  
         2. 展平：经过前面卷积层和池化层后，输出是一个形状为：(通道数 C, 高 H, 宽 W) 或 (H, W, C)（取决于框架使用 channel_first 还是 channel_last）。`7x7x512` 的特征图（即 512 个 7x7 的特征图）——> 25088 维向量。
  
         3. 全连接层其实就是一个标准的神经网络层，形式如下：
         $$
           y=Activation(W⋅x+b)
         $$
           其中：
  
           ​	x：展平后的特征向量（如 25088 维）
  
           ​	W：权重矩阵（比如 4096 x 25088）
  
           ​	b：偏置项
  
           ​	y：输出（比如 4096 维）
  
           ​	Activation：激活函数（如 ReLU）
  
         4. 最终输出：Softmax 分类：最后一层全连接层的输出维度等于你要分类的类别数（如 CIFAR-10 是 10 类），然后再接一个 Softmax 函数，输出每个类别的概率
  
           ```
           [0.1, 0.05, 0.8, ..., 0.05] → 表示最可能是第3类
           ```
  
         ![全连接](/media/yls/1T硬盘4/picture/全连接.png.webp)
  
    - CNN的实际应用：
      1. 图片分类、检索
      2. 目标定位检测
      3. 目标分割
      4. 人脸识别
      5. 骨骼识别
  
  - **循环神经网络—RNN：**
  
    RNN是一种有效的处理序列数据的算法。比如：文章内容、语音音频、股票价格走势等，之所以它能处理序列数据，是因为在序列中前面的输入也会影响到后面的输出，相当于有了 “ 记忆功能 ”。但是RNN存在严重的短期记忆问题，长期的数据影响也小。 
  
    - RNN独特价值：卷积神经网络 – CNN和普通的算法大部分都是输入和输出的一一对应，也就是一个输入得到一个输出。不同的输入之间是没有联系的。需要处理「序列数据 – 一串相互依赖的数据流」的场景就需要使用 RNN 来解决了。比如文章里的文字内容、语音中的音频内容和股票市场中的价格走势等。
  
      ![RNN优势](/media/yls/1T硬盘4/picture/RNN优势.png.webp)
  
    - 基本原理：神经网络的结构 ——> 输入层 – 隐藏层 – 输出层
  
      RNN 跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。如下图所示：
  
      ![rnn-1](/media/yls/1T硬盘4/picture/rnn-1.gif)
  
      假如需要判断用户的说话意图（问天气、问时间、设置闹钟…），用户说了一句“what time is it？”我们需要先对这句话进行分词：按照顺序输入 RNN ，我们先将 “what”作为 RNN 的输入，得到输出「01」,然后，我们按照顺序，将“time”输入到 RNN 网络，得到输出「02」。这个过程我们可以看到，输入 “time” 的时候，前面 “what” 的输出也产生了影响（隐藏层中有一半是黑色的）。以此类推，前面所有的输入都对未来的输出产生了影响，大家可以看到圆形隐藏层中包含了前面所有的颜色。如下图所示：
  
      ![rnn_input](/media/yls/1T硬盘4/picture/input.gif)
  
    - LSTM – 长短期记忆网络：由于RNN短期的记忆影响较大（如橙色区域），但是长期的记忆影响就很小（如黑色和绿色区域），这就是 RNN 存在的短期记忆问题。导致无法处理很长的输入序列，训练RNN需要投入很大成本，因此引入LSTM。
  
      ![RNN对比LSTM](/media/yls/1T硬盘4/picture/LSTM.png.webp)
  
      LSTM 类似上面的划重点，**他可以保留较长序列数据中的「重要信息」，忽略不重要的信息**。这样就解决了 RNN 短期记忆的问题。
  
      ![画重点](/media/yls/1T硬盘4/picture/画重点.png.webp)
  
    - 从 LSTM 到 GRU
  
      Gated Recurrent Unit – GRU 是 LSTM 的一个变体。他保留了 LSTM 划重点，遗忘不重要信息的特点，在long-term 传播的时候也不会被丢失。GRU 主要是在 LSTM 的模型上做了一些简化和调整，在训练数据集比较大的情况下可以节省很多时间。
  
      ![GRU](/media/yls/1T硬盘4/picture/gru.png.webp)
  
    - 应用场景：
      1. 文本生成
      2. 语音识别
      3. 机器翻译
      4. 生成图像描述
      5. 视频标记
  
  - **生成对抗网络（GAN）：**
    - **生成器(Generator**)：通过机器生成数据（大部分情况下是图像），目的是“骗过”判别器
    - **判别器(Discriminator**)：判断这张图像是真实的还是机器生成的，目的是找出生成器做的“假数据”
  
  - **深度强化学习 - RL**：
    - 有模型学习（Model-Based）对环境有提前认知，可以提前考虑，但是缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现不好。
    - 免模型（Model-Free）放弃了模型学习，在效率上不如前者，但是这种方式更加容易，也容易在真实场景下调整到很好的状态。所以免模型学习更加受欢迎，得到了更加广泛的开发和测试。
  
  - **深度神经网络（DNN）：**
  
    DNN是一种具有多个隐藏层的神经网络模型，其核心在于其深度，即包含多个隐藏层。这些隐藏层通过非线性变换，使得模型能够捕捉到数据中的复杂关系和模式。DNN通常由输入层、隐藏层和输出层组成，每一层都包含多个神经元，神经元之间通过权重和偏置进行连接。
    
    - 训练过程：DNN的训练过程是一个权重学习和优化的过程。在训练开始时，网络中的权重和偏置是随机初始化的。然后，通过前向传播计算网络的预测输出，并与真实标签进行比较，计算损失函数。接下来，利用反向传播算法计算损失函数关于每个**权重（控制输入信号的强弱，是神经网络学习的主要对象。）**和**偏置（提供输出的平移，使网络可以拟合更复杂的数据分布。）**的梯度，并根据这些梯度更新权重和偏置，以最小化**损失函数**。在DNN的训练中，常用的**优化算法**包括梯度下降（Gradient Descent）及其变种（如批量梯度下降、随机梯度下降、小批量梯度下降）和更先进的优化算法（如Adam、RMSProp、Adagrad等）。
    
      **权重：**权重是连接神经网络不同层之间神经元的系数。每一个神经元的输出会乘以一个权重，决定了该输入对后续神经元影响的大小。反映了输入特征对最终输出的重要性。网络在训练过程中通过反向传播算法不断调整权重，使模型能够更好地拟合数据。
    
      **偏置：**偏置是加在每个神经元上的一个常数项。它不依赖于输入数据，单独存在于每个神经元。偏置用于调整激活函数的输出，使模型具有更强的表示能力。即使所有输入为0，偏置也能让神经元有非零输出。
    
      假设输入为   x ，权重为   w ，偏置为   b ，神经元的输出为   y ：
      $$
       y = f ( w ⋅ x + b ) 
      $$
      其中，  f 是激活函数（如ReLU、Sigmoid等）。
    
      1. 前向传播：DNN从输入层到输出层的信息传递过程。在前向传播过程中，输入数据通过每一层的神经元进行加权求和和激活函数变换，最终生成输出。
      2. 损失函数：用于衡量DNN预测结果与真实标签之间的差距。常见的损失函数包括均方误差（MSE）、交叉熵损失等。通过最小化损失函数，可以优化DNN的权重和偏置项，提高模型的预测性能。
      3. 反向传播：DNN训练过程中的一种算法，用于计算损失函数关于权重和偏置项的梯度。这些梯度随后用于更新权重和偏置项，以最小化损失函数。反向传播算法通过链式法则计算梯度，从输出层开始逐层向前传播，直到更新完所有层的权重和偏置项。
    
    - 应用场景：
      1. 图像识别：图像分类、目标检测、图像分割
      2. 视频分析：用于视频内容的理解，分析以及异常处理
      3. 增强现实：辅助增强现实技术，实现更精准的物体跟踪和场景重建
      4. 机器翻译
      5. 文本生成：自动生成自然语言文本，用于内容创造、语言翻译和聊天机器人
      6. 情感分析：分析文本中的情感倾向
      7. 语音识别和合成：语音识别与合成方面也有广泛应用
    
    
  
  

### 5日：

##### torch、显卡驱动cuda安装：

- **安装步骤：**

  ~~~shell
  ## 2. 创建新环境（以python3.9为例）
  ```bash
  conda create -n icefall python=3.9 -y
  conda activate icefall
  ```
  
  ## 3. 安装PyTorch与CUDA（以torch 2.1.0 + cuda 12.1为例）
  ```bash
  conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=12.1 -c pytorch -c nvidia -y
  ```
  
  ## 4. 安装k2
  ```bash
  pip install k2==1.23.4 -f https://k2-fsa.github.io/k2/html/whl/torch-2.1.0.html
  ```
  
  ## 5. 验证
  ```python
  import torch
  import k2
  print(torch.__version__)
  print(k2.__version__)
  ```
  
  ## 6. 安装icefall及其他依赖
  ```bash
  cd path/to/icefall
  pip install -e .
  ```
  ~~~

- **检测不到k2库：**

  k2是C++扩展模块，需要动态链接libpython3.9.so.1.0。在conda环境下，某些Linux发行版或conda-forge渠道未自动安装这个库，或者LD_LIBRARY_PATH没有包含它。具体步骤如下：

  - 安装libpython，激活你的icefall环境后，运行把`libpython3.9.so.1.0`放到`$CONDA_PREFIX/lib/`里。

  ```shell
  conda activate icefall
  conda install libpython
  ```

  - 如果还不行，手动设置LD_LIBRARY_PATH，可以将这行写入`~/.bashrc`或者每次激活环境后执行

  ```shell
  find $CONDA_PREFIX -name "libpython3.9.so*"   # 检查库是否存在
  # 输出（如/home/fp/anaconda3/envs/icefall/lib/libpython3.9.so.1.0），说明库已安装。
  
  export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
  ```

- **numpy版本兼容问题：**

  ```shell
  pip uninstall numpy
  pip install numpy==1.26.4
  ```

##### 模型训练：

- **prepare脚本运行：**

下载解压数据集（如 Aishell 原始 tar.gz 包）；整理音频和标注，将音频、标注文本等文件转为同一结构（如 wav.scp、text、utt2spk、spk2utt等 Kaldi/Lhotse格式）；清洗数据，去除有问题的音频、无表注的文本、无声文件等；分离训练、验证、测试集整理好各自的文件清单。

- **训练步骤：**

  ```shell
  cd icefall/egs/aishell/ASR
  
  # 1. 添加	libpython3.9.so.1.0路径
  export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
  
  # 2. 计算FBANK特征
  python local/compute_fbank_aishell.py
  
  # 3. 训练
  python zipformer/train.py
  
  # 4. 解码评估
  python zipformer/decode.py --epoch 29 --avg 10 --exp-dir ./exp-zipformer
  ```




### 8日：

##### 模型训练思路（SmolVLA）：

现代视觉模型（如 ViT、DETR、YOLO 系列）已经能高效完成这些任务，并集成到 VLA 架构中进行统一处理。VLA 可以结合 ASR 模块将语音转为文本，再由语言模型理解并生成动作指令（例如：“打电话给妈妈” → 调用通讯录接口）。它可以根据视觉和语言输入做出具体操作，比如：检测到老人摔倒 → 触发报警 → 自动拨打紧急联系人；听到“帮我记下来” → 启动语音识别 → 记录内容到指定应用

![image-20250608092420621](/home/yls/.config/Typora/typora-user-images/image-20250608092420621.png)



### 9日：

##### Mind2Web：

开发和评估能够根据语言指令在任何网站上完成复杂任务的通用智能体。

##### MMIna：

数据集旨在评估智能体在复杂互联网任务中的表现，特别是处理跨多个网站的信息提取和操作。MMInA强调了在现实环境中进行评估的重要性，并提出了新的评估协议，以更全面地衡量智能体在多跳任务中的表现。此外，数据集还引入了记忆增强方法，以提高智能体在处理复杂任务时的性能。

##### （消融实验）ablation study：

同时提出多个思路提升某个模型的时候，为了验证这几个思路分别都是有效的，做的控制变量实验的工作。

比如为了提升baseline的性能， 给它添加了模块A、B，加完后效果提升很多。为了验证A、B两个模块是不是真的对模型有提升效果则需要做ablation study。

1. 在baseline的基础上添加模块A，看效果
2. 在baseline的基础上添加模块B，查看效果
3. 在baseline的基础上同时添加模块A、B查看效果



### 10日：

##### prompt：

- **CoT（Chain-of-Thought）：**是一种引导模型进行多步推理的方法，通过给出带有中间推理步骤的示例，让模型模仿这种“思考链”来解决复杂问题。核心思想是鼓励模型输出类似人类的逐步推理过程，而不是直接给出答案。
- **ToT（Tree of Thoughts）：**不只是单一线性推理，而是将多个可能的推理路径组织成树状结构，通过搜索或评估选择最优路径。核心思想是引入评分机制，对多路径搜索进行打分排序选出最佳路径，并且支持回溯和修改决策。
- **ReAct（Reasoning + Acting）：**允许模型在推理的同时调用工具（API），适用于交互式任务或需要外部信息的场景。核心思想在推理中穿插调用工具的行为，类似人在解决问题时会使用外部资源。

##### 参数与Token关系：

参数决定了模型如何处理token，token是模型在训练和推理阶段操作的对象，模型学到的参数去理解和生成token序列。

- **模型参数：**是神经网络中通过训练学习到的权重（weights），通过大量的数据学习语言规律，调整参数以提高预测准确性，它们决定了模型如何将输入（比如一段文字）映射为输出（比如回答或预测）。
  - 作用：参数越多，模型表达能力越强，能记住和理解的语言模式就越复杂。
  - 如何理解：参数可想象成一个 “ 大脑 ” 的连接方式，每个连接的强度就是参数值。训练过程就是在不断调整这些连接的强度，让模型更准确地理解和生成语言。
  - 训练方法：梯度下降（SGD / Adam）等优化算法；反向传播不断更新参数。

- **Token：**模型处理文本时的基本单位，它可能不是一个完整的单词，可能是词的一部分、标点符号、甚至是子词（subword）。

  - 用途：输入给模型时，将文本转化为token，模型内部再对token进行处理，最后输出新的token。

  - 作用：构成训练样本（输入token + 目标token）；根据输入token预测下一个token；通过交叉熵损失函数衡量预测误差。

    1. 交叉熵损失函数：衡量两个概率分布之间差异的函数，常用于分类任务中。它广泛应用于语言模型、图像识别、语音识别等场景。使用交叉熵损失来衡量预测和真实值的差异

       - CTC Loss（Connectionist Temporal Classification）：适用于输入和输出不一致的情况，可以自动对其语音帧和字符。

       - 交叉熵损失（Cross-Entropy Loss）：当语音识别模型使用类似Transformer的结构时，通常会把输出看作一个序列分类任务，每个时间步输出一个词或子词的概率分布。
       - Sequence-to-Sequence + Attention 中使用的损失函数：输入是一段语音编码（encoder output），输出是一串token（decoder output），一般使用交叉熵损失来逐个预测每个输出token。

##### RAG和sigmoid概率问题：

Sigmoid 输出的是神经网络学到的预测概率，而 RAG 中的 chunk 相关性得分是基于向量相似度的匹配程度。虽然它们都可以表示“相关性”，但一个是模型训练出来的，一个是检索出来的；一个用于分类决策，一个用于信息筛选。两者可以结合使用，提升 RAG 系统的效果。

- sigmoid函数：一种激活函数，将任意实数映射到 [0,1] 区间，常用于二分类任务中表示概率。  
  $$
  \sigma(x) = \frac{1}{1 + e^{-x}}
  $$

  - 在一个文本分类模型中，最后一层输出是 `z = 2.0`；
  - 经过 sigmoid 得到 `p = σ(2.0) ≈ 0.88`；
  - 表示该模型认为这个句子属于类别 A 的概率为 88%；

- RAG匹配子块概率（embedding）：在 RAG 架构中，当用户输入一个 query 时，会先从向量数据库中检索出最相关的知识片段（chunk），然后把这些信息作为上下文传给生成模型。 这里的“相关性概率”其实更像是一个 **相似度得分（similarity score）** ，通常用 **余弦相似度（cosine similarity）**  来衡量。

  - 用户输入：“如何做西红柿炒鸡蛋？” 向量数据库中找到三个最相关的 chunk：

    1. chunk1: “鸡蛋打散后下锅翻炒...” → 相似度 0.92
    2. chunk2: “西红柿切片放入锅中...” → 相似度 0.85
    3. chunk3: “加盐调味...” → 相似度 0.76

    这些分数可以理解为“这些知识与用户问题的相关程度”。 



### 11日：

##### 模型转换中的模型配置问题：

在构建RKNN模型之前，需要先对模型进行通道均值、量化图片RGB2BGR转换、量化类型等的配置，这
些操作可以通过config接口进行配置。

- 模型转换参数：

  - **mean_values：**输入的均值。参数格式是一个列表，列表中包含一个或多个均值子列表，多输入模型对应多个子列表，每个子列表的长度与该输入的通道数一致，例如[[128,128,128]]，表示一个输入的三个通道的值减去128。默认值为None，表示所有的mean值为0。
  - **std_values：**输入的归一化值。参数格式是一个列表，列表中包含一个或多个归一化值子列表，多输入模型对应多个子列表，每个子列表的长度与该输入的通道数一致，例如[[128,128,128]]，表示设置一个输入的三个通道的值减去均值后再除以128。默认值为None，表示所有的std值为1。
  - **quant_img_RGB2BGR：**表示在加载量化图像时是否需要先做RGB2BGR的操作。如果有多个输入，则用列表包含起来，如[True, True, False]。默认值为False。该配置一般用在Caffe的模型上，Caffe模型训练时大多会先对数据集图像进行RGB2BGR转换，此时需将该配置设为True。另外，该配置只对量化图像格式为jpg/png/bmp有效，npy格式读取时会忽略该配置，因此当模型输入为BGR时，npy也需要为BGR格式。该配置仅用于在量化阶段（build接口）读取量化图像或量化精度分析（accuracy_analysis接口），并不会保存在最终的RKNN模型中，因此如果模型的输入为BGR，则在调用toolkit2的inference或C-API的run函数之前，需要保证传入的图像数据也为BGR格式。
  - **quantized_dtype：**量化类型，目前支持的量化类型有w8a8、w4a16、w8a16、w4a8、
    w16a16i和w16a16i_dfp。默认值为w8a8。
    - w8a8：权重为8bit非对称量化精度，激活值为8bit非对称量化精度。（RK2118不支持）
    - w4a16：权重为4bit非对称量化精度，激活值为16bit浮点精度。（仅RK3576支持）
    - w8a16：权重为8bit非对称量化精度，激活值为16bit浮点精度。（仅RK3562支持）
    - w4a8：权重为4bit非对称量化精度，激活值为8bit非对称量化精度。（暂不支持）
    - w16a16i：权重为16bit非对称量化精度，激活值为16bit非对称量化精度。（仅
    RV1103/RV1106支持）
    - w16a16i_dfp：权重为16bit动态定点量化精度，激活值为16bit动态定点量化精度。（仅
    RV1103/RV1106支持）

  - **quantized_algorithm：**计算每一层的量化参数时采用的量化算法，目前支持的量化算法
    有：normal，mmse及kl_divergence。默认值为normal。
    - normal量化算法的特点是速度较快，推荐量化数据量一般为20-100张左右，更多的数据量下精度未必会有进一步提升。
    - mmse量化算法由于采用暴力迭代的方式，速度较慢，但通常会比normal具有更高的精度，推荐量化数据量一般为20-50张左右，用户也可以根据量化时间长短对量化数据量进行适当增减。
    - kl_divergence量化算法所用时间会比normal多一些，但比mmse会少很多，在某些场景下（feature分布不均匀时）可以得到较好的改善效果，推荐量化数据量一般为20-100张左右。
  - **quantized_method：**目前支持layer或者channel。默认值为channel。
    - layer：每层的weight只有一套量化参数；
    - channel：每层的weight的每个通道都有一套量化参数，通常情况下channel会比layer精度更高。

  - **float_dtype：**用于指定非量化情况下的浮点的数据类型，目前支持的数据类型有float16。默认值为float16。
  - **target_platform：**指定RKNN模型是基于哪个目标芯片平台生成的。目前支持 “rv1103”、‘’rv1103b“、“rv1106”、“rv1106b”、“rk2118”、“rk3562”、“rk3566”、“rk3568”、“rk3576”和
    “rk3588”。该参数对大小写不敏感。默认值为None。
  - **custom_string：**添加自定义字符串信息到RKNN模型，可以在runtime时通过query查询到该信息，方便部署时根据不同的RKNN模型做特殊的处理。默认值为None。
  - **remove_weight：**去除conv等权重以生成一个RKNN的从模型，该从模型可以与带完整权重的RKNN模型共享权重以减少内存消耗。默认值为False。
  - **compress_weight：**压缩模型权重，可以减小RKNN模型的大小。默认值为False。
  - **single_core_mode：**是否仅生成单核模型，可以减小RKNN模型的大小和内存消耗。默认值为False。目前仅对RK3588 / RK3576生效。默认值为False。
  - **dynamic_input：**用于根据用户指定的多组输入shape，来模拟动态输入的功能。格式[[input0_shapeA, input1_shapeA, ...], [input0_shapeB, input1_shapeB, ...], ...]。默认值为None，实验性功能。假设原始模型只有一个输入，shape为[1,3,224,224]，或者原始模型的输入shape本身就是动态的，如shape为[1,3,height,width]或[1,3,-1,-1]，但部署的时候，需要该模型支持3种不同的输入shape，如[1,3,224,224], [1,3,192,192]和[1,3,160,160]，此时可以设置dynamic_input=[[[1,3,224,224]], [[1,3,192,192]], [[1,3,160,160]]]，转换成RKNN模型后进行推理时，需传入对应shape的输入数据。

##### funASR语音识别包下载：

- github上拉取开源包：

```shell
git clone https://github.com/alibaba/FunASR.git && cd FunASR
```



### 12日：

##### 连接时序分类器（CTC）：

一种用于序列预测问题的算法，特别适用于输入和输出序列长度不一致的情况。主要用于解决语音识别中的对齐问题。CTC允许模型直接从输入序列预测输出序列，而不需要显式的对齐信息。

- **核心概念：**
  - 序列到序列学习：CTC是一种序列到序列的学习方法，适用于诸如语音识别、手写识别等任务，在这些任务中，输入信号（如音频波形或笔迹）与输出标记（如单词或字符）之间没有固定的对齐关系。
  - 空白标签（Blank Label）：CTC引入了一个特殊的 “ 空白 ” 标签，用来表示无输出或者间隔。这个空白标签帮助模型区分连续重复的字符，并且允许模型条过某些输入步骤，这对于处理变长序列尤为重要。

- **工作原理：**CTC通过定义一个损失函数来训练模型，该损失函数考虑了所有可能的输入序列到输出序列的映射路径，并通过动态规划的计算方式计算出最有路径的概率。具体来说，CTC使用前向-后向算法（类似于隐马尔科夫模型中的算法）来高效的计算每个输出序列的概率分布。
  - 路径可能性计算：给定一个输入序列，CTC会考虑所有可能的标记序列作为潜在输出，并为每种情况分配一个概率值。
  - 合并相同连续标记：由于CTC允许在输出中插入空白标签以及重复标记，因此需要将这些冗余去除以获得最终的输出序列。
  - 最大化正确输出的概率：在训练过程中，目标是最小化真实标签序列的负对数似然，从而调整模型参数使得正确输出的概率最大化。

- **对比CTC与HMM**
  - CTC：提供了一种无需显式对齐的方式来进行序列预测，适合于端到端的学习场景。它简化了训练过程，因为不需要预先定义输入输出之间的精确对应关系。
  - HMM：需要明确的状态空间和状态转移模式，更适合于传统统计方法下的序列分析任务。不过，它要求对输入输出之间有较为严格的先验知识或者预定义的结构。

##### 单步自回归模型：

- 定义：是一种在序列生成任务中不依赖于先前输出的模型结构。具有推理速度快、资源利用率高的优势，但生成质量略逊于自回归模型。它适用于对延迟敏感、对生成质量容忍度较高的场景，并可通过知识蒸馏、迭代优化等方式提升性能。与传统的 自回归模型（Autoregressive Models, AR）  不同，NAR 模型可以在一步之内或并行地生成整个目标序列。**一次前向传播就预测出整个输出序列** ，不需要像传统模型那样一步步生成 token。



### 15日：

- funASR语音识别离线、实时demo测试，开源模型实时测试效果不理想
- 宠物识别文档归纳，简单语音指令归纳



### 16日：

- 宠物识别开发文档书写：梳理从数据集下载到模型训练、模型转换最后模型部署的全流程


- 中控通信代码查看
  - 查看已发布话题命令：ros2 topic list
  - 表情检测和摔倒检测发布者节点名

- 要在 Python 中使用 `pyaudio` 实现 **PCM 16kHz 单声道**  的音频，将录制的音频保存为 `.wav` 文件，用于后续ASR的识别。



### 17日：

##### Docker Engine检查安装：

- 检查是否安装旧版本：

  ```shell
  sudo apt-get remove docker docker-engine docker.io containerd runc
  ```

- 更新现有的包索引并安装一些必要的包：

  ```shell
  sudo apt-get update
  sudo apt-get install ca-certificates curl gnupg
  ```

- 添加Docker的官方GPG密钥：确认下载的Docker安装包的真实性和完整性，针对Docker本身

  ```shell
  sudo install -m 0755 -d /etc/apt/keyrings
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
  sudo chmod a+r /etc/apt/keyrings/docker.gpg
  ```

- 设置仓库：

  ```shell
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  ```

- 安装Docker Engine：

  ```shell
  sudo apt-get update
  sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  ```

- 验证安装：

  ```shell
  sudo docker run hello-world
  ```

- 管理 Docker 作为非 root 用户：避免每次都使用 `sudo`，可以将当前用户添加到 docker 组中

  ```shell
  sudo usermod -aG docker ${USER}		# 添加用户至docker组
  newgrp docker		# 生效
  ```

##### 音频采集模块：

用于用户语音输入指令，保存至本地供模型识别文字

##### 语音识别模块（ASR）

可使用本地部署和云端（依赖网络）

- API：百度智能云语音识别接口调通（"access_token": "24.5b626df05983e6e85b44a30a119dea96.2592000.1752722976.282335-119258328"）

  ![image-20250617110836831](/home/yls/.config/Typora/typora-user-images/image-20250617110836831.png)

##### 大模型理解与意图识别模块：

- 本地部署：脚本下载模型文件并配置相关库。
  - datsets库中data_files.py文件中确实 get_metadata_patterns 函数：
  - MODULE_SUPPORTS_METADATA缺失：



### 18日：

##### 本地部署大模型：

Ubuntu 上使用 Docker 和 Ollama 本地部署低成本大模型，并通过 VS Code 实现 Python 调用

**docker部署Ollama：**

- 拉取并运行Ollama容器：

  ```shell
  # 拉取 Ollama 镜像（支持 ARM 架构的 RK3588 等设备）
  docker pull ollama/ollama
  
  # 运行 Ollama 容器，映射端口并持久化存储模型数据
  docker run -d -p 11434:11434 -v ollama:/root/.ollama ollama/ollama
  ```

- 验证Ollama服务：

  ```shell
  # 检查容器状态
  docker ps | grep ollama
  
  # 测试 Ollama API（返回空响应表示服务正常）
  curl http://localhost:11434/api/tags
  ```

**Ollama拉取大模型：**

- 拉取轻量级大模型：通过 [Ollama 网站](https://ollama.com/library)下载低资源占用的模型，例如`deepseek-r1:1.5b`（4 位量化版，显存需求约 4GB）：

  ```shell
  # 查看 Ollama 容器状态
  docker ps | grep ollama
  
  # 进入容器终端
  docker exec -it <container_id> /bin/bash 		# <container_id>替换为容器id
  
  # 下载模型（如 llama3:8b-q4_0 或 deepseek-r1:8b-q4_0）
  ollama pull deepseek-r1:1.5b
  
  # 查看模型
  ollama list
  
  # 输出类似
  NAME            SIZE    MODIFIED
  llama3:8b-q4_0  4.0GB   1 minute ago
  
  # 关闭本地运行大模型
  docker stop <container_id>
  ```

**使用VS Code 进行Python开发：**

- 安装Python依赖：本地环境中安装ollama-python客户端库：

  ```shell
  pip install ollama
  ```

- 使用VS Code的远程开发功能

  - 安装 Dev  Containers 扩展 ：在 VS Code 中安装 `Dev Containers` 扩展，直接在 Docker 容器内开发调试 

  - 配置开发环境 ：创建 `.devcontainer/devcontainer.json` 文件，定义 Python 环境和依赖安装脚本。

- 创建调用代码：在 VS Code 中创建脚本（如 `ollama_demo.py`），调用 Ollama 的 API

  ```python
  import ollama
  
  # 调用模型生成响应
  response = ollama.generate(model="llama3:8b-q4_0", prompt="你好，请用中文回答：1+1等于几？")
  print(response["response"])
  ```

- 限制 Docker 容器资源：编辑 Docker 运行参数，限制 CPU 和内存使用（避免影响系统稳定性）：

  ```shell
  docker run -d \
    --cpus="2" \  # 限制使用 2 个 CPU 核心
    -m "4g" \    # 限制内存为 4GB
    -p 11434:11434 \
    -v ollama:/root/.ollama \
    ollama/ollama
  ```

##### 天气查看API调用：

- 心知天气API：密钥申请，API调用测试正常，涉及用户ID、密钥封装

- 大模型天气查询接口整合：终端显示错误：
  - 解决API正常调用返回网址可以正常打开且返回内容正确，但是通过API访问数据后段显示时API拒绝（400）：
  
    **问题在于使用所有参数（包括 sig）拼接 query 去做签名，但 sig 还未生成，此时 params 里没有 sig。签名算法要求：只用参与签名的参数，不含 sig。**
  
    而之后你把 sig 加到 params 里，最后 `urlencode(params)` 会把 sig/ts/public_key等全部参数加到URL里。
  
    但**如果参数顺序、参与签名的参数有问题，API会拒绝（400）。**
  
    ```python
    params['public_key'] = public_key
    params.setdefault('ts', str(int(time.time())))
    query = "&".join(f"{key}={value}" for key, value in sorted(params.items())).encode()
    params['sig'] = b64encode(hmac.new(secret_key.encode(), query, hashlib.sha1).digest()).decode()
    url = f"https://api.seniverse.com/v3/weather/now.json?{urlencode(params)}"
    response = urlopen(url)
    ```
  
    - 官方Seniverse签名流程（重点）
      1. 用所有 **参与签名**的参数（不含 sig），按key排序，拼接成查询字符串（如：`a=xxx&b=yyy&public_key=...&ts=...`）
      2. 用 secret_key 以 HMAC-SHA1 方式对上面字符串做签名
      3. base64编码后得到 sig
      4. 最终请求URL的参数是：所有参数+sig
  
    ```python
    params['public_key'] = public_key
    params.setdefault('ts', str(int(time.time())))
    # 用于签名的参数，不包含sig
    sign_params = {k: v for k, v in params.items()}
    query = "&".join(f"{key}={value}" for key, value in sorted(sign_params.items()))
    sig = b64encode(hmac.new(secret_key.encode(), query.encode(), hashlib.sha1).digest()).decode()
    params['sig'] = sig
    url = f"https://api.seniverse.com/v3/weather/now.json?{urlencode(params)}"
    response = urlopen(url)
    ```
  
  - 实际返回中根本没有`status`字段，`weather_data.get('status')` 得到的是 `None`，所以一直走“错误”分支。导致最终请求API服务失败。



### 19日：

##### 语音指令：

- 离线录音脚本、ASR语音识别模块（离线、接口）、本地大模型部署、天气查询接口测试完成

##### 24服务器视觉模型开发环境搭建：

要在Windows平台上使用VS Code运行YOLO模型，并利用Miniconda和Git等工具来部署环境，你可以按照以下步骤操作。这里以配置YOLO v5为例，其他版本的YOLO可能需要做相应调整。

###### **1. 安装必要软件**

- 安装VS Code: 访问VS Code官网下载并安装适合Windows的版本。

  - 下载插件：python

- 安装Git: 访问Git官网下载并安装Git for Windows，这将帮助你从GitHub上克隆YOLO仓库。

  - Windows操作系统下注意要选择添加系统环境路径

- 安装Miniconda: 访问Miniconda官网下载适合Windows的Miniconda安装包，并进行安装。Miniconda是Anaconda的一个轻量级版本，可以帮助你管理Python环境。

  - 添加系统环境路径：

    打开 **控制面板 > 系统 > 高级系统设置 > 环境变量**，在“系统变量”或“用户变量”中找到 `Path` 变量，点击“编辑”，添加以下路径（根据你的安装目录调整）：

    ```
    C:\Users\<你的用户名>\Miniconda3
    C:\Users\<你的用户名>\Miniconda3\Scripts
    C:\Users\<你的用户名>\Miniconda3\Library\bin
    ```

###### **2. 创建并激活Conda环境**

打开命令提示符或Anaconda Prompt，然后执行以下命令创建一个新的Conda环境（例如名为`yolov5-env`），并激活它：

```shell
初始化添加powershell
conda init powershell		# 重新打开VS Code打开终端

创建虚拟环境
conda create --name YOLO python=3.8		# 确保你的环境中已安装了Python 3.8或更高版本
conda activate YOLO
```

###### **3. 克隆YOLO v5仓库**

在你的环境中，通过Git克隆ultralytics仓库：

```shell
git clone https://github.com/ultralytics
```



### 22日：

##### 环境配置需要的包（requirement.txt）：

opencv-python、numpy、psutil、matplotlib、tqdm、request、pandas、yaml、ultralytics

- torch、cuda安装：

  ```shell
  # 安装 CUDA 支持版本（cu118）
  pip install torch --index-url https://download.pytorch.org/whl/cu118
  
  # 安装 torchvision 和 torchaudio（使用清华源加速）
  pip install torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

  - 安装过程中报错：

  ```SHELL
  WARNING: Connection timed out while downloading.
  ERROR: Could not install packages due to an OSError: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'C:\\Users\\fp\\AppData\\Local\\Temp\\pip-unpack-mwm5k1ev\\torch-2.7.1+cu118-cp39-cp39-win_amd64.whl'
  Consider using the `--user` option or check the permissions.
  ```

  报错信息说明在用pip安装torch等包时，出现了文件被占用（WinError 32），通常是由于以下原因：

  - 临时文件夹内残留旧文件（尤其是 `.whl` 文件），导致pip无法覆盖或访问。
    - 进入目录：`C:\Users\fp\AppData\Local\Temp`删除所有临时文件，或更改pip临时目录

  - 杀毒软件或Windows安全中心拦截了pip的写入操作。
    - 临时关闭杀毒软件、Windows Defender等，再尝试安装

  - 权限问题——尤其是在Miniconda环境下，有时权限设置会导致文件锁定。

  - 磁盘或网络问题——下载大文件时网络不稳定导致文件不完整或被多次尝试访问。

    - 用浏览器下载对应的 `.whl` 文件（[PyTorch官网](https://download.pytorch.org/whl/cu118/torch-2.1.2%2Bcu118-cp39-cp39-win_amd64.whl)）。也可以去[官网](https://download.pytorch.org/whl/torch/)寻找需要的版本
    - 把文件放到如 `D:\tmp` 目录。
    - 用如下命令进行本地安装：

    ```shell
    pip install D:\tmp\torch-2.1.2+cu118-cp39-cp39-win_amd64.whl
    ```

##### weight加载失败：

PyTorch 2.4+ 版本中引入的新安全机制 导致的问题。从 PyTorch 2.6 开始，`torch.load(..., weights_only=True)` 成为了默认行为，目的是为了防止加载模型时执行任意代码（如反序列化恶意代码）。PyTorch 新版本限制了这种加载方式，除非你手动将相关类加入白名单或使用 `weights_only=False` 加载。

##### YOLO数据集配置文件默认验证集名为Val

##### torchision版本问题：

```shell
ValueError: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.

torch.cuda.is_available(): False
torch.cuda.device_count(): 0
os.environ['CUDA_VISIBLE_DEVICES']: None
See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.
```

- [官网](https://download.pytorch.org/whl/cu118/torchvision/)下载指定版本的torchvision，在终端中安装。

- 查看显卡适配的torchision：已3070为例

  ```shell
  pip install torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu118
  ```

##### 数据集标签问题：

- 数据集中标签文件需要与代码中数据集种类一致，从0开始

  ```shell
  RuntimeError: CUDA error: device-side assert triggered
  CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
  For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
  ```

  解决办法：

  - 所有标签文件的类别编号必须全是 `0`。

  - 格式必须是5列，且后四列0-1之间小数。

  - 没有多余、损坏、空白行。

  - 用下面脚本批量检查所有标签文件。

    ```python
    import os
    import glob
    
    # 设置你的标签文件夹路径，支持多个子集（如train/labels, val/labels等）
    label_dirs = [
        r"C:\datasets\blood\train\labels",
        r"C:\datasets\blood\val\labels",
        r"C:\datasets\blood\test\labels"
    ]
    
    # 你希望替换成的类别编号
    new_class_id = 0
    
    def process_label_file(file_path, new_class_id):
        lines_out = []
        changed = False
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue  # 跳过空行
                parts = line.strip().split()
                if len(parts) < 5:
                    print(f"标签格式错误: {file_path} 行: {line.strip()}")
                    continue
                # 修改类别编号
                if parts[0] != str(new_class_id):
                    parts[0] = str(new_class_id)
                    changed = True
                lines_out.append(' '.join(parts))
        # 只在需要时覆盖原文件
        if changed:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines_out) + '\n')
            print(f"已修改: {file_path}")
    
    def batch_modify_labels(label_dirs, new_class_id):
        for label_dir in label_dirs:
            if not os.path.isdir(label_dir):
                print(f"目录不存在: {label_dir}")
                continue
            label_files = glob.glob(os.path.join(label_dir, "*.txt"))
            print(f"{label_dir} 共找到 {len(label_files)} 个标签文件")
            for file_path in label_files:
                process_label_file(file_path, new_class_id)
    
    if __name__ == "__main__":
        batch_modify_labels(label_dirs, new_class_id)
    ```




### 23日：

##### 模型训练OpenCV 试图读取图片时内存分配失败：

- 内存不足
- 图片损坏或格式异常，图片过大
- batch_size调小，寻找阈值

##### 使用Dify完成智能体创建：

- 使用docker[启动本地部署大模型](# 18日：)

- 使用docker compose本地部署dify：

  - docker compose工具：Linux中如果已安装 Docker Engine 20.10.13 及以上，`docker compose` 已作为子命令集成。

    ```shell
    # 查看docker版本
    docker version
    ```

  - 克隆Dify代码仓库：

    ```shell
    # 假设当前最新版本为 0.15.3
    git clone https://github.com/langgenius/dify.git --branch 0.15.3
    ```

  - 启动Dify：

    1. 进入 Dify 源代码的 Docker 目录

       ```shell
       cd dify/docker
       ```

    2. 复制环境配置文件

       ```shell
       cp .env.example .env
       ```

    3. 启动 Docker 容器，详细细节见Dify[官方文档](https://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose)

       ```shell
       docker compose up -d
       
       # 运行命令后，你应该会看到类似以下的输出，显示所有容器的状态和端口映射：
       [+] Running 11/11
        ✔ Network docker_ssrf_proxy_network  Created                                   
        ✔ Network docker_default             Created                                   
        ✔ Container docker-redis-1           Started                                   
        ✔ Container docker-ssrf_proxy-1      Started                                   
        ✔ Container docker-sandbox-1         Started                                   
        ✔ Container docker-web-1             Started                                   
        ✔ Container docker-weaviate-1        Started                                   
        ✔ Container docker-db-1              Started                                   
        ✔ Container docker-api-1             Started                                   
        ✔ Container docker-worker-1          Started                                  
        ✔ Container docker-nginx-1           Started     
        
        # 检查所有容器是不是都正常运行
        docker compose ps
       ```

  - 更新Dify：进入 dify 源代码的 docker 目录，按顺序执行以下命令：

    ```shell
    cd dify/docker
    docker compose down
    git pull origin main
    docker compose pull
    docker compose up -d
    ```

  - 访问Dify：进入下列的地址，设置帐号密码登陆就可以开始愉快的模型应用创作之旅了。

    ```shell
    # 本地环境
    http://localhost/install
    ```

- Dify从右上头像—>设置—>模型供应商—>安装模型供应商设置模型，例如使用ollama拉取deepseek-r1:1.5b

![image-20250623161146787](/home/yls/.config/Typora/typora-user-images/image-20250623161146787.png)



### 24日：

##### Dify创建agent如何添加tool问题

不要在tool那一栏下的marketplace安装，选择在右上角插件处搜索安装。

##### 模型工具适配问题

- deepseek-r1-1.5b模型不支持调用外部接口或工具，需要选择其他模型进行测试

- 如duckduckgo一样的浏览器工具存在抓取网页版结果可能会遇到速率限制问题，导致Dify Agent 已经正确调用了 DuckDuckGo tool，但DuckDuckGo接口返回了限流（RateLimit），最终无法获得结果。

- 网络工具：

  - 本地部署SearXNG：

    1. github拉取代码，使用Dify部署在本地。

    2. SearXNG部署完成但是不能使用报错：

       ```
       Forbidden
       
       You don't have the permission to access the requested resource. It is either read-protected or not readable by the server.
       ```




### 25日：

##### 解决本地部署SearXNG403错误：

需要修改`secret_key`，不允许为默认值，调整了每秒每分允许的请求数，避免在运行时出错，以及search的formats允许返回json格式（一定要加，否则报403错误）。

```yaml
# see https://docs.searxng.org/admin/settings/settings.html#settings-use-default-settings
default_doi_resolver: 'doi.org'		# 可以解决访问500错误
use_default_settings: true
server:
  # base_url is defined in the SEARXNG_BASE_URL environment variable, see .env and docker-compose.yml
  secret_key: "R+PzvVHeg97zDPWbGZKK4RfeVu+ZjZQPhkqsqysoUYU="

  limiter: false  # enable this when running the instance for a public usage on the internet
  image_proxy: true
  api:
    enabled: true
ui:
  static_use_hash: true
redis:
  url: redis://redis:6379/0
search:
  safe_search: 0
  autocomplete: ""
  default_lang: ""
  formats:
    - html
    - json
    - csv
    - rss
ratelimit:
    enabled: true
    # 调整每秒允许的请求数
    per_second: 5
    # 调整每分钟允许的请求数
    per_minute: 60

```

##### 容器端口冲突：

使用docker compose启动Dify容器和SearXNG容器发现有端口冲突，Dify中nginx容器端口为80与SearXNG中caddy容器冲突。

- 解决方案：不修改nginx端口号使用默认80端口，修改Caddy端口号修改结果如下

```yaml
# 修改Caddy配制文件Caddyfile
reverse_proxy localhost:8888 {		# 将端口号修改为8888
	# https://github.com/searx/searx-docker/issues/24
	header_up Connection "close"
}
# 修改SearXNG配置文件settings.yml
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    networks:
      - searxng
    ports:
      - "127.0.0.1:8888:8080"
```



### 26日：

##### 修改searxng配置文件

在dify中配置searxng相关内容，包括settings.yml、limitor.toml、uwsgi.ini使两个容器可以互不干扰的打开

##### 搜索引擎连接问题

dify不能连接本地部署的搜索引擎，但是searxng是可以正常使用的



### 29日：

##### 验证GPU使用

- 重新下载CUDA启动，使用官方的测试文件检查GPU是否被使用，根据NVIDIA CUDA Toolkit 自带的示例程序输出内容显示均正常。

  ```
  C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\extras\demo_suite下bandwidthTest、deviceQuery文件直接拖至cmd界面中执行
  ```

##### agent添加本地部署搜索引擎

- [dify部署](# 23日：)

- 部署searXNG搜索引擎：修改settings.yml文件，docker-compose.yaml文件。

  dify创建智能体连接searXNG填写base url：http://192.168.1.137:8081

  ```yaml
  # settings.yml文件
  use_default_settings: true
  server:
    # base_url is defined in the SEARXNG_BASE_URL environment variable, see .env and docker-compose.yml
    secret_key: "R+PzvVHeg97zDPWbGZKK4RfeVu+ZjZQPhkqsqysoUYU="  # change this!
    limiter: false  # enable this when running the instance for a public usage on the internet
    image_proxy: true
  ui:
    static_use_hash: true
  redis:
    url: redis://redis:6379/0
  search:
    formats:
      - html
      - json
      - rss
  ```

  ```yaml
  # docker-compose.yaml文件
  version: "3.7"
  
  services:
    redis:
      container_name: redis
      image: docker.io/valkey/valkey:8-alpine
      command: valkey-server --save 30 1 --loglevel warning
      restart: unless-stopped
      networks:
        - searxng
      volumes:
        - valkey-data2:/data
      cap_add:
        - SETGID
        - SETUID
        - DAC_OVERRIDE
      logging:
        driver: "json-file"
        options:
          max-size: "1m"
          max-file: "1"
  
    searxng:
      container_name: searxng
      image: docker.io/searxng/searxng:latest
      restart: unless-stopped
      networks:
        - searxng
      ports:
        - "8081:8080"
      volumes:
        - ./searxng:/etc/searxng:rw
      environment:
        - SEARXNG_BASE_URL=http://${SEARXNG_HOSTNAME:-localhost}/
        - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
        - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
      cap_add:
        - CHOWN
        - SETGID
        - SETUID
      logging:
        driver: "json-file"
        options:
          max-size: "1m"
          max-file: "1"
  
  networks:
    searxng:
  
  volumes:
    valkey-data2:
  
  ```



### 30日：

##### 显卡CPU和内存使用问题（数据管道瓶颈）：

- 首先受限于硬件的原因（RTX3070），batch不能调整的太大一般低于64导致显卡一次处理的图片不多使用率自然不高。

- 将图片加载进行预处理时使用内存和CPU这导致两者使用率较GPU更高，尤其是数据准备阶段（CPU在做数据加载、解码、增强等）。

- 双显卡训练问题：修改device参数设置

  ```python
  # 根据显卡数量和任务数量动态调整这两个参数
  model.train(..., device=[0,1], workers=16)
  ```

- 可能是cpu对数据的预处理速度供不上gpu的使用，因此gpu要等待cpu进行数据预处理，待处理好的数据传回来之后gpu再继续工作。

##### 编写agent系统提示词：



## 七月

### 1日：

##### 智能体推理缓慢问题：

使用本地部署的qwen3:8b大模型完成推理执行过程非常缓慢因为参数量太大，连续发出请求发生电脑直接断电的问题触发"过热保护"。同时解释了为什么智能体不能正常工作往往只返回json格式的原始文件而没有正确的输出总结内容

###### 使用Function Calling策略的LLM工作流程：

LLM 在此策略中不仅仅是生成自由格式的文本，而是要生成一个结构化的请求（例如，一个指明函数名称和参数的 JSON 对象），这个请求可以被 Dify 平台或工具本身准确解析并执行 。LLM 是否能够持续稳定地产生符合预定义工具规范的、格式正确的结构化输出，是 Function Calling 策略可靠性的关键。

- **用户输入 (User Query):** 用户向 Agent 发出指令或问题。
- **LLM 意图识别 (LLM Intent Recognition):** Agent 内的 LLM 分析用户输入，理解其真实意图。
- **LLM 工具选择 (LLM Tool Selection):** 根据识别到的意图，LLM 从可用的工具列表中选择最合适的工具来完成任务。
- **LLM 参数提取 (LLM Parameter Extraction):** LLM 从用户输入中提取调用所选工具所需的参数。
- **工具执行 (Tool Execution):** Dify 平台根据 LLM 提供的工具名称和参数，实际执行该工具。
- **结果返回 (Result Returned):** 工具执行的结果会返回给 LLM (在某些场景下) 或直接输出到工作流的下一步。

##### 智能助手实现思路：

###### 语音指令：

联网实现在线搜索和信息爬取。实现多轮聊天，查询天气等。

- 启动服务：通过关键词唤醒服务，具体实现考虑封装成ROS节点和主控联系，主控打开服务器网址启动agent
- 本地语音识别内容上传问题、主控开启agent后怎么做到回答的内容输出

###### 机器人语音控制：

- 启动服务：ROS节点发布指令例如（向前走、向后走、左右拐）
- 考虑精确控制小车移动、躲避障碍物以及如何让小车执行指令

##### 智能体开发：

计划流血检测模型训练完成后在24服务器上部署，本地访问测试使用效果。
将dify容器、searXNG容器开启后通过链接http://localhost/apps进入。

##### 流血检测模型开发：

新思路使用YOLO提供的分割模型yolo-seg对血液的形状进行训练，另外考虑颜色等影响。



### 2日：

##### 关键词触发功能：

创建发布者话题名为asr_exacute，实现主控获取关键词触发信息达到打开语音助手的目的以及机器人控制指令。

完成小车控制指令识别，包括向前、向后、向左、向右以及前进后退。

##### 语音指令开发文档书写



### 3日：

##### 流血检测训练集优化：

YOLOv8n-seg原始模型mAP@50通常在60%-70%区间，经过优化可提升至90%以上。修改数据集重新训练模型

重新下载数据集进行模型训练

##### 智能体怎么提升提问和回答的准确度：

```
user -> query -> embedding模型 -> token + RAG -> cross-encoder（召回） -> LLM -> 回答
 		  ↓
 	输入参数量过大时
LlamaIndex分块提取关键信息 -> 将多模态多渠道信息切块（chunk）,建立索引
								   		  ↓
								   keywords查询Node
```



### 6日：

##### 设置YOLO默认settings.yaml:

```python
from ultralytics.cfg import handle_yolo_settings
handle_yolo_settings([])
```

这会在终端输出当前的所有 Ultralytics 设置，包括 settings.yaml 路径等信息。

##### 设置缓冲区解决windows系统git下载失败问题：

```shell
git config --global http.postBuffer 524288000
```

##### 完成流血检测模型的训练：



### 7日：

##### 正向样本标签：

- 正向样本（positive sample）：指图片中包含你要检测的目标（如血液、猫、车辆等）。对于目标检测（如YOLO），每张正向样本图片都要有一个与之同名的标签文件（.txt），里面详细描述了目标的位置和类别。

  - **类别编号**：目标属于哪个类别（从0开始编号）。

  - **x_center, y_center**：目标的中心点坐标（相对于图片宽高归一化，范围0~1）。

  - **width, height**：目标的宽和高（也是相对于图片宽高归一化，范围0~1）。

  ```
  类别编号 x_center y_center width height
  
  # 举例
  0 0.454 0.550 0.255 0.497
  0 0.484 0.331 0.575 0.650
  ```

​	表示图片中有两个血液目标，分别在不同的位置和大小。

- 模型如何利用这些标签学习：开始训练后，模型读出图片的标签文件。通过表卡文件中的内容学习识别到的类别以及物体的位置信息，模型会尝试预测所有目标框，并和标签对比，优化自己的预测能力。如果**没有标签**内容（空文件），这张图片就被当作**“负样本”**，**模型学会这里没有目标**。

- 标注工具：可视化标注，生成YOLO格式

  [LebalImg](https://github.com/HumanSignal/labelImg)：单类别/多类别物体标注，轻量级需求。支持鼠标框选目标，保存即生成对应txt文件（支持类别自定义）。另外可对标签文件进行修正。

  [Roboflow Annotate](https://roboflow.com/annotate)：网页版，团队协作、自动标注、类别管理强大。团队协作、云端管理、自动标注。

  [Labelme](https://github.com/wkentaro/labelme)：主要用于分割（可导出bbox），支持多边形标注。

  [CVAT](https://github.com/opencv/cvat)：Web端，适合大项目和团队，支持丰富标注类型和批量操作。

##### 使用Labelmg生成标签：

- 安装使用：

  - pip 安装Labelmg

    ```shell
    # 安装运行Labelmg
    pip install labelimg
    labelimg
    ```

  - 源码编译安装：确保你已经安装了 Git、Python、PyQt5 和 lxml

    ```shell
    # 克隆 LabelImg 的 GitHub 仓库
    git clone https://github.com/tzutalin/labelImg.git
    
    # 安装 PyQt5 和 lxml
    pip install pyqt5 lxml
    
    # 编译并运行 LabelImg
    pyrcc5 -o libs/resources.py resources.qrc
    python labelImg.py
    ```

- 修改默认设置：清空并自定义 `predefined_classes.txt`

  - 源码运行的，路径类似：labelImg/data/predefined_classes.txt，删除预设置的类别名。

  - pip 安装的，可以使用如下命令查找路径：然后在输出的 `Location` 目录下找到 `labelImg/data/predefined_classes.txt`

    ```shell
    pip show labelimg
    ```

- 启动 LabelImg 时指定自定义标签文件

  ```shell
  # 参数分别图片文件位置 预定义标签文本位置
  python labelImg.py ./images ./my_labels.txt   
  ```

##### 制作血液检测负样本数据集：

考虑到机器人专供家庭使用并且需要和用户交互，对于负样本需要考虑在家里会出现的常见红色物体最好是抓住与人交互的状态下的照片。

- 类别考虑：红苹果、红樱桃、红辣椒、红玫瑰、红宝石、红气球、红灯笼、红椅子、红杯子、番茄酱（红番茄）、红色指甲油、红蛋糕	
- 其他因素考虑：考虑不同亮度下图片的效果，调整图片亮度；考虑图片旋转剪裁后重新制作成样本。



### 8日：

##### 准备负样本数据集：

红板凳、红墨水、红花盆

##### 测试负样本：

使用模型对负样本数据集进行测试，剔除能够识别的样本将不能识别的样本归纳制作数据集。



### 9日：

##### agent中RAG构建：

学习在智能体中实现长文本总结以及本地知识库搭建

##### 血液检测数据集准备完毕：

- 数据集增设正向数据以及强负样本并添加标签文件，开始新数据集模型训练

- 新模型的精度为0.802可以识别花朵有一定泛化能力



### 10日：

##### 血液检测模型转换测试：

- 自定义模块C3k2缺失问题：

  ```shell
  AttributeError: Can't get attribute 'C3k2' on <module 'ultralytics.nn.modules.block' from '/home/yls/miniconda3/envs/rknn160/lib/python3.8/site-packages/ultralytics/nn/modules/block.py'>
  ```

  - 解决办法：`C3k2` 是 YOLOv8 中新增的模块之一，当前安装的 `ultralytics` 版本太旧，缺少这个模块。

    ```shell
    pip install --upgrade ultralytics
    ```

##### 配置服务器Ollama                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              环境：

- 设备24服务器配置不支持WSL2，不能使用docker-desktop本地部署Ollama

- 官网下载windows版本Ollama安装包

  ```shell
  # 拉取官方支持的大模型
  ollama run <模型名>
  
  # 验证拉取是否成功
  ollama list
  ```

  

### 13日：

##### ‌安装 Infortress 服务端

访问 [Infortress 官网](https://infortress.com/) 下载对应操作系统的服务端程序（Windows/macOS/Linux），双击安装包按向导完成安装。

- 安装路径避免使用系统盘（如 C 盘）‌
- 安装完成后自动生成数据目录 `./Data`（勿手动修改）‌
- 启动并登录‌：打开 Infortress 服务端 → 使用邮箱注册账号 → 进入主界面‌

##### 配置 AnythingLLM 与 Infortress 对接

‌获取 AnythingLLM 的 API 密钥：打开 AnythingLLM → 左下角进入设置 → 找到 ‌API 密钥‌ → 点击【创建新密钥】→ 复制生成的密钥‌

关联密钥到 Infortress：在 Infortress 服务端 → 左侧菜单选择【设置】→ 找到 ‌本地知识库‌ → 粘贴复制的 API 密钥 → 点击保存‌

##### 模型部署

下载Node.js，AnythingLLM 是基于 Node.js 构建的，所以需要安装 Node.js 环境

- 安装Yarn

  ```shell
  npm install -g yarn
  
  # 验证安装
  yarn --version
  ```

  

### 14日：

##### 模型转换并测试：

- 创建rknn工具链虚拟环境：用于将onnx格式文件转化为rknn格式，可根据创建环境所指定的python选择工具链[安装包](/home/yls/rknn-toolkit2/rknn-toolkit2/rknn-toolkit2/packages/x86_64)，安装命令如下。

  ```shell
  pip install rknn_toolkit2-2.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
  ```

- 准备材料：用于转换的模型、推理图片以及转换脚本（x86架构下和arm架构下）

- 开始转换：根据yolov8或者yolo11分别选择转换脚本

  - yolov8：选择rknn_infer_x86.py脚本将onnx格式模型转换测试，rknn_infer.py脚本将脚本转换后部署在开发板测试。

  - yolo11：选择[convert.py](https://github.com/airockchip/rknn_model_zoo/blob/main/examples/yolo11/python/convert.py)脚本转换模型并部署测试

    ```shell
    # 进入/home/yls/YOLOV8-on-RK3588/rk3588/python测试转换脚本
    python convert.py path/to/model.onnx rk3588
    ```

- 模型部署测试：ros2功能包形式在开发板上测试
  - 创建功能包：放入模型，修改后处理逻辑多流血判定进行截图，发布流血话题
  - 目前存在同意对象多次检测截图，以及识别精度过低的情况

##### 整理流血检测模型数据集



### 15日：

##### 血液检测文档书写：

完成血液检测开发档案书写以及后续优化方向。

##### 服务器智能体开发：

完成服务器模型部署，准备开始智能体开发。

- Dify连接模型时存在端口占用问题：Ollama 安装脚本通常会注册一个 systemd 服务（名称为 `ollama`），使用的端口号默认为11434与docker部署冲突。

  ```shell
  curl -fsSL https://ollama.com/install.sh | sh
  
  # 停止服务
  sudo systemctl stop ollama
  
  # 使用docker启动ollama
  docker run -d -p 11434:11434 -v ollama:/root/.ollama ollama/ollama
  ```




### 16日：

##### 智能体创建：

Dify调用ollama本地部署的模型完成智能体框架的创建

##### 智能能体推理缓慢：

对于一般性问题可以正常回答，但是推理速度很慢

###### 中文输入问题：

虚拟机内浏览器不能使用中文输入，环境是 ubuntu 22.04，在火狐浏览器中。

```shell
lsb_release -a 		# 查看ubuntu版本信息
```

###### 驱动更新：

ubuntu 默认驱动 Nouveau 不能完全发挥显卡性能

1.  彻底清理旧驱动：

   ```bash
   # 卸载所有 NVIDIA 相关组件
   sudo apt purge *nvidia* *cuda* *cudnn*
   sudo apt autoremove --purge
   sudo apt autoclean
   
   # 删除残留文件
   sudo rm -rf /etc/apt/sources.list.d/cuda*
   sudo rm -rf /usr/local/cuda*
   sudo rm -rf ~/.nv/
   ```

2. 禁用 Nouveau 驱动：

   ```bash
   # 创建黑名单文件
   sudo nano /etc/modprobe.d/blacklist-nouveau.conf
   
   # 添加内容
   blacklist nouveau
   options nouveau modeset=0
   
   # 完成后执行
   sudo update-initramfs -u
   sudo reboot
   ```

3. 安装推荐驱动（带 Secure Boot 支持）

   ```bash
   # 添加官方 NVIDIA PPA
   sudo add-apt-repository ppa:graphics-drivers/ppa
   sudo apt update
   
   # 安装推荐驱动（带开源内核模块）
   sudo apt install nvidia-driver-570-server nvidia-dkms-570-server
   ```

4. 处理 Secure Boot

   ```bash
   # 启用安全启动支持
   sudo apt install mokutil
   
   # 创建 MOK 密钥（安装过程中会提示设置密码）
   sudo update-secureboot-policy --enroll-key
   ```

5. 重启并完成 MOK 注册

   ```bash
   sudo reboot
   ```

   - 重启后重要步骤：

     系统会进入 蓝色 MOK 管理界面

     选择 `Enroll MOK` > `Continue` > `Yes`

     输入之前设置的密码

     选择 `OK` 并重启

6. 验证安装：

   ```bash
   # 检查驱动加载
   nvidia-smi
   
   # 验证内核模块
   lsmod | grep nvidia
   
   # 检查安全启动状态
   mokutil --sb-state
   ```

##### docker默认不支持使用显卡问题：

模型不能只考虑需要的内存大小，参数量决定了显卡推理的速度，即使使用A100跑满依旧存在响应缓慢的问题 （17日使用A100测试deepseek-r1:32b模型）



### 17日：

##### 安装 NVIDIA 容器工具：

解决模型推理缓慢问题，docker部署的dify默认不支持使用本地 GPU ，即使安装了正确的显卡驱动也会检测不到设备，需要用 docker 部署 [NVIDIA 容器工具](https://github.com/NVIDIA/libnvidia-container)让dify 接入的 ollama 拉取的模型可以使用本地显卡进行推理。

- 检查是否安装 NVIDIA 驱动：

  ```bash
  # 检查驱动是否已安装
  nvidia-smi  # 正常输出表示驱动正常
  
  # 若未安装，按以下步骤操作：
  # 添加 NVIDIA 仓库（Ubuntu 示例）
  sudo apt update && sudo apt install -y ubuntu-drivers-common
  sudo ubuntu-drivers autoinstall
  sudo reboot
  ```

- 安装 NVIDIA 容器工具：

  ```bash
  # 获取 OS 信息
  distribution=$(. /etc/os-release; echo $ID$VERSION_ID)
  # 添加 GPG 密钥到信任环
  curl -fsSL  https://nvidia.github.io/libnvidia-container/gpgkey  | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
  # 添加仓库地址到 APT 源
  curl -s -L "https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list " | \
    sed "s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g" | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
  
  # 安装 nvidia-docker2（关键步骤）
  sudo apt update				# 注意这两条指令要同时执行，否则会报错E: 无法定位软件包 nvidia-container-toolkit
  sudo apt install -y nvidia-container-toolkit
  
  # 验证安装
  nvidia-ctk --version
  NVIDIA Container Toolkit version X.X.X		# 输出示例
  ```

  - docker测试 NVIDIA 容器时遇到

    ```shell
    # 测试指令
    sudo docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
    
    # 报错
    Unable to find image 'nvidia/cuda:12.0-base' locally
    docker: Error response from daemon: manifest for nvidia/cuda:12.0-base not found: manifest unknown: manifest unknown
    
    # 正确输出示例
    nvidia/cuda    12.2.0-runtime-ubuntu22.04   5f87b1ee9f9a   20 months ago   1.97GB
    
    # 进入已有容器查看是否能手动调用 nvidia-smi
    sudo docker run -it --rm --gpus all nvidia/cuda:12.2.0-runtime-ubuntu22.04 bash
    which nvidia-smi
    ```

    解决办法：运行以下命令查看本地已有的镜像，或者更新 NVIDIA 官方的 CUDA 镜像

    ```bash
    # 检查本地镜像
    docker images | grep nvidia/cuda
    
    # 下载官方镜像
    sudo docker run --rm --gpus all nvidia/cuda:12.4.0-base nvidia-smi
    ```

- 配置 Docker 使用 GPU：

  ```bash
  # 查看本地已有的镜像
  docker images
  
  # 使用支持 CUDA 的 Ollama Docker 镜像
  docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
  ```

  

##### 智能体网络和知识库：

###### 配置 Google 搜索引擎：

- 获取 API 密钥：支持科学上网情况下前往https://serpapi.com网页申请免费搜索引擎 API ，前期每月共有100次的免费使用额度。SerpAPI 申请免费的 API 支持调用 Google 搜索引擎。只支持内网情况下使用 duckduckgo 测试联网搜索功能。

- 配置工具密钥：

  - 下载 Google 工具：进入 Dify Marketplace 搜索 Google下载，在智能体创建界面应用将 SerpAPI 的密钥配置到密钥框中。其他工具过程类似。

  - 报错：

    1. Ollama 拉取的 deepseek-r1:14b 模型不支持“tools”功能（即不支持 Function Calling），因此无法调用 Dify 的时间工具或任何其他工具。

       ```bash
       [ollama] Error: PluginInvokeError: {"args":{"description":"[models] Error: API request failed with status code 400: {\"error\":\"registry.ollama.ai/library/deepseek-r1:14b does not support tools\"}"},"error_type":"InvokeError","message":"[models] Error: API request failed with status code 400: {\"error\":\"registry.ollama.ai/library/deepseek-r1:14b does not support tools\"}"}
       
       # 模型不支持使用工具需要修改部署的模型  尝试拉取其他支持的模型比如qwen3:14b
       ```

    2. 

###### 配置知识库：

- 运行embedding模型：选择 ollama [官网](https://ollama.com/search?c=embedding)中合适的 Embedding 模型，用于测试开发的智能体选择了 nomic-embed-text:v1.5，如下图所示：

  ```bash
  # 拉取 embedding 模型
  ollama pull nomic-embed-text:v1.5
  ```

  ![image-20250717155212260](/home/yls/.config/Typora/typora-user-images/image-20250717155212260.png)

- 准备自定义知识库：选择中控语音通话的开发文档来测试模型基于本地知识库回答问题的能力。测试文件 pjusa2基本认识.md

  ```
  顶端导航栏选择知识库	->	创建新知识库	->	导入新文件（支持 TXT、 MARKDOWN、 MDX、 PDF、 HTML、 XLSX、 XLS、 DOCX、 CSV、 VTT、 PROPERTIES、 MD、 HTM，每个文件不超过 15MB。）	->	选择 Embedding 模型根据提示对文件进行分块
  ```

  注：知识库中不能添加图片，否则模型总结知识库时会报错。

  ​	模型存在幻觉对未知的知识有胡乱回答的可能。

​		正式使用知识库时采用 md 文档形式

​		![知识库](/home/yls/图片/知识库.png)



##### 设计智能助手开场词：

- 简单自我介绍
- 默认设置可能提问的问题



### 20日：

- threshold 参数：用于判断无语音段落的阈值。

- VAD：从包含语音的一段信号中准确地确定语音的起始点和终止点，区分语音和非语音信号，它是语音处理技术中的一个重要方面

##### 语音助手理论学习：

学习语音助手语音识别以及语音输出的原理

##### 模型 API 申请

尝试连接 API 测试语音识别以及语音转录

- 申请API：登陆阿里云百炼平台注册账号并进行实名认证

- 创建 API-Key ：模型界面选择需要的模型，在左下角 API-Key 处选择创建 API 。

  ![image-20250720145618931](/home/yls/.config/Typora/typora-user-images/image-20250720145618931.png)

- 配置环境：

  - [配置API Key](https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2803795.html&renderType=iframe)到环境变量：

    1. Linux环境下：执行以下命令来将环境变量设置追加到`~/.bashrc `文件中。

       ```bash
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       echo "export DASHSCOPE_API_KEY='YOUR_DASHSCOPE_API_KEY'" >> ~/.bashrc
       
       # 执行以下命令，使变更生效。
       source ~/.bashrc
       
       # 重新打开一个终端窗口，运行以下命令检查环境变量是否生效。
       echo $DASHSCOPE_API_KEY
       ```

    2. Windows环境下：在Windows系统中，您可以通过系统属性、CMD或PowerShell配置环境变量。

       ```cmd
       # 1. 在CMD中运行以下命令。
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       setx DASHSCOPE_API_KEY "YOUR_DASHSCOPE_API_KEY"
       
       # 在新的CMD窗口运行以下命令，检查环境变量是否生效。正确结果如下图所示
       echo %DASHSCOPE_API_KEY%
       ```

       ![](/media/yls/1T硬盘4/picture/API_cmd.png)

       ```powershell
       # 2. 在PowerShell中运行以下命令。
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       [Environment]::SetEnvironmentVariable("DASHSCOPE_API_KEY", "YOUR_DASHSCOPE_API_KEY", [EnvironmentVariableTarget]::User)
       
       # 在新的PowerShell窗口运行以下命令，检查环境变量是否生效。正确结果如下图所示
       echo $env:DASHSCOPE_API_KEY
       ```

       ![](/media/yls/1T硬盘4/picture/API_PowerShell.png)

    3. Mac环境下：如果您希望API Key环境变量在当前用户的所有新会话中生效，可以添加永久性环境变量。

       ```bash
       # 1. 在终端中执行以下命令，查看默认Shell类型。
       echo $SHELL
       
       # 2. 根据默认Shell类型进行操作。
       
       a.执行以下命令来将环境变量设置追加到 ~/.bash_profile 文件中。
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       echo "export DASHSCOPE_API_KEY='YOUR_DASHSCOPE_API_KEY'" >> ~/.bash_profile
       
       # 执行以下命令，使变更生效。并重新打开一个终端检查是否设置成功
       source ~/.bash_profile
       echo $DASHSCOPE_API_KEY
       
       b.执行以下命令来将环境变量设置追加到 ~/.zshrc 文件中。
       # 用您的百炼API Key代替YOUR_DASHSCOPE_API_KEY
       echo "export DASHSCOPE_API_KEY='YOUR_DASHSCOPE_API_KEY'" >> ~/.zshrc
       
       # 执行以下命令，使变更生效。并重新打开一个终端检查是否设置成功
       source ~/.bash_profile
       echo $DASHSCOPE_API_KEY
       ```

  - [配置SDK](https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2712193.html&renderType=iframe)：阿里云百炼官方提供了 Python 与 Java 编程语言的 SDK，也提供了与 OpenAI 兼容的调用方式（OpenAI 官方提供了  Python、Node.js、Java、Go 等 SDK）。本文为您介绍如何安装 OpenAI SDK 以及 DashScope SDK。

    ```shell
    # 如果运行失败，您可以将pip替换成pip3再运行
    pip install -U openai
    ```




### 21日：

##### 优化浏览器搜索

用 Dify + Ollama + DuckDuckGo 的方式做联网搜索，之所以可以搜新闻却“搜不到天气”，大概率是因为 DuckDuckGo 的搜索结果里对“天气”这类查询返回的是**卡片化/结构化数据**（例如只给出“今日天气请见 weather.com”并附一个外链），而不是一段可直接拿来当答案的文本。LLM 收到这种结果后，只能把链接抛给用户，于是看起来就像“搜索失败”。

- 解决方案：通过自定义工具形式形式访问

  1. 申请 API Key ：可以选择 高徳、和风以及彩云三家均提供免费查询额度，笔者试了和风和彩云分别提供50000以及10000额度每月其中彩云支持设置额度提醒功能好评。

     <img src="/home/yls/.config/Typora/typora-user-images/image-20250721100353532.png" alt="image-20250721100353532" style="zoom: 50%;" />

  2.  创建自定义工具：

     - 选择自定义工具新建工具根据模板修改结构，这一步可以使用 [Swagger](https://editor.swagger.io/) 或者直接让AI生成，注意不要泄漏接口密钥，生成示例如下：

       ```json
       {
         "openapi": "3.1.0",
         "info": {
           "title": "Weather Query",
           "description": "根据城市名返回实时天气",
           "version": "v1.0.0"
         },
         "servers": [{ "url": "https://wttr.in" }],
         "paths": {
           "/{location}": {
             "get": {
               "operationId": "getWeather",
               "summary": "获取天气",
               "parameters": [
                 {
                   "name": "location",
                   "in": "path",
                   "required": true,
                   "schema": { "type": "string" },
                   "description": "城市名，例如 Beijing"
                 },
                 {
                   "name": "format",
                   "in": "query",
                   "schema": { "type": "string", "default": "%l:+%c+%t+%h" },
                   "description": "返回格式"
                 }
               ],
               "responses": {
                 "200": {
                   "description": "纯文本天气信息"
                 }
               }
             }
           }
         }
       }
       ```

     - 如接口需鉴权，点击「设置鉴权」选择 API Key 通常选择 Bearer 填写 token
     - 测试接口功能是否正常：点击右下角 「测试」输入参数 `location = Shanghai`能看到返回 `Shanghai: ⛅ +33°C 湿度 59%` 即成功

- 更换浏览器工具：替换为 Google 可以做到浏览器信息查询得到正确数据。

##### 语音输入输出



### 22日：

##### 语音输入设置：

###### 接口调用

- 使用 Chat GPT 接口调用语音服务

###### 本地部署

- Xinference 本地部署语音识别以及 TTS 模型

##### 源码部署 Dify：

###### 中间件启动

- **源代码本地启动：**

  前提：设置 Docker 和 Docker Compose，在安装 Dify 之前，请确保您的设备符合。使用 Miniconda 创建 Python >3.12 虚拟环境。

  ```
      CPU >= 2 核
      RAM >= 4 GiB
  ```

  - **克隆 Dify 仓库：**运行 git 命令克隆 [Dify 仓库](https://github.com/langgenius/dify)。

    ```bash
    git clone https://github.com/langgenius/dify.git
    ```

  - **使用 Docker Compose 启动中间件**

    Dify 后端服务需要一系列用于存储（如 PostgreSQL / Redis / Weaviate（如果本地不可用））和扩展能力（如 Dify 的 [sandbox](https://github.com/langgenius/dify-sandbox) 和 [plugin-daemon](https://github.com/langgenius/dify-plugin-daemon) 服务）的中间件。通过运行以下命令使用 Docker Compose 启动中间件：

    ```bash
    cd docker
    cp middleware.env.example middleware.env
    docker compose -f docker-compose.middleware.yaml up -d
    ```

  - **设置后端服务：**

    1. API 服务：为前端服务和 API 访问提供 API 请求服务
    2. Worker 服务：为数据集处理、工作区、清理等异步任务提供服务

    - 环境准备：使用 Miniconda 创建 Python 3.12环境

      ```bash
      conda create -n env python=3.12
      ```

    - 启动 API 服务：进入 API 文件夹启动 Dify 后端服务

      ```bash
      # 切换到 api 目录
      cd dify/api
      
      # 准备环境变量配置文件
      cp .env.example .env
      
      # 生成随机密钥并替换 .env 文件中的 SECRET_KEY 值
      awk -v key="$(openssl rand -base64 42)" '/^SECRET_KEY=/ {sub(/=.*/, "=" key)} 1' .env > temp_env && mv temp_env .env
      
      # 安装依赖 使用 uv 管理依赖。 通过运行以下命令使用 uv 安装所需依赖
      pip install nv 
      uv sync 			# 对于 macOS：使用 brew install libmagic 安装 libmagic。
      
      # 执行数据库迁移 执行数据库迁移到最新版本
      uv run flask db upgrade
      
      # 启动 API 服务
      uv run flask run --host 0.0.0.0 --port=5001 --debug
      
      # 预期示例
      * Debug mode: on
      INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
       * Running on all addresses (0.0.0.0)
       * Running on http://127.0.0.1:5001
      INFO:werkzeug:Press CTRL+C to quit
      INFO:werkzeug: * Restarting with stat
      WARNING:werkzeug: * Debugger is active!
      INFO:werkzeug: * Debugger PIN: 695-801-919
      ```

  - 启动 Worker 服务：

    要从队列中消费异步任务，例如数据集文件导入和数据集文档更新，请按照以下步骤启动 Worker 服务

    ```bash
    # 对于 macOS 或 Linux
    uv run celery -A app.celery worker -P gevent -c 1 --loglevel INFO -Q dataset,generation,mail,ops_trace
    
    # 对于 Windows
    uv run celery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail,ops_trace --loglevel INFO
    
    # 预期输出
    -------------- celery@bwdeMacBook-Pro-2.local v5.4.0 (opalescent)
    --- ***** -----
    -- ******* ---- macOS-15.4.1-arm64-arm-64bit 2025-04-28 17:07:14
    - *** --- * ---
    - ** ---------- [config]
    - ** ---------- .> app:         app_factory:0x1439e8590
    - ** ---------- .> transport:   redis://:**@localhost:6379/1
    - ** ---------- .> results:     postgresql://postgres:**@localhost:5432/dify
    - *** --- * --- .> concurrency: 1 (gevent)
      -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)
      --- ***** -----
      -------------- [queues]
      .> dataset          exchange=dataset(direct) key=dataset
      .> generation       exchange=generation(direct) key=generation
      .> mail             exchange=mail(direct) key=mail
      .> ops_trace        exchange=ops_trace(direct) key=ops_trace
    
    [tasks]
    . schedule.clean_embedding_cache_task.clean_embedding_cache_task
    . schedule.clean_messages.clean_messages
    . schedule.clean_unused_datasets_task.clean_unused_datasets_task
    . schedule.create_tidb_serverless_task.create_tidb_serverless_task
    . schedule.mail_clean_document_notify_task.mail_clean_document_notify_task
    . schedule.update_tidb_serverless_status_task.update_tidb_serverless_status_task
    . tasks.add_document_to_index_task.add_document_to_index_task
    . tasks.annotation.add_annotation_to_index_task.add_annotation_to_index_task
    . tasks.annotation.batch_import_annotations_task.batch_import_annotations_task
    . tasks.annotation.delete_annotation_index_task.delete_annotation_index_task
    . tasks.annotation.disable_annotation_reply_task.disable_annotation_reply_task
    . tasks.annotation.enable_annotation_reply_task.enable_annotation_reply_task
    . tasks.annotation.update_annotation_to_index_task.update_annotation_to_index_task
    . tasks.batch_clean_document_task.batch_clean_document_task
    . tasks.batch_create_segment_to_index_task.batch_create_segment_to_index_task
    . tasks.clean_dataset_task.clean_dataset_task
    . tasks.clean_document_task.clean_document_task
    . tasks.clean_notion_document_task.clean_notion_document_task
    . tasks.deal_dataset_vector_index_task.deal_dataset_vector_index_task
    . tasks.delete_account_task.delete_account_task
    . tasks.delete_segment_from_index_task.delete_segment_from_index_task
    . tasks.disable_segment_from_index_task.disable_segment_from_index_task
    . tasks.disable_segments_from_index_task.disable_segments_from_index_task
    . tasks.document_indexing_sync_task.document_indexing_sync_task
    . tasks.document_indexing_task.document_indexing_task
    . tasks.document_indexing_update_task.document_indexing_update_task
    . tasks.duplicate_document_indexing_task.duplicate_document_indexing_task
    . tasks.enable_segments_to_index_task.enable_segments_to_index_task
    . tasks.mail_account_deletion_task.send_account_deletion_verification_code
    . tasks.mail_account_deletion_task.send_deletion_success_task
    . tasks.mail_email_code_login.send_email_code_login_mail_task
    . tasks.mail_invite_member_task.send_invite_member_mail_task
    . tasks.mail_reset_password_task.send_reset_password_mail_task
    . tasks.ops_trace_task.process_trace_tasks
    . tasks.recover_document_indexing_task.recover_document_indexing_task
    . tasks.remove_app_and_related_data_task.remove_app_and_related_data_task
    . tasks.remove_document_from_index_task.remove_document_from_index_task
    . tasks.retry_document_indexing_task.retry_document_indexing_task
    . tasks.sync_website_document_indexing_task.sync_website_document_indexing_task
    
    2025-04-28 17:07:14,681 INFO [connection.py:22]  Connected to redis://:**@localhost:6379/1
    2025-04-28 17:07:14,684 INFO [mingle.py:40]  mingle: searching for neighbors
    2025-04-28 17:07:15,704 INFO [mingle.py:49]  mingle: all alone
    2025-04-28 17:07:15,733 INFO [worker.py:175]  celery@bwdeMacBook-Pro-2.local ready.
    2025-04-28 17:07:15,742 INFO [pidbox.py:111]  pidbox: Connected to redis://:**@localhost:6379/1.
    ```

  - 设置 Web 服务：启动用于前端页面的 web 服务。

    环境准备：要启动 web 前端服务，需要 [Node.js v22 (LTS)](http://nodejs.org/) 和 [PNPM v10](https://pnpm.io/)。

    ```bash
    # 安装 nvm
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash
    source ~/.bashrc        # 重新加载 shell
    
    # 安装最新 LTS（长期支持）版本
    nvm install --lts
    nvm use --lts
    
    # 验证
    node -v     # 应输出 v20.x 或 v18.x
    npm  -v
    
    # 安装pnpm
    npm i -g pnpm
    ```

    - 启动 Web 服务：

      ```bash
      # 进入 web 目录
      cd ../web
      
      # 安装依赖
      pnpm install --frozen-lockfile
      
      # 准备环境变量配置文件 在当前目录中创建一个名为 .env.local 的文件，并从 .env.example 复制内容。根据您的需求修改这些环境变量的值
      # For production release, change this to PRODUCTION
      NEXT_PUBLIC_DEPLOY_ENV=DEVELOPMENT
      # The deployment edition, SELF_HOSTED or CLOUD
      NEXT_PUBLIC_EDITION=SELF_HOSTED
      # The base URL of console application, refers to the Console base URL of WEB service if console domain is
      # different from api or web app domain.
      # example: http://cloud.dify.ai/console/api
      NEXT_PUBLIC_API_PREFIX=http://localhost:5001/console/api
      # The URL for Web APP, refers to the Web App base URL of WEB service if web app domain is different from
      # console or api domain.
      # example: http://udify.app/api
      NEXT_PUBLIC_PUBLIC_API_PREFIX=http://localhost:5001/api
      
      # SENTRY
      NEXT_PUBLIC_SENTRY_DSN=
      NEXT_PUBLIC_SENTRY_ORG=
      NEXT_PUBLIC_SENTRY_PROJECT=
      
      # 构建 web 服务
      pnpm build
      
      # 启动 web 服务
      pnpm start
      
      # 预期输出
      ▲ Next.js 15
         - Local:        http://localhost:3000
         - Network:      http://0.0.0.0:3000
      
       ✓ Starting...
       ✓ Ready in 73ms
      ```

  - 访问 Dify：

    初次登录通过浏览器访问 [http://127.0.0.1:3000](http://127.0.0.1:3000/) 即可享受 Dify 所有激动人心的功能。后续在同一台设备上通过http://127.0.0.1:3000/apps登陆使用。



### 23日：

##### 本地部署 TTS 模型：

- 克隆源码至本地：

  ```bash
  git clone https://github.com/2noise/ChatTTS
  ```

- 本地部署 TTS 模型：

  ```bash
  conda create --name xinference python=3.10
  conda activate xinference
  pip install ChatTTS -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install xinference -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

- 安装成功后，只需要输入如下命令，就可以在服务上启动 Xinference 服务：

  ```bash
  xinference-local -H 0.0.0.0
  ```

  Xinference 默认会在本地启动服务，端口默认为 9997。因为这里配置了-H 0.0.0.0参数，非本地客户端也可以通过机器的 IP 地址来访问 Xinference 服务。启动成功后可以通过 http://0.0.0.0:9777来访问 Xinference 的 WebGUI 界面了。



### 24日：

#### 大模型超参数 Temperature ：

在生成式语言模型中，Temperature 参数控制了模型[生成文本](https://so.csdn.net/so/search?q=生成文本&spm=1001.2101.3001.7020)时的`多样性和随机性`。简单来说，Temperature 参数决定了模型在生成下一个单词时，选择概率的分布是否平滑或者更加尖锐。这个参数本质上是一个对模型[概率分布](https://so.csdn.net/so/search?q=概率分布&spm=1001.2101.3001.7020)的重新缩放因子，用来调整输出的熵值，进而影响输出的随机程度。较高的Temperature参数会使模型更具"创造性"，例如在生成散文时可能很有用。而较低的Temperature参数会让模型更具"确定性"，这在问答类应用场景中非常实用。

##### Dify 工具实现语音 IO：

使用硅基流动接口接入 FunAudioLLM/SenseVoiceSmall STT模型、FunAudioLLM/CosyVoice2-0.5B TTS 模型。

###### 模型回答问题：

- 问题描述：Dify 接入的大模型（如 DeepSeek-R1、QwQ 等）**强制开启“深度思考模式”**，导致：
  1. 响应时间过长；
  2. 思考过程被展示给用户；
  3. TTS 语音播报时从思考内容开始读起，体验极差。

- 解决方案：

  1. 使用 Qwen3 模型（推荐）：Qwen3 支持关闭思考模式，可通过以下方式实现

     | 方法           | 操作                                                    | 说明               |
     | -------------- | ------------------------------------------------------- | ------------------ |
     | **参数控制**   | 在 Dify 的模型配置中，添加参数：`enable_thinking=false` | 完全关闭思考过程   |
     | **提示词控制** | 在 prompt 中加入 `/no_think`                            | 动态关闭当前轮思考 |

  2. 使用 DeepSeek-R1 / QwQ 模型（无法关闭思考）：由于这些模型**不支持关闭思考**，需通过**后处理**屏蔽思考内容，模型仅输出结果将过滤后的文本传给 TTS 模型，**避免思考内容被语音播报**

     | 步骤                  | 操作                                        |
     | --------------------- | ------------------------------------------- |
     | **1. 添加工作流节点** | 在 Dify 工作流中，添加一个 **代码执行节点** |
     | **2. 过滤思考内容**   | 使用正则表达式删除 `<think>...</think>` 块  |

     ```python
     # 代码示例
     import re
     
     def main(text: str) -> dict:
         cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
         return {"result": cleaned.strip()}
     ```

###### 接入硅基流动 TTS 模型：

- 前往硅基流动官网申请 API 密钥，在 Dify 模型供应商中配置

- 测试 TTS 模型：

  - 获取 Dify 访问 Token：

    1. docker 直接部署：登录 Dify → 右上角用户头像 → **「设置」→「访问令牌」→ 复制 Token**。

       ```bash
       curl -X POST 'http://localhost:5001/console/api/apps/c793a193-c520-4f64-9885-3de3a0f96de3/text-to-audio' \
         -H 'Authorization: Bearer <这里粘贴控制台 token>' \
         -H 'Content-Type: application/json' \
         -d '{"text":"你好世界","voice":"zh-CN-XiaoxiaoNeural"}' \
         --output test.mp3
       ```

    2. 中间件源码本地部署：直接调用登录接口拿 token，将 token 取代<这里粘贴控制台 token>，调用成功返回一段示例音频。

       ```bash
       curl -X POST http://localhost:5001/console/api/login \
         -H 'Content-Type: application/json' \
         -d '{"email":"你的管理员邮箱","password":"你的密码"}'
         
         
       # 成功返回示例
       {
         "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
         "token_type": "Bearer"
       }
       ```

       

### 27日：

##### TTS 输出问题：

使用硅基流动密钥调用 STT、TTS 模型接口给 Dify 创建的智能体赋能，测试过程中发现在 STT 模型可以正常使用但是 TTS 模型不论是语音试听还是生成回答问题的语音均显示加载中。下面是问题排查的具体步骤：

- 测试后端网页请求：基于后端服务没有报错确定是因为前端请求后端服务时出错。

  ```bash
  curl -X POST http://127.0.0.1:5001/console/api/apps/c793a193-c520-4f64-9885-3de3a0f96de3/text-to-audio \ -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoiNzFkYTA5NzktMGNlYy00ODMwLTllNjUtYjZjM2YyMDU0ODAxIiwiZXhwIjoxNzUzMzUzMjU2LCJpc3MiOiJTRUxGX0hPU1RFRCIsInN1YiI6IkNvbnNvbGUgQVBJIFBhc3Nwb3J0In0.xkdWZLDXqEAW0xqIENYXdnm0PUOmab-ql9o6bGCvu1E" \
    -H "Content-Type: application/json" \
    -d '{"text":"你好世界","voice":"zh-CN-XiaoxiaoNeural"}' \
    --output ok.mp3
  ```

- 前端网页请求问题：OPTIONS 请求没有返回 → CORS 预检被后端直接拦掉，浏览器拿不到 `Access-Control-Allow-*` 头，于是后续真正的 POST 也被阻断。

  ```bash
  # 在 Flask 版 Dify 里，只要给 /console/api/* 加上 CORS 中间件即可
  pip install flask-cors
  ```

  ```python
  from flask_cors import CORS
  ...
  CORS(app, resources={
      r"/console/api/*": {
          "origins": ["http://localhost:3000"],
          "allow_headers": ["Authorization", "Content-Type"],
          "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
      }
  })
  ```

- 浏览器音频拦截：Firefox + NextJS 前端 对 MP3 通过 MediaSource 播放 的支持不完整所致。

  ```bash
  # 网页日志报错内容
  MediaSource.addSourceBuffer: Type not supported in MediaSource  
  Cannot play media. No decoders for requested formats: audio/mpeg
  ```

  解决办法：启动后端服务使用 Google 浏览器打开 http://127.0.0.1:3000/apps 进入开发界面。

##### 调用本地部署 TTS 模型

本地部署 xinference 平台为 Dify 提供模型接口，可以使用 docker 部署或者源码部署，所需空间很大保证挂载的空间大于100G。

- docker 部署[参考](https://blog.csdn.net/zero_forever_lzm/article/details/147305634?ops_request_misc=&request_id=&biz_id=102&utm_term=ubuntu%20xinference%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2%20%E5%B9%B6%E4%BD%BF%E7%94%A8xinfe&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-147305634.142^v102^control&spm=1018.2226.3001.4187)

- 源码部署：可以通过 pip 来安装，如果需要使用 Xinference 进行模型推理，可以根据不同的模型指定不同的引擎，参考[官方文档](https://inference.readthedocs.io/zh-cn/latest/getting_started/installation.html#installation)：

  ```bash
  # 安装需要的依赖
  pip install "xinference[all]"
  
  # 启动服务
  nohup xinference-local --host 0.0.0.0 --port 8892 & > output.log
  ```

  - 下载模型失败：[address=0.0.0.0:43795, pid=832165] The ChatTTS model is not correct: /home/yls/.xinference/cache/v2/ChatTTS



### 28日：

##### 完善语音助手

完成服务器上智能语音助手开发包括：多论对话问答、知识库检索、语音输入输出功能。其中 LLM 使用本地部署 Qwen3:8b、Embedding 模型使用本地部署 nomic-embed-text:v1.5、STT 、TTS 模型采用接口形式实现。

##### 后端实现音频模型调用

使用 Langchain 代码实现接口或本地模型给dify提供音频模型服务。

##### 智能体数字人构建



### 29日：

##### 构建智能聊天机器人：

前端使用gradio构建，gradio是一个开源的Python库，旨在快速构建机器学习模型的交互式网页界面。它允许用户通过简单的几行代码创建可视化的机器学习模型演示项目。希望后续项目可以拓展为 LangChain + Gradio 实现的语音智能机器人，模型依赖接口也可以选择本地部署接入为中控调用提供接口。

###### 开发过程中遇到的 bug：

```
ValueError: When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.
```

- 问题描述：前端部署依赖 gradio Pyton开源库，由于虚拟环境中配置的 python 版本为3.9，该版本支持的库最高版本为 4.44.1，此版本不支持聊天机器人的前端部署。需要升级 python版本。[详情见](https://pypi.org/project/gradio/)

  ![image-20250729164738179](/home/yls/.config/Typora/typora-user-images/image-20250729164738179.png)

  ```bash
  # miniconda 创建的虚拟环境中的 python 版本支持升级，可能需要重新安装某些包
  conda install python=3.10
  ```

##### 数字人开发：



### 30日：

##### 智能体应用：

######  1. 基于大语言模型（LLM）的任务规划 Agent 设计 

- **提示工程** ：利用 Chain-of-Thought（思维链）等提示工程技术，引导 LLM 将复杂任务分解为多个子步骤，逐步推理解决问题的路径，提升任务规划的逻辑性和可行性。
- **外部工具调用** ：集成函数 API 等外部工具，使 Agent 能够调用数据库查询、搜索引擎搜索等功能，获取外部信息辅助任务规划，拓展其知识边界和规划能力。
- **记忆管理** ：借助向量数据库（如 Pinecone）存储长期上下文信息，让 Agent 能够在任务规划过程中参考历史经验、长期目标等，实现更连贯、更全面的任务规划，避免因记忆有限导致的规划失误。

###### 2. 客服 Agent 的对话管理系统设计 

- **NLU 模块（意图识别 + 槽位填充）** ：利用 BERT 等预训练语言模型进行意图识别，判断用户当前的意图，如咨询产品、投诉问题等；同时进行槽位填充，提取用户表达中的关键实体信息，例如产品名称、问题描述等细节内容。
- **对话状态跟踪（DST）** ：实时维护用户的目标、上下文信息以及对话所处的阶段等状态信息，确保对话具有连贯性和针对性，让客服 Agent 能准确理解用户需求并做出合理的回应。
- **策略模块** ：可以基于规则（采用有限状态机来定义不同对话场景下的固定应对策略），或者采用强化学习（如 Deep Q-Networks）方法，让客服 Agent 根据对话状态和历史信息，动态选择最优的回应策略，提升对话效果。

###### 3. 在 ROS 中实现 Agent 导航模块的核心组件 

- **SLAM（如 Gmapping、Cartographer）** ：用于构建环境地图，同时对 Agent 自身在地图中的位置进行定位，为后续的路径规划提供基础地图信息和自身位姿信息。
- **路径规划（A***/D* Lite 全局规划，TEB 局部避障）** ：全局规划算法如 A*、D* Lite 等负责从起点到终点生成一条可行的全局路径，而局部避障算法 TEB 则针对实时出现的障碍物等局部情况，调整局部路径，确保 Agent 能够安全、高效地沿着规划路径行进。
- **控制（MoveBase 集成 PID 或 MPC）** ：MoveBase 作为控制模块的核心，集成 PID 控制或 MPC 控制方法，根据路径规划结果实时控制 Agent 的运动，实现精准的轨迹跟踪和避障操作。

###### 4. AI Agent 在具身智能（Embodied AI）中的关键技术 

- **多模态感知** ：融合视觉（如通过摄像头获取图像信息）、触觉（感知物体硬度、纹理等）、听觉（接收声音信号）等多种感知模态，使 Agent 能更全面、准确地感知物理世界，就像人类通过多种感官认知环境一样。
- **物理交互建模** ：利用刚体动力学仿真工具（如 PyBullet）对物理交互过程进行建模，让 Agent 能提前预判与物体碰撞、抓取等操作的结果，优化交互策略，提升在物理世界中的操作能力。
- **仿真到真实（Sim2Real）** ：采用域随机化等技术，在仿真环境中对各种环境因素（如光照、物体摩擦系数等）进行随机扰动，提升 Agent 从仿真环境迁移到真实环境时的适应能力，确保其在真实场景中也能稳定、有效地运行。

##### RAG-Fusion：

通过生成多个用户查询并重新排序结果，利用逆向排名融合和自定义向量评分加权进行综合、准确的搜索。RAG-Fusion旨在弥合用户明确询问与意图询问之间的差距，更接近于发现通常隐藏的变革性知识。常用向量数据库（Elasticsearch、Pinecone）

![RAG-Fusion图解](/media/yls/1T硬盘3/picture/RAG-Fusion.png)

##### RRF（逆向排序融合）：

$$
\text{RRFscore}(d) = \sum_{i=1}^{n} \frac{1}{k + \text{rank}_i(d)}
$$

------

###### 公式解释

1. **逆向排名贡献**
2. **平滑常数 k**
   - 作用：避免排名为1时分母过小。
   - 经验值：k=60 能平衡排名差异的敏感度，实际可调整。
3. **缺失文档处理**
4. **得分融合**
   文档的最终得分是其在所有系统中贡献值的求和。得分越高，融合后排名越靠前。

------

###### 示例说明

假设两个排序系统（n=2）和 k=60：

- 系统A 结果：`[doc1, doc2, doc3]` → 排名：`doc1=1, doc2=2, doc3=3`
- 系统B 结果：`[doc2, doc4, doc1]` → 排名：`doc2=1, doc4=2, doc1=3`

计算 RRF 得分：

| 文档 | 系统A排名 | 系统B排名 | 计算过程                             | RRFscore |
| ---- | --------- | --------- | ------------------------------------ | -------- |
| doc1 | 1         | 3         | 1/(60+1) + 1/(60+3) = 1/61 + 1/63$   | ≈ 0.0325 |
| doc2 | 2         | 1         | 1/(60+2) + 1/(60+1) = 1/62 + 1/61$   | ≈ 0.0325 |
| doc3 | 3         | 未出现    | 1/(60+3) + 1/(60+61) = 1/63 + 1/121$ | ≈ 0.0241 |
| doc4 | 未出现    | 2         | 1/(60+61) + 1/(60+2) = 1/121 + 1/62$ | ≈ 0.0241 |

**融合后排序**：`doc1 ≈ doc2` > `doc3 ≈ doc4`（同分时可进一步按原始排名细化）。

------

###### 特点与优势

- 无需分数归一化：直接使用排名，避免不同系统分数尺度差异问题。
- 强调头部文档：排名靠前的文档贡献显著，符合信息检索的典型需求。
- 鲁棒性：对噪声和部分系统缺失不敏感。
- 应用场景：搜索引擎结果融合、多召回模型排序、交叉验证模型集成等。

##### 模型微调：

###### 1. CPT（Continued Pre-Training，继续预训练）

通过无标注数据进行无监督继续预训练，强化或新增模型特定能力。

数据要求：需要大量文本数据(通常几GB到几十GB)数据质量要高，最好是你目标领域的专业内容。

适用场景：

- 让模型学习特定领域的知识，比如医学、法律、金融

- 增强模型对某种语言或方言的理解

- 让模型熟悉你所在行业的专业术语

###### 2. SFT(Supervised Fine-Tuning)监督微调

通常需要几千到几万条高质量的问答对，答案要准确、风格统一。

适用场景：

- 训练客服机器人

- 创建特定任务的助手(比如代码助手、写作助手)

- 让模型学会特定的对话风格

###### 3. DPO(Direct Preference Optimization)偏好训练

引入负反馈，降低幻觉，使得模型输出更符合人类偏好

工作原理：给模型同一个问题的两个不同答案，告诉模型哪个答案更好，让模型学会倾向于生成更好的答案。

![DPO](/media/yls/1T硬盘4/picture/DPO.png)

适用场景：

- 让模型的回答更符合人类偏好

- 减少有害内容的生成

- 提高回答的质量和安全性

##### RAG ：

**经典 Naive RAG**：以文本处理为核心，遵循 “索引 - 检索 - 生成” 的标准流程；

**Modular RAG**：相较于 Naive RAG，知识整合与检索策略更为灵活。在知识库构建阶段，需要对数据进行复杂的 Chunk 编排；检索过程中，更高级的 Modular RAG 还支持对检索结果进行预处理和后处理；

**Agentic RAG**：依托智能体架构，不仅能够高效管理私域数据检索，还配备了一套专门的工具链，极大增强了知识检索能力。

![RAG](/media/yls/1T硬盘4/picture/RAG.png)

##### Prompt Engineering 和 Context Engineering：

###### 提示词工程：

提升单次交互质量的系统性方法。通过结构化、优化和迭代提示词提高AI在特定任务上的输出质量。

![](/media/yls/1T硬盘4/picture/prompt engineering.png)

###### 上下文工程：

通过管理多维度信息（如历史对话、外部数据、工具调用），为AI提供更全面的背景，是构建智能AI系统的核心。

![](/media/yls/1T硬盘4/picture/context engineering.png)



### 31日：

##### 协程函数 async def ：

- 什么是协程函数：协程一种比传统函数更高级的控制结构，由程序员手动控制其切换，线程在操作系统级别进行调度，可能导致频繁的上下文切换开销。协程则由 Python 解释器调度，开销更低且不会发生竞争资源的问题。协程可以在程序的多个点之间切换（在一个地方暂停并在程序的另一个点恢复），从而实现并发执行达到不需要多线程以及多进程的开销。
- 什么是 async def () ：用于定义一个协程函数。
- 什么是 async for () ：用于异步迭代可等待对象的异步迭代器。类似普通 for 循环，但是可以在异步环境中使用。
- 什么是 yield ：yield 用于定义生成器函数，生成器函数在每次 yield 语句处暂停。

##### 数字人构建：

目前开源可接入智能体的数字人不满足需求。

##### 语音助手工具创建：

设置读取命令并记录到本地的 MCP ，考虑怎么样可以把它集成到语音助手中。



## 八月

### 3日：

##### 本地部署模型推理缓慢分析：

笔者是在一台虚拟机中下载了支持在虚拟机上使用的显卡驱动镜像，使用的 ollama 进行本地部署没有使用源码+权重文件部署。验证了模型运作时的显卡使用率是正常的。部署配置为CPU Intel(R) Xeon(R) Gold 6254 CPU @  3.10GHz，运行内存可用118G，显卡A100本地部署 qwen3-20b，理论上配置是足够的，但是实际运行起来推理的速度很慢。以下是优化建议：

| 优先级 |              操作项               |             说明             |
| :----: | :-------------------------------: | :--------------------------: |
|   高   |        换用 **vLLM** 部署         | 显著提升吞吐，避免Ollama瓶颈 |
|   高   |      在**宿主机裸机**上测试       |    排除虚拟机I/O延迟问题     |
|   中   |   检查CPU是否满载，考虑CPU瓶颈    |    Xeon 6254单核性能较弱     |
|   中   | 尝试更高精度量化（如Q8\_0或FP16） |    减少量化带来的计算损失    |
|   低   |    关闭思考模式（`/no_think`）    |       减少冗余推理步骤       |

###### 1. 虚拟机部署的隐藏瓶颈：PCIe直通与I/O延迟

即使GPU显存占用正常，虚拟机环境下的PCIe直通（passthrough）可能存在带宽瓶颈或延迟过高的问题，这会严重影响推理速度，尤其是像Qwen3-20B这样的大模型对数据传输延迟非常敏感。

**建议：**

- 在宿主机上直接部署测试一次，排除虚拟化带来的性能损耗。
- 检查是否启用了IOMMU、NUMA绑定、CPU pinning等优化。

######  2. Ollama默认未开启高效推理后端（如vLLM）

Ollama虽然方便，但默认使用的是transformers推理路径，并未启用像vLLM、TensorRT-LLM这类专为高吞吐优化的引擎

。这会导致：

- KV缓存未优化；
- 多卡并行效率低；
- GPU利用率虽高，但实际吞吐极低。

**建议：**

- 改用 vLLM + Docker 部署 Qwen3-20B，支持1D tensor parallelism，可显著提升推理效率

- 示例命令（单卡）：

  ```bash
  python -m vllm.entrypoints.openai.api_server \
    --model qwen3-20b \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 4096
  ```

###### 3. CPU性能瓶颈：Xeon Gold 6254 单核性能较弱

虽然有118G内存，但Qwen3-20B在推理时仍需要大量CPU参与（如tokenization、调度、KV缓存管理）。Xeon Gold 6254 是服务器级CPU，单核性能不强，主频偏低（3.1GHz），在Ollama这类非GPU极致优化的框架下容易成为瓶颈。

**建议：**

- 使用 `htop` 或 `perf top` 查看推理时CPU是否满载。
- 若CPU瓶颈明显，可尝试使用 vLLM + CUDA graph优化，减少CPU参与。



##### 提示词技巧：

| Element                   | Description                                                  | Examples                                                   | Tips               |
| ------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------ |
| Instruction 指令词        | a specific task or instruction you want the model to perform想要模型执行的特定任务或指令。 | 简述，解释，翻译，总结，生成代码 ...                       | clear and specific |
| Context 背景              | external information or additional context that can steer the model to better responses包含外部信息或额外的上下文信息，引导语言模型更好地响应。 | 我是一个小学生；你是苏格拉底...                            | Act as 扮演        |
| Input Data 输入           | the input or question that we are interested to find a response for用户输入的内容或问题。 | 总结时提供的文本；编写SQL代码时提供的数据库/表结构信息 ... | use ### or """"    |
| Output Indicator 输出要求 | the type or format of the output.指定输出的类型或格式        | 50字；4句话；以 JSON格式输出                               |                    |

1. 先提问推理模型获得结果（这一步可以多重复几次，选取出现次数最多的结果作为大模型的回答）

2. 上述方法无效，加入样本提示并尝试 1 - 5 个样本

3. 样本方式无效，尝试零样本思维链

   ```
   添加提示词  “让我们逐步思考”
   ```

4. 零样本思维链无效，尝试加入思维链中间过程

   ```
   prompt="""
   Q：我去超市买了5块蛋糕，吃掉两块，然后又买了5块，还剩多少块蛋糕？
   A：8块
   Q：我去超市买了5块蛋糕，给了邻居1块蛋糕和修理工1块。然后我去买了8块，我还剩下多少块蛋糕？
   A：11块
   Q：我去超市买了5根香蕉，丢了3根，然后吃了1根，还剩下多少根？
   A：1根
   Q：我去市场卖了10根苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了一个。我还剩下多少个苹果？
   """
   ```

   

### 4日：

##### 智能客服搭建：

后端实现工具调用，考虑嵌入智能客服框架中。

##### 数字人搭建：

尝试解决连接工具时出现错误。



### 5日：

##### docker 环境安全删除：

删除 GitHub 源码只是删了“启动脚本”，真正占用空间的是 Docker 的镜像、容器和数据卷。要彻底清理指定环境，必须进入其项目目录执行 docker compose down，并手动删除相关卷和镜像，才能真正释放磁盘空间。

```bash
# 1. 进入项目目录
cd /path/to/

# 2. 停止并删除容器和网络
docker compose down

# 3. 删除数据卷（关键！）
docker volume ls		# 查看有哪些卷
docker volume inspect <卷名>		# 检查每个卷是否被任何容器引用
docker volume rm <VOLUME NAME>		# 删除指定卷

# 4. 删除镜像（可选）
docker image ls		# 检查有哪些镜像，按需删除
docker rmi <REPOSITORY/TAG:ACTIVE>		# 例如docker rmi langgenius/dify-sandbox:0.2.12

# 5. 清理系统资源
docker system prune --all --force
```



##### Ten-Agent 部署数字人嵌入失败：

测试网络原因对 Ten-Agent 部署影响，发现在具备科学上网能力的电脑上部署可以正确时识别 AGORA_APP_ID 和AGORA_APP_CERTIFICATE 但是在只能访问国内网站的服务器上不能导致不能部署。如下图所示：

![image-20250805111134524](/home/yls/.config/Typora/typora-user-images/image-20250805111134524.png)



### 6日：

##### Ten-Agent Room connected 连接失败问题：

排查了核心容器是否正常工作，确认 Playground 是否正确连接 Agent 服务，测试了 Agent 在端口 8080 运行。现排查清除是网络连接问题。

- 申请 Deepgram 密钥：目前申请个人账户免费使用，商用版需要付费

  ```
  a2765fe4df4bf221f87623c908198045ec85ab9c
  ```

##### Ten-Agent 网络连接失败问题：

[官方 Github](https://github.com/TEN-framework/ten-framework/issues/372) 给出的修改意见是重新配置环境 



### 7日：

##### 解决Ten-Agent 网络连接失败问题：

修改配置文件爱你，更换拓展文件下的节点属性重新启动容器并构建 Ten-Agent，重新启动前端界面。修改 agora   

鉴权并重新启动连接成功

##### 语音助手回答问题：

语音助手存在回答问题时连同思考过程一并回答 



### 10日：

##### 小车语音助手测试：

- 设置 Insecure origins treated as secure：板子上麦克风识别不到，外接麦克风然后按如下设置重启浏览器可以正常使用语音输入功能。

使用浏览器打开下面链接，chrome://flags/#unsafely-treat-insecure-origin-as-secure

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/efb5dfacc66d4789b60bc0bc90b50f57.png)

- 语音输出功能测试：

  现在已经使用的模型

![image-20250810173737404](/home/yls/.config/Typora/typora-user-images/image-20250810173737404.png)



### 11日：

##### 服务器模型部署：

首先需要区分官方的更新驱动是对于本机的应用更新而非 docker 的镜像，其次更新 Ollama 会保留上一个版本的配置所以可以放心更新。

- Ollama 镜像更新：拉取最新的 Docker 镜像，这会把官方最新的镜像下载下来，确保容器里跑的也是新版 Ollama。

  ```bash
  sudo docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 ollama/ollamadocker pull ollama/ollama:latest
  
  # 关闭正在运行的 Ollama 镜像
  docker ps -a  # 找到旧容器ID
  docker stop <container_id>
  docker rm <container_id>
  
  # 重新启动镜像
  sudo docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
  
  # 验证容器版本
  docker exec -it <container_id> ollama --version
  ```

  

- 运行容器报错：failed to bind host port for 0.0.0.0:11434... address already in use，明确报错是由于端口已经被占用导致使用如下指令查看是哪个进程占用。

  ```bash
  sudo lsof -i :11434
  
  # 杀死进程
  sudo kill -9 PID
  ```

  1. 如果是由于非 Ollama 占用则杀死进程或更换 镜像端口号解决。

  2. 如果是因为 Ollama 占用分两种情况：

     - 均为容器打开的 Ollama 则只需要关闭之前打开的容器就好；

     - 若一个是本机打开的应用，一个是借由 Docker 打开的镜像则需要决定使用哪一种方式，笔者使用 Docker 打开。则需要进行如下设置：

       ```bash
       sudo systemctl stop ollama
       sudo systemctl disable ollama  # 可选：防止开机自启
       
       # 重新运行 Docker 指令
       sudo docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
       ```

       

##### 智能助手测试：

- 输出规范化：现有智能体对规范输出不敏感，进行 TTS 转化时将思考过程一并转化，思考如何规避这一步骤。根据 Github 官方 [Issues](https://github.com/langgenius/dify/issues/14191) 确认是由于模型供应商一般将模型的思考过程以及生成结果一并打包输出导致 TTS 转化时会读出思考过程。官方也没有进行优化。

- 测试接口调用形式：API 的未来调用应包含此 `conversation_id`，以确保与 Dify 机器人的对话连续性。但是会报错 The requested URL was not found on the server.

  ```bash
  curl -X POST 'http://192.168.1.28/v1/chat-messages' \
  --header 'Authorization: Bearer app-kPdQWVvNqI6b7zoJ2OVGUQFB' \
  --header 'Content-Type: application/json' \
  --data-raw '{
      "inputs": {},
      "query": "What are the specs of the iPhone 13 Pro Max?",
      "response_mode": "streaming",
      "conversation_id": "",
      "user": "abc-123",
      "files": [
        {
          "type": "image",
          "transfer_method": "remote_url",
          "url": "https://cloud.dify.ai/logo/logo-site.png"
        }
      ]
  }'
  ```
  
  

### 12日：

##### 智能客服 API 测试：

以下采用 python 脚本的形式测试调用 dify 接口。但是报错 {"code": "unauthorized", "message": "Access token is invalid", "status": 401}

```python
import requests
import json

url = "https://api.dify.ai/v1/completion-messages"

headers = {
    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',
    'Content-Type': 'application/json',
}

data = {
    "inputs": {"text": 'Hello, how are you?'},
    "response_mode": "streaming",
    "user": "abc-123"
}

response = requests.post(url, headers=headers, data=json.dumps(data))

print(response.text)
```



##### 开发板测试：

通过修改浏览器默认配置，将服务器地址修改为可信任地址使麦克风的权限打开并可以正常使用，但是扬声器不论是开发板内置还是外接设备都不能正常使用。



### 13日：

##### 语音助手优化：

- 使用自定义回答来规避回答包含思考过程，注意在系统提示词中添加请直接回答问题，不要展示推理过程 **/no_think** 

  ```text
  用户提问：查询合肥天气
  
  标准回答：合肥今天天气多云，气温33°C，湿度63%。建议注意防晒和补充水分哦！
  
  用户提问：可以和我聊一聊庐剧吗
  
  标准回答：庐剧是安徽省的传统戏曲剧种，起源于清代，距今已有200多年历史，被誉为“黄梅戏的前身”。它融合了民间小调、地方语言和民俗文化，具有独特的艺术魅力。
  ```

- 修改大模型关闭思考模式：使用硅基流动的调用 Qwen3-14B 大模型可以关闭思考模式回答十分简短解决了语音播报有思考过程的问题，但是可能回答的不够智能。ollama 本地部署同款模型没有看到这个选项。
- 开发板测试：目前问题输入后回答问题很快且没有输出思考过程。



##### 语音助手接口测试：

返回内容不正确，以下是部分返回值

![image-20250813173802046](/home/yls/.config/Typora/typora-user-images/image-20250813173802046.png)



### 14日：

##### 语音助手指令测试：

通过将代码移植到主控电脑的并运行智能助手识别程序做到基于关键词的指令识别，目前有机器人端以及服务器端两种指令。机器人端有方向控制以及前进、后退两种，通过以前的功能包已实现。服务器端做到部署了语音助手可以通过特定的唤醒词进行使用。



##### 智能体接口测试：

使用智能体地址 + API key 的形式安全访问服务器的目的。

- 显示拒绝连接：curl: (7) Failed to connect to 192.168.1.28 port 443 after 11 ms: 连接被拒绝
  - 解决办法：确认 Dify 服务是不是用 HTTPS（443），还是 HTTP（80），如果浏览器里用的是 `http://` 而不是 `https://`，那应该用 80 端口。

- 接口访问正常可以检查到正确访问的日志，但是目前都是流式访问获取到的结果不够直观。官方文档中注明 Agent 模式下不能进行阻塞模式访问。

  ![image-20250814161859866](/home/yls/.config/Typora/typora-user-images/image-20250814161859866.png)



- python 智能助手调用脚本：采用流式输出，然后使用 python 官方的 json 包对输出结果进行解析输出。

  ```python
  import requests
  import json
  
  url = "http://192.168.1.28/v1/chat-messages"
  headers = {
      "Authorization": "Bearer app-kPdQWVvNqI6b7zoJ2OVGUQFB",  # 替换为你的API Key
      "Content-Type": "application/json"
  }
  payload = {
      "inputs": {},
      "query": "今天杭州的天气如何？",
      "response_mode": "streaming",
      "conversation_id": "",
      "user": "decade",
      "files": [
          {
              "type": "image",
              "transfer_method": "remote_url",
              "url": "https://cloud.dify.ai/logo/logo-site.png"
          }
      ]
  }
  
  # 发起流式请求
  response = requests.post(url, headers=headers, json=payload, stream=True)
  
  answer_parts = []
  for line in response.iter_lines():
      if line:
          text = line.decode('utf-8')
          if text.startswith("data: "):
              try:
                  event = json.loads(text[6:])
                  # 提取流式返回的自然语言部分
                  if event.get("event") in ["agent_message", "message"]:
                      ans = event.get("answer", "")
                      if ans:
                          answer_parts.append(ans)
              except Exception:
                  continue
  
  # 拼接完整答案
  final_answer = ''.join(answer_parts)
  print("Agent answer:", final_answer)
  
  ```

- 现在用户输入问题需要手动：考虑做一个一直运行的程序调用语音识别以及语音输出接口来辅助 Dify 创建的智能体做到一个智能语音助手。

  

##### 语音控制打电话：

- 基于 Linphone 支持拨打电话的功能，想法是将这个功能通过 MCP 的方式接入 Dify 创建的智能体，来实现智能体语音控制打电话的功能。



### 17日：

##### 整合语音助手：

通过关键词唤醒的方式打开语音助手，解决无用语音的转录，节省 Token 支出。关键词唤醒后打开带有自动语音检测和文本转语音功能的脚本，通过访问服务器获取回答并以语音形式返回给用户。

##### 语音功能接口测试：

- 函数 create_async_playwright_browser 和函数 create_sync_playwright_browser 区别：
  1. create_async_playwright_browser 函数
     - **异步初始化：**这个函数是异步的，返回一个 async 浏览器实例。它适用于异步环境中，例如在 asyncio 事件循环中运行的代码。
     - **使用方式：**需要在 async 函数中使用 await 调用。
     - **适用场景：**当你需要在异步环境中操作 Playwright 时，例如在异步的 Web 应用程序或异步脚本中。
  2. create_sync_playwright_browser  函数
     - **同步初始化**：这个函数是同步的，返回一个同步的浏览器实例。它适用于同步环境中，例如在普通的 Python 脚本或同步函数中。
     - **使用方式**：直接调用，不需要 `await`。
     - **适用场景**：当你需要在同步环境中操作 Playwright 时，例如在普通的 Python 脚本或同步的 Web 应用程序中。

- 创建 .ipynb 文件时报错：

  ```bash
  It looks like you are using Playwright Sync API inside the asyncio loop.Please use the Async API instead
  # 考虑是因为异步环境（如 Jupyter Notebook 或其他支持 asyncio 的环境）中使用了 同步 Playwright API。Jupyter Notebook 默认运行在一个异步事件循环中，而 create_sync_playwright_browser() 是同步的，不兼容异步环境。需要改用异步版本的 Playwright 浏览器初始化函数 create_async_playwright_browser()，并确保整个代码运行在异步环境中。
  ```

- 使用 python  脚本进行服务器请成功：

  ```python
  from pathlib import Path
  from openai import OpenAI
  
  speech_file_path = Path(__file__).parent / "siliconcloud-generated-speech.mp3"
  
  client = OpenAI(
      api_key="api-key", # 从 https://cloud.siliconflow.cn/account/ak 获取
      base_url="https://api.siliconflow.cn/v1"
  )
  
  with client.audio.speech.with_streaming_response.create(
    model="FunAudioLLM/CosyVoice2-0.5B", # 支持 fishaudio / GPT-SoVITS / CosyVoice2-0.5B 系列模型
    voice="FunAudioLLM/CosyVoice2-0.5B:claire", # 系统预置音色
    # 用户输入信息
    input="合肥是安徽省的省会，位于中国东部，是一个历史悠久且发展迅速的城市。",
    response_format="mp3" # 支持 mp3, wav, pcm, opus 格式	
  ) as response:
      response.stream_to_file(speech_file_path)
  ```



### 18日：

##### 语音回复功能优化：

现在集成回复自动转语音并播放，播放后自动删除音频功能，完成 TTS 模块开发。

##### 语音识别功能开发：

借助阿里云百炼平台的接口实现自动语音识别并转化为文字，需要考虑什么是后启动程序，又在什么中断识别让大模型回答问题并返回结果。

- 启动程序：将程序做为关键词识别程序的后处理操作，即关键词触发后（基于 sherpa-onnx 的关键词识别技术）执行语音识别和后续操作。
- 识别中断：考虑设置单词识别时长，避免用户短时间内提出过多问题，或者长时间说话导致一直在执行语音识别从而不断发送给模型识别的情况。
- 开启提示：考虑设置语音助手开启的提示音。这一点可以参考阿里云百炼开启识别时输出的日志。
- VAD（静音检测）：VAD 的核心任务是**鉴别音频信号中的语音出现（speech presence）和语音消失（speech absence）**，也就是区分语音和非语音（或静音）部分。想象一下，在一个有背景噪音的环境中，VAD 就像一个智能“守门人”，它能准确识别什么时候有人在说话，什么时候是纯粹的环境噪音或沉默。



### 19日：

LangSmith API KEY ：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

LangSmith project name:  pr-cold-structure-88

阿里云 ASR 模型计费标准：[官网](https://help.aliyun.com/zh/isi/product-overview/billing-10)

![image-20250819163154752](/home/yls/.config/Typora/typora-user-images/image-20250819163154752.png) 



##### 静音检测开发：

使用两种方法尝试实现静音检测，分别是 webrtcvad 开源包检测以及基于音频能量的简易VAD。两种方法都报错没有做到语音识别，设备检测和采样率均有问题



##### 整合语音助手：

使用设置定是录制转换的检测算法作为服务器端模型输入，实现ASR自动检测。



##### 语音助手实现策略：

- ASR 检测定时发送给大模型输出答案。
- 手动录制音频到本地然后检测离线音频作为模型的输入。
- VAD 静音检测实现对于什么时候检测是么时候上传问题给大模型。



### 20日：

##### VAD 相关内容阅读：

阅读文章尝试寻找新方法实现语音识别的静音检测。



##### 语音助手整合：

- 报错：识别语音后按照设计每五秒检测并发送一次这条功能是正确的，但是请求发送给智能体这一步显示请求失败测试了将参数 timeout 依旧报错。使用 curl 请求排查发现是服务器大模型没有正确输出导致的，需要使用 docker 重新器启动容器，解决了请求超时的问题。

  ```bash
  请您通过麦克风讲话，每5秒的语音内容会作为输入发送到指定API
  [DEBUG] 新增识别内容：今天
  [DEBUG] 新增识别内容：今天，合肥天气。
  [DEBUG] 新增识别内容：今天，合肥天气。
  [DEBUG] 新增识别内容：今天，合肥天气。
  [DEBUG] 拼接后数据：[{'sentence_id': 0, 'text': '今天', 'timestamp': 1755670951.2646205}, {'sentence_id': 0, 'text': '今天，合肥天气。', 'timestamp': 1755670952.0628905}, {'sentence_id': 0, 'text': '今天，合肥天气。', 'timestamp': 1755670952.6690311}, {'sentence_id': 0, 'text': '今天，合肥天气。', 'timestamp': 1755670952.8709922}]
  [DEBUG] 拼接后的 query : 今天 今天，合肥天气。 今天，合肥天气。 今天，合肥天气。
  [ERROR] 请求失败：HTTPConnectionPool(host='192.168.1.28', port=80): Read timed out. (read timeout=10)
  [DEBUG] 新增识别内容：合肥天
  [DEBUG] 拼接后数据：[{'sentence_id': 1, 'text': '合肥天', 'timestamp': 1755670994.0657747}]
  [DEBUG] 拼接后的 query : 合肥天
  [ERROR] 请求失败：HTTPConnectionPool(host='192.168.1.28', port=80): Read timed out. (read timeout=10)
  [DEBUG] 新增识别内容：合肥天气。
  [DEBUG] 新增识别内容：合肥天气。
  [DEBUG] 新增识别内容：合肥天气。
  [DEBUG] 拼接后数据：[{'sentence_id': 1, 'text': '合肥天气。', 'timestamp': 1755671004.4271932}, {'sentence_id': 1, 'text': '合肥天气。', 'timestamp': 1755671004.4272685}, {'sentence_id': 1, 'text': '合肥天气。', 'timestamp': 1755671004.4273171}]
  [DEBUG] 拼接后的 query : 合肥天气。 合肥天气。 合肥天气。
  [ERROR] 请求失败：HTTPConnectionPool(host='192.168.1.28', port=80): Read timed out. (read timeout=10)
  ```

- 流式输出结构解析：识别的问题发送给模型回答返回的内容是流式输出，需要使用 json 包对输出的答案进行提取。

  ```python
  answer_parts = []  # 存储流式返回的 answer 片段
                  for line in response.iter_lines():
                      if line:
                          text = line.decode('utf-8')
                          if text.startswith("data: "):
                              try:
                                  event = json.loads(text[6:])  # 去除 "data: " 前缀
                                  if event.get("event") in ["agent_message", "message"]:
                                      ans = event.get("answer", "")
                                      if ans:
                                          answer_parts.append(ans)
                                          print(f"[INFO] 流式回答片段: {ans}")
                                  elif event.get("event") == "message_end":
                                      print(f"[INFO] 完整回答: {''.join(answer_parts)}")
                              except json.JSONDecodeError as e:
                                  print(f"[ERROR] JSON 解析失败: {e}")
                                  continue
              except Exception as e:
                  print(f"[ERROR] 请求失败：{e}")
  ```

- 修改回复提取逻辑以及问题生成逻辑：现有的回答均建立在持续5s的语音识别拼接基础上，涉及重复文字的拼接问题，需要改良；生成的答案由于没有完全去除思考过程需要对流式输出的内容做限定。具体如下图：

  ![image-20250820163005836](/home/yls/.config/Typora/typora-user-images/image-20250820163005836.png)

  ![image-20250820163120478](/home/yls/.config/Typora/typora-user-images/image-20250820163120478.png)



### 21日：

##### VAD 开发以及模型部署：

可以尝试使用 Webrtc-VAD、Silero-VAD、FSMN-VAD 三个开源包做静音检测，模型部署方面可以考虑本地部署 TTS、STT 模型然后借助 dify 工作流的方式使用接口调用。

##### 修复语音助手问题：

整体流程：

```
[关键词唤醒] → [启动语音识别] → [5秒后自动停止] → [发送请求] → [生成回答] → [语音播报] → [停止识别] → [退出程序]
```



- 拼接重复文字：在实时语音识别场景中，ASR（自动语音识别）系统通常会不断更新识别结果（即“中间结果”），导致短时间内出现重复或递增的文本片段（如“今天” → “今天合” → “今天合肥” → “今天合肥天气”）。而需求是**只将完整的最终识别结果发送给服务器**，避免重复和冗余。

  - 解决办法：阿里云 `dashscope` 的 `TranslationRecognizerCallback` 会为每个**完整句子**分配一个唯一的 `sentence_id`。**只有当一个句子被确认为完整时，才会生成新的 `sentence_id`**。我们可以通过判断 `sentence_id` 是否变化，来识别“完整句子”。

    ```python
            current_time = time.time()
            
            # 只保留最近5秒内的识别结果
            filtered = [item for item in self.transcriptions
                        if current_time - item["timestamp"] <= 5]
            
            if not filtered:
                return 
    
            # 核心修改：按 sentence_id 分组，取每个句子的最新文本
            sentence_dict = {}
            for item in filtered:
                sid = item["sentence_id"]
                # 只保留该 sentence_id 的最新文本（最长或最完整）
                if sid not in sentence_dict or len(item["text"]) > len(sentence_dict[sid]["text"]):
                    sentence_dict[sid] = item
    
            # 取出所有完整句子的文本（sentence_id 存在即为完整句）
            # 可以选择只发送最后一个完整句子（最符合“最终结果”）
            complete_sentences = [item["text"].strip() for item in sentence_dict.values() if item["text"].strip()]
    
            if not complete_sentences:
                return
    
            # 方案1：只发送最后一个完整句子（推荐）
            query = complete_sentences[-1]  # 只取最后一个完整句子
    ```

- 错误识别问题：模型生成回答通过接给播放音频时语音识别功能也会同步进行，这个时候会造成语音错误识别，考虑使用一次关键词唤醒只回答一个问题的方法避免这个情况的发生。

  

### 24日：

##### 回复内容优化：

- 语音回复：经过语音合成的回复会被语音识别接口错误识别然后传给模型作为新问题，考虑使用设置语音输出时间限制的方式来规避这种错误。实测虽然在设置的时间内造成问题组装的错误但是最终问题的回复上不会有问题。

  

- 回复内容：现有部署的模型 Qwen3: 14B 使用 prompt 去除思考过程不完全，会返回 <think> </think> 两个无用字段且使用 TTS 转语音识别会对这两个字段进行播报，考虑在进行流式输出解析时过滤掉这两个字段。

  - 使用通义接口使用 Qwen2: 14B 时有关闭 Thinking 模式的选项但是使用 Ollama 本地部署没有。

  ```python
  def _remove_think_tags(self, text):
    """去除文本中的<think>标签及其内容"""
    # 使用正则表达式匹配和移除<think>...</think>标签及其内容
    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
  ```

  

##### 模型剪枝：

旨在通过减少神经网络中不必要的参数和连接，来优化模型的效率和性能。剪枝可以很好地衡量模型轻量化程度与精度的关系，是替换轻量化结构完全没办法比的，大部分轻量化模块都是由时间换空间，而且精度还会下降得比较多，但是剪枝可以很好地避免这个问题。



##### 更新关键词文档：

设计语音助手唤醒词以及开场白，使用 sherpa-onnx 库添加识别打电话给儿子、女儿的关键字。

```bash
sherpa-onnx-cli text2token --tokens keyword_tokens.txt --tokens-type ppinyin keywords_raw.txt keywords.txt
```



### 25日：

##### 本地部署 Kitten TTS ：

部署完测试发现只支持英语的 Text2Speech。



##### ChatTTS 本地部署：

- 使用 requirements.txt 安装包：

  ```bash
  pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

  

### 26日：

##### 本地部署 ChatTTS：

部署环境 ubuntu22.04 ，使用 git 克隆官方代码，创建虚拟环境，安装必要的库。下载模型文件编写测试脚本。

```bash
# 克隆原始项目
git clone https://github.com/2noise/ChatTTS 	# 这一步如果有外网的网络条件可以选择 lfs 下载，注意使用时需要提前安装

# 安装指令
sudo apt-get install git-lfs
git lfs install

# 创建虚拟环境
conda ceate -n chattts python=3.10 -y
conda activate chattts

# 按照 requirements.txt 下载需要的库，国内可选择清华源或者阿里源
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 下载模型文件，注意这里不是下载项目源码，地址一定选择 huggingface 不是 github
git clone https://huggingface.co/2Noise/ChatTTS

# 测试指令
python tts_test.py --text "ChatTTS 是一个开源的文本转语音大模型，支持中文和英文。" --seed 1234 --speed 1.1 --out ./demo.wav
```

```python
# 使用最简单的测试脚本
import torch
import ChatTTS
from IPython.display import Audio

# 初始化ChatTTS
chat = ChatTTS.Chat()
chat.download_models()

# 定义要转换为语音的文本
texts = ["你好，欢迎使用ChatTTS！"]

# 生成语音
wavs = chat.infer(texts, use_decoder=True)

# 播放生成的音频
Audio(wavs[0], rate=24_000, autoplay=True)
```



##### RAG 后端实现：

`Streamlit`前端界面，结合`LangChain`框架`retriever`工具与`DashScope`向量模型服务、`DeepSeek`大模型服务，从0到1实现了轻量化的RAG知识库系统，支持上传多个PDF文档，系统将自动完成文本提取、分块、向量化，并构建基于 FAISS 的检索数据库。用户随后可以在页面中输入任意问题，系统会调用大语言模型（如 DeepSeek-Chat）对 PDF 内容进行语义理解和回答生成。



### 27日：

##### 实时因子（RTF）：

通常用实时率(Real Time Factor, RTF)来衡量识别的速度，实时率等于识别花的时间除以语音本身的时间。实时率为1就表示用户一说完话结果就能出来(前提是忽略假设录音实时的传给语音识别系统，时间情况很多时候是在服务器端进行解码的，因此会有网络的延迟)。实时率大于1就表示话说完了，系统还得再处理一段时间。而实时率小于1表示识别速度比说话速度快，这样万一有网络延迟，它还能追上来。

解释 RTF 值：

- RTF < 1：实时或更快的生成。生成语音所需时间少于语音时长。比如 RTF = 0.5 意味着生成一分钟语音只需要 30 秒。
- RTF > 1：非实时。生成语音所需时间长于语音时长。比如 RTF = 2 意味着生成一分钟语音需要 2 分钟。

RTF 越低，意味着 TTS 系统生成语音的速度越快。实时因子低的 TTS 系统在需要实时响应的应用场景（如语音助手）中尤为关键。



##### ChatTTS 本地部署：

尝试使用不同版本的源码以及手动下载和脚本下载模型文件的方式进行模型部署，最后要么卡在下载失败（切换下载源解决），要么卡在函数缺失（检查发现是官方在类中的函数命名已经修改但是官方文档中中文说明文档没有说明而且官方示例也没有使用新的函数名），要么卡在一些看不懂的错误上。



### 28日：

##### ChatTTS 本地部署文档书写：

##### 创建语音转文字服务：

- 创建访问密钥：

  ```bash
  export API_KEYS="iCare20250828,iCare20250831"
  ```

  

### 31日：

##### 库名与文件名重复问题：

文件的名字跟真正的 `langchain` 三方库同名，导致 `import` 时 Python 先找到并加载了 `langchain.py`，于是找不到真正的 `langchain.agents` 模块了。这是一个常见的python陷阱，其中本地文件名与第三方软件包的名称相撞。

```
File "/media/yls/1T硬盘4/code/Agent/LangChain-MCP/langchain.py", line 3, in <module>
    from langchain.agents import create_tool_calling_agent, AgentExecutor
ModuleNotFoundError: No module named 'langchain.agents'; 'langchain' is not a package
```

- 解决办法：直接修改文件名保证与第三方库名不相同，删除缓存文件重新打开 vscode 运行即可。



##### API 服务搭建：

- 测试适用本地服务：在 postman 发出的请求问题一直出在缺少各种不同的字段。

  ![image-20250831143813385](/home/yls/.config/Typora/typora-user-images/image-20250831143813385.png)

  

  main.py 脚本如下：

  ```python
  from fastapi import FastAPI, HTTPException
  from fastapi.responses import FileResponse
  from pydantic import BaseModel
  import numpy as np
  from wave import Wave_write
  import os
  from types import SimpleNamespace
  
  import ChatTTS
  chat = ChatTTS.Chat()
  chat.load(compile=False)
  
  app = FastAPI()
  
  class AudioRequest(BaseModel):
      text: str
      params_refine_text: dict
  
  @app.post("/generate-audio/")
  def generate_audio(request: AudioRequest):
      try:
          # 修正：将 dict 转为对象
          params_refine_text_obj = SimpleNamespace(**request.params_refine_text)
          wavs = chat.infer(request.text, params_refine_text=params_refine_text_obj)[0]
          sample_rate = 24000
          audio_data_rescaled = (wavs * 28000).astype(np.int16).flatten()
  
          os.makedirs('audio_files', exist_ok=True)
          file_path = 'audio_files/test.wav'
  
          with Wave_write(file_path) as wave_file:
              wave_file.setparams((1, 2, sample_rate, len(audio_data_rescaled), 'NONE', 'not compressed'))
              wave_file.writeframes(audio_data_rescaled.tobytes())
  
          return FileResponse(path=file_path, filename="test.wav", media_type='audio/wav')
      except Exception as e:
          raise HTTPException(status_code=500, detail=str(e))
  
  if __name__ == "__main__":
      import uvicorn
      uvicorn.run(app, host="0.0.0.0", port=8000)
  ```

  

- 官方示例接口：在项目源码中 examples/api 文件夹下有示例接口声明文件，文件目录结构如下：

  ```
  ├── client.py
  ├── main.py
  ├── openai_api.py
  ├── postScript.py
  ├── README.md
  └── requirements.txt
  ```

  

## 九月

### 1日：

##### 模型加载运行：

虚拟机内网络环境差需要自行下载模型文件并放入指定文件夹内 ~/.cache/huggingface/hub/models--2Noise--ChatTTS 中再重新启动接口服务。具体模型放置目录结构如下：

```bash
├── blobs
│   ├── 12d59e7d0af9ccfd5deb4ec01b4db3855f3d7314
│   ├── 5ea569e3431b0ed2aa1c699461017c7174d2f56d
│   ├── 8df13367906f6cd6b1f88b3cc6f1f15599b19e94
│   ├── 9c7b3d09af3f9fea19072d4a35aecee15779f51c
│   ├── b62fb7fbd3c9b91498b869b32343642d03a25fc0
│   └── be32c1231832c60ddad7e0c2e8bd027f51a183b2
├── refs
│   └── main
└── snapshots			# 用于存放模型文件
    └── 1a3c04a8b0651689bd9242fbb55b1f4b5a9aef84
        ├── asset
        │   ├── Decoder.pt
        │   ├── Decoder.safetensors
        │   ├── DVAE_full.pt
        │   ├── DVAE.pt
        │   ├── DVAE.safetensors
        │   ├── Embed.safetensors
        │   ├── gpt
        │   │   ├── config.json
        │   │   └── model.safetensors
        │   ├── GPT.pt
        │   ├── spk_stat.pt
        │   ├── tokenizer
        │   │   ├── special_tokens_map.json
        │   │   ├── tokenizer_config.json -> ../../../../blobs/b62fb7fbd3c9b91498b869b32343642d03a25fc0
        │   │   └── tokenizer.json
        │   ├── tokenizer.pt
        │   ├── Vocos.pt
        │   └── Vocos.safetensors
        └── config
            ├── decoder.yaml -> ../../../blobs/9c7b3d09af3f9fea19072d4a35aecee15779f51c
            ├── dvae.yaml -> ../../../blobs/8df13367906f6cd6b1f88b3cc6f1f15599b19e94
            ├── gpt.yaml -> ../../../blobs/be32c1231832c60ddad7e0c2e8bd027f51a183b2
            ├── path.yaml -> ../../../blobs/5ea569e3431b0ed2aa1c699461017c7174d2f56d
            └── vocos.yaml -> ../../../blobs/12d59e7d0af9ccfd5deb4ec01b4db3855f3d7314
```



在项目源码找到 FastAPI 启动程序 main.py 启动接口服务，具体步骤如下：

```bash
# 首先检查模型文件是否正确
ls ~/.cache/huggingface/hub/models--2Noise--ChatTTS/snapshots/1a3c04a8b0651689bd9242fbb55b1f4b5a9aef84/asset/Vocos.safetensors

# 如果输出文件存在运行
fastapi dev main.py
```



##### 模型接口测试：

使用官方给出的测试脚本发现一段 9s 的生成时间差不多需要1分钟，测试脚本如下：

```python
import datetime
import os
import zipfile
from io import BytesIO

import requests

chattts_service_host = os.environ.get("CHATTTS_SERVICE_HOST", "localhost")
chattts_service_port = os.environ.get("CHATTTS_SERVICE_PORT", "8000")

CHATTTS_URL = f"http://{chattts_service_host}:{chattts_service_port}/generate_voice"


# main infer params
body = {
    "text": [
        "四川美食确实以辣闻名，但也有不辣的选择。",
        "比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。",
    ],
    "stream": False,
    "lang": None,
    "skip_refine_text": True,
    "refine_text_only": False,
    "use_decoder": True,
    "audio_seed": 12345678,
    "text_seed": 87654321,
    "do_text_normalization": True,
    "do_homophone_replacement": False,
}

# refine text params
params_refine_text = {
    "prompt": "",
    "top_P": 0.7,
    "top_K": 20,
    "temperature": 0.7,
    "repetition_penalty": 1,
    "max_new_token": 384,
    "min_new_token": 0,
    "show_tqdm": True,
    "ensure_non_empty": True,
    "stream_batch": 24,
}
body["params_refine_text"] = params_refine_text

# infer code params
params_infer_code = {
    "prompt": "[speed_5]",
    "top_P": 0.1,
    "top_K": 20,
    "temperature": 0.3,
    "repetition_penalty": 1.05,
    "max_new_token": 2048,
    "min_new_token": 0,
    "show_tqdm": True,
    "ensure_non_empty": True,
    "stream_batch": True,
    "spk_emb": None,
}
body["params_infer_code"] = params_infer_code


try:
    response = requests.post(CHATTTS_URL, json=body)
    response.raise_for_status()
    with zipfile.ZipFile(BytesIO(response.content), "r") as zip_ref:
        # save files for each request in a different folder
        dt = datetime.datetime.now()
        ts = int(dt.timestamp())
        tgt = f"./output/{ts}/"
        os.makedirs(tgt, 0o755)
        zip_ref.extractall(tgt)
        print("Extracted files into", tgt)

except requests.exceptions.RequestException as e:
    print(f"Request Error: {e}")

```



### 2日：

##### 紧急呼救关键词：

在开发板上测试了常见的一些呼救词，目前实现识别并发送呼叫信息。现有指令内容如下：

```
# 语音助手唤醒词
小明你好

# 小车控制指令
向前
向后
向左
向右
前进
后退
跟我走

# 语音控制打电话
打给儿子
打给女儿

# 紧急呼救类
救命
着火
有人需要帮助
快报警
我不行了快叫救护车
```



##### 优化 ChatTTS 本地部署文档：

将 FastAPI 主程序启动方式以及测试脚本测试结果写入文档。	



##### 关键词唤醒于语音助手整合测试：

- 音频设备检测报错：外界的音频设备没有检测到，导致程序运行是没有问题输入。通过强制指定设备解决输入问题，但是生成的音频不能播放。具体报错信息以及暂时解决方案如下：

  ```bash
  TranslationRecognizerCallback open.
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.front
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround40
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround41
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround50
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround51
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround71
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958
  ALSA lib confmisc.c:1369:(snd_func_refer) Unable to find definition 'cards.0.pcm.hdmi.0:CARD=0,AES0=4,AES1=130,AES2=0,AES3=2'
  ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory
  ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM hdmi
  ALSA lib confmisc.c:1369:(snd_func_refer) Unable to find definition 'cards.0.pcm.hdmi.0:CARD=0,AES0=4,AES1=130,AES2=0,AES3=2'
  ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory
  ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM hdmi
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline
  ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline
  ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp
  ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp
  ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card
  ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'
  ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card
  ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'
  
  
  # 查看设备状态
  arecord -l
  
  # 示例输出
  card 3: rockchipi2s [rockchip-i2s], device 0: HiFi audio-i2s-0 []  # 关注 card 后面的数字修改参数
    Subdevices: 1/1
    Subdevice #0: subdevice #0
  ```

  ```python
  # 参数修改
  def on_open(self) -> None:
      global mic, stream
      print("TranslationRecognizerCallback open.")
      mic = pyaudio.PyAudio()
      # 根据 arecord -l 的输出调整设备索引
      stream = mic.open(
          format=pyaudio.paInt16,
          channels=1,
          rate=16000,
          input=True,
          input_device_index=3  # 替换为实际设备索引
      )
  ```




### 3日：

##### 关键词唤醒于语音助手整合测试：

完成语音助手开发板测试，中间遇到音频生成但是播放不成功。可能使用是因为声卡驱动问题也可能是设备未被检测到或是声音太小导致播放音频没有声音。下面列出常见的错误排查方法：

1. 先确认最简单的：系统里到底有没有声卡，以及当前用户有没有权限

   ```bash
   # 终端执行
   aplay -l
   ```

   - 如果提示 “no soundcards found…” → 说明系统根本没识别出声卡，先解决驱动/硬件问题，后面的步骤都不用看了。

   - 正常会列出类似：

     ![image-20250903094014405](/home/yls/.config/Typora/typora-user-images/image-20250903094014405.png)

     

     1.2 再确认当前用户是否在 `audio` 组，执行完成后重新登录或重启。

     ```
     groups $USER
     
     # 如果没有 audio，执行
     sudo usermod -a -G audio $USER
     ```



2. 安装/切换播放后端（99% 的 Linux 桌面系统已经自带 PulseAudio 或 PipeWire）

   pydub 默认会依次尝试：
   simpleaudio → pyaudio → ffplay → avplay。
   在 Linux 上最稳、最不需要折腾的是让 pydub 直接走 PulseAudio（或 PipeWire），而不是裸 ALSA。

   - 装 PulseAudio 命令行工具（很多发行版默认已装）

     ```bash
     sudo apt install pulseaudio pulseaudio-utils
     ```

   - 让 pydub 强制使用“pulse”设备（最简单的方法：加两行环境变量）
     在运行脚本前临时加：

     ```bash
     export AUDIODEV=pulse
     export AUDIODRIVER=pulse
     python your_script.py
     ```

     或者写在 Python 里（放在 import 之前）：

     ```python
     import os
     os.environ["AUDIODEV"] = "pulse"
     os.environ["AUDIODRIVER"] = "pulse"
     ```

     

3. 终极兜底：让 pydub 用 ffplay / ffprobe（完全绕开 ALSA）

   - 与上面安装 PulseAudio 工具一样，Ubuntu/Debian 版本安装命令如下：

     ```
     sudo apt install ffmpeg
     ```

   - 测试脚本：

     ```python
     from pydub import AudioSegment
     from pydub.playback import _play_with_ffplay
     
     speech_file_path = "temp_speech_1756804721.mp3"
     audio = AudioSegment.from_mp3(speech_file_path)
     _play_with_ffplay(audio)
     
     # 或者直接用 subprocess 调用 ffplay
     import subprocess, os
     subprocess.run(["ffplay", "-nodisp", "-autoexit", "temp_speech_1756804721.mp3"])
     ```



4. 如果不是以上配置问题，检查是不是因为系统混音器声音设置偏低，或者宿主机屏幕带有声卡而输出与输出选择的声卡不匹配导致的音频在播放但是没有声音。具体检查方法如下：

   - 系统混音器是不是静音了

     ```
     # 终端执行
     alsamixer          # 键盘方向键左右选通道，m 键取消 Mute，↑ 调大音量
     
     # 如果用的是 PulseAudio，就是图形化界面管理
     pavucontrol        # 图形界面，把 Playback / Output 都拉到 100%
     ```

   - 确认 ffplay 实际走的是哪个声卡：有的机器有多块声卡（HDMI、主板集成声卡、USB 耳机等），ffplay 可能默认把声音输出到 HDMI，而你的显示器又没接音箱，于是听不到。

     ```bash
     # 这一条指令可以显示设备所有的声卡，但是具体切换不会
     ffplay -nodisp -autoexit -devices
     ```

     

##### 紧急呼救关键词：

使用 sherpa-onnx 进行关键词识别



##### 语音助手关键词唤醒：

整体实现逻辑是在关键词检测时，当检测到特定的语音助手唤醒词后启动外部语音助手服务。由于关键词检测脚本持续启动而虽然语音服务每一次只进行一次对话，但是只要用户希望对话直接叫机器人"名字"就可以实现持续对话。

```python
# 在检测到关键词后，启动外部语音助手脚本。将以下代码插入到 audio_callback 中：
def audio_callback(self, msg):
    self.frames = self.frames[-32000:]
    self.frames.extend(msg.data)
    try:
        s = self.keyword_spotter.create_stream()
        s.accept_waveform(16000, np.array(self.frames, dtype=np.float16))
        s.input_finished()
        while True:
            if not self.keyword_spotter.is_ready(s):
                r = self.keyword_spotter.get_result(s)
                if r:
                    self.get_logger().info(f"{r} is detect")
                break
            r = self.keyword_spotter.get_result(s)
            if r:
                self.get_logger().info(f"Detection Keyword:{r}")
                # 创建 String 类型消息并发布
                msg_out = String()
                msg_out.data = r
                self.publisher.publish(msg_out)

                # 启动外部语音助手脚本
                self._start_assistant_script()

                break
            self.keyword_spotter.decode_streams([s])

    except Exception as e:
        logger.info(f"stopping by user: {e}")
        
        
# 在 KeywordSpotter 类中新增方法，用于启动外部脚本：
def _start_assistant_script(self):
    """启动外部语音助手脚本"""
    import subprocess
    import threading

    def run_script():
        try:
            # 替换为实际路径
            script_path = "/path/to/assistant_script.py"
            subprocess.Popen(["python3", script_path])
            self.get_logger().info("已启动语音助手脚本")
        except Exception as e:
            self.get_logger().error(f"启动语音助手脚本失败: {e}")

    # 使用线程避免阻塞 ROS2 主线程
    thread = threading.Thread(target=run_script, daemon=True)
    thread.start()
```



### 4日：

##### 语音打电话功能实现：

暂时商量的是使用 "打电话给儿子， 打电话给女儿" 两个词来测试，后面可以考虑用户自定义关键词来作为语音控制打电话的对象。



##### 紧急呼救关键词开发完成：

针对常见呼救关键词进行识别并在开发板上完场测试。



### 7日：

##### 下载火焰识别数据集：

寻找 YOLO 支持的数据集并根据demo查看模型的检测效果。了解其他可以在嵌入式开发板上实现并部署的火焰检测方案。



##### 火焰检测模型训练：

一轮训练结束，考虑对模型进行目标测试，设计易混物体比如灭火器。



### 8日：

##### 火焰识别模型测试：

测试结果显示一般，检测出的火焰目标存在置信度偏低的情况。另外测试图片中有一张完全没有检测到，但是没有对灭火器进行误检。



##### 功能包封装测试：

完成模型转换并将火焰检测模型放进开发板测试，但是出现所有检测对象的置信度均为0.5的 bug ，怀疑是因为模型的识别精度不够导致触发设定的最低重新检查模型检测逻辑。现在基本确定是由于检测到的物体置信度太低导致的。模型转换是损失太多精度导致开发板上模型的识别效率低，置信度低。

- 配置文件：

  ```yaml
  fire_detect_model: '/rknn_model/fire_best.rknn'
  
  model_w: 640
  model_h: 640 
  objectThresh: 0.1		# 置信度阈值，当把这个阈值调高以后输出结果默认置信度为0.5
  nmsThresh: 0.65
  
  classes:
    - "fire"
  ```

- 功能包下 utils 文件夹下 rknn_utils.py 文件中对低置信度目标设置的过滤机制，简单的来说就是当模型检测的物体置信度低于配置文件中设置的 objectThresh 则自动视为未检测到目标。

  ```python
  def filter_boxes(boxes, box_confidences, box_class_probs, objectThresh, keypoints=None):
      """Filter boxes with object threshold.
      """
      box_confidences = box_confidences.reshape(-1)
      candidate, class_num = box_class_probs.shape
  
      class_max_score = np.max(box_class_probs, axis=-1)
      classes = np.argmax(box_class_probs, axis=-1)
  
      _class_pos = np.where(class_max_score * box_confidences >= objectThresh)
      scores = (class_max_score * box_confidences)[_class_pos]
  
      boxes = boxes[_class_pos]
      classes = classes[_class_pos]
      if keypoints is not None:
          keypoints = keypoints[_class_pos]
          return boxes, classes, scores, keypoints
  
      return boxes, classes, scores
  ```

  

- ROS2 节点文件：文件中设置了目标检测输出内容以及对于低置信度目标的处理办法，即强制设置为0.5.

  ```python
      def image_callback(self, msg):
          # 图像大小640*480（w*h）
          # ros2消息类型(imgmsg)转换为np.array
          cv_img = bridge.imgmsg_to_cv2(msg, "bgr8")
          boxes, scores, classes = self.fire_detect.rknn_inference(cv_img)
          # print(emotions, scores)
          if boxes is None:
              self.msg.data = '无目标'
              logger.info('未检测到目标')
          else:
  
              for box, score, cl in zip(boxes, scores, classes):
                  top, left, right, bottom = [int(_b) for _b in box]
  
                  color = (0, 255, 0)
                  # color = (0, 255, 127)
  
                  # Draw the bounding box on the image
                  cv2.rectangle(cv_img, (top, left), (right, bottom), color, 2)
  
                  # Create the label text with class name and score
                  label = f'{self.CLASSES[cl]}: {score:.2f}'
  
                  # Calculate the dimensions of the label text
                  (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX,
                                                                   0.5, 1)
  
                  # Calculate the position of the label text
                  label_x = top
                  label_y = left - 10 if left - 10 > label_height else left + 10
  
                  # Draw a filled rectangle as the background for the label text
                  cv2.rectangle(cv_img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height),
                                color, cv2.FILLED)
  
                  # Draw the label text on the image
                  cv2.putText(cv_img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX,
                              0.5, (0, 0, 0), 1, cv2.LINE_AA)
  
          # 显示图像
          cv2.imshow('Image', cv_img)
          cv2.waitKey(1)  # 等待按键事件，1毫秒
          self._publisher.publish(self.msg)
  ```

  修改为下面的代码验证上述的错误分析:

  ```python
  def image_callback(self, msg):
          # 图像大小640*480（w*h）
          # ros2消息类型(imgmsg)转换为np.array
          cv_img = bridge.imgmsg_to_cv2(msg, "bgr8")
          boxes, scores, classes = self.fire_detect.rknn_inference(cv_img)
  
          # 添加调试打印
          print(f"DEBUG: boxes={boxes}, scores={scores}, classes={classes}")
  
          # 初始化消息为无目标
          self.msg.data = '无目标'
          has_high_confidence_target = False
      
          # 即使boxes不为None，也可能都是低置信度的框
          if boxes is not None and len(boxes) > 0:
              for box, score, cl in zip(boxes, scores, classes):
                  top, left, right, bottom = [int(_b) for _b in box]
              
                  # ！！！新增：设置一个高置信度阈值来判断是否真的是目标！！！
                  high_confidence_threshold = 0.5  # 您可以调整这个值
              
                  if score >= high_confidence_threshold:
                      has_high_confidence_target = True
                      # 只有高置信度目标才更新消息
                      self.msg.data = f'检测到{self.CLASSES[cl]}, 置信度:{score:.2f}'
  
                  # 无论置信度高低，都画框（这是您原来的逻辑）
                  color = (0, 255, 0) if score >= high_confidence_threshold else (0, 0, 255)  # 高置信度绿色，低置信度红色
              
                  # Draw the bounding box on the image
                  cv2.rectangle(cv_img, (top, left), (right, bottom), color, 2)
              
                  # Create the label text with class name and score
                  label = f'{self.CLASSES[cl]}: {score:.2f}'
              
                  # Calculate the dimensions of the label text
                  (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX,
                                                               0.5, 1)
              
                  # Calculate the position of the label text
                  label_x = top
                  label_y = left - 10 if left - 10 > label_height else left + 10
              
                  # Draw a filled rectangle as the background for the label text
                  cv2.rectangle(cv_img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height),
                            color, cv2.FILLED)
              
                  # Draw the label text on the image
                  cv2.putText(cv_img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX,
                          0.5, (0, 0, 0), 1, cv2.LINE_AA)
      
          # 根据是否有高置信度目标来记录日志
          if has_high_confidence_target:
              logger.info(f'检测到目标: {self.msg.data}')
          else:
              logger.info('未检测到高置信度目标')
      
          # 显示图像
          cv2.imshow('Image', cv_img)
          cv2.waitKey(1)  # 等待按键事件，1毫秒
          self._publisher.publish(self.msg)
  ```



### 9日：

##### 重新训练模型：

学习调整训练参数，替换数据集重新训练模型验证模型的检测精度。

- lr0：该参数设置优化器的初始学习率。指定学习率为 0.0001，这是许多优化任务的常见起始值。这一点针对小模型例如 YOLOv8n 设置的初始学习率调小。

- lrf：该参数指定最终学习率，其计算方式为初始学习率乘以lrf。它有助于在训练过程中逐渐降低学习率以稳定学习过程。（`最终学习率 = lr0 × lrf`）

  ```python
  # 采用余弦退火法并修改了 lr0 和 lrf
  lr0=0.1,
  lrf=0.001,
  cos_lr=True,
  
  # 结果此轮调整收益很高但是，精度最终也只在0.7左右
  ```

  

https://github.com/Username378/Fire_smoke_monitoring_system/blob/master/models/new.pt



##### 优化模型识别脚本：

ros2 节点文件中设置的检测逻辑是初始开启检测到物体终端无返回值但是如果初始为检测到物体就会一直显示未检测到目标，检查发布节点发布内容发现不受影响所以未修改这部分逻辑，在发布者节点中检查了发送内容不符合阅读逻辑，将发送内容修改为（类别 + 数量）分别显示。具体修改如下：

```python
def image_callback(self, msg):
    cv_img = bridge.imgmsg_to_cv2(msg, "bgr8")
    boxes, scores, classes = self.pet_detect.rknn_inference(cv_img)

    if boxes is None:
        self.msg.data = '无目标'
        logger.info('未检测到目标')
    else:
        # 统计每个类别的数量
        from collections import Counter
        class_counts = Counter([self.CLASSES[cl] for cl in classes])
        
        # 构建详细的消息
        detection_info = []
        for class_name, count in class_counts.items():
            detection_info.append(f"{class_name}:{count}个")
        
        self.msg.data = f'检测到目标: {", ".join(detection_info)}'

        for box, score, cl in zip(boxes, scores, classes):
            top, left, right, bottom = [int(_b) for _b in box]
            color = (0, 255, 0)
            cv2.rectangle(cv_img, (top, left), (right, bottom), color, 2)
            label = f'{self.CLASSES[cl]}: {score:.2f}'
            # ...（其余绘制代码）

    cv2.imshow('Image', cv_img)
    cv2.waitKey(1)
    self._publisher.publish(self.msg)
```

 

##### 配置新板子运行环境：

```bash
pip install PyAudio==0.2.14 -i https://pypi.tuna.tsinghua.edu.cn/simple 
Defaulting to user installation because normal site-packages is not writeable
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple 
Collecting PyAudio==0.2.14
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/26/1d/8878c7752febb0f6716a7e1a52cb92ac98871c5aa522cba181878091607c/PyAudio-0.2.14.tar.gz  (47 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Building wheels for collected packages: PyAudio
  Building wheel for PyAudio (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Building wheel for PyAudio (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [17 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.linux-aarch64-3.10
      creating build/lib.linux-aarch64-3.10/pyaudio
      copying src/pyaudio/__init__.py -> build/lib.linux-aarch64-3.10/pyaudio
      running build_ext
      creating build/temp.linux-aarch64-3.10
      creating build/temp.linux-aarch64-3.10/src
      creating build/temp.linux-aarch64-3.10/src/pyaudio
      aarch64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/include -I/usr/include -I/usr/include/python3.10 -c src/pyaudio/device_api.c -o build/temp.linux-aarch64-3.10/src/pyaudio/device_api.o
      src/pyaudio/device_api.c:9:10: fatal error: portaudio.h: No such file or directory
          9 | #include "portaudio.h"
            |          ^~~~~~~~~~~~~
      compilation terminated.
      error: command '/usr/bin/aarch64-linux-gnu-gcc' failed with exit code 1
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for PyAudio
Failed to build PyAudio
ERROR: Could not build wheels for PyAudio, which is required to install pyproject.toml-based projects
```

说明 PyAudio 编译时找不到 `portaudio.h` 头文件，这是因为系统中没有安装 PortAudio 的开发库。

- 解决办法：

  ```bash
  # 对于 Ubuntu
  sudo apt update
  sudo apt install portaudio19-dev
  
  # 重新安装 PyAudio
  pip install PyAudio==0.2.14 -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

  

### 10日：

##### rknn 工具链安装：

注意要看安装的工具链版本，首先是看系统是不是 aarch64 架构，其次查看 python 版本号是否匹配。

```bash
# 确认系统架构和 Python 版本
uname -m
python3 --version

# 安装工具链
pip install rknn_toolkit_lite2-2.3.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl
```



##### 功能包测试：

在新开发板上进行了关于视觉识别的测试，其中由于没有显示器人体跟踪无法测试。音频检测缺少显示器也无法测试。

```bash
qt.qpa.xcb: could not connect to display :0
qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "/home/ubt/.local/lib/python3.10/site-packages/cv2/qt/plugins" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: xcb.
```



现有完成测试如下：

- 流血检测：功能正常但是检测精度低，需要持续获取真实数据然后再训练模型检测。

  ![image-20250910102247487](/home/yls/.config/Typora/typora-user-images/image-20250910102247487.png)

- 宠物识别：正常
- 表情识别：正常



##### 火焰识别模型训练：

调整了模型训练参数但是模型识别的精度依旧只有0.7左右。



### 11日：

##### 模型训练脚本问题：

训练脚本和指令存在严重的冲突和配置问题，这直接导致了训练失效。关键在于300轮训练每一轮的损失都很大，一般来说模型训练50轮左右损失会大幅降低。

同时使用了两种配置方式，导致了冲突：

- **Python脚本**：在 `model.train()` 中设置了大量参数（如 `batch=64`, `data=...`, `device=...`）。

- **命令行指令**：又通过命令行传递了另一套参数（如 `--batch 64`, `--data ...`, `--device 0,1`）。

**最关键的是**，您的命令行指令使用了 `-m torch.distributed.run` 进行分布式训练，但您的Python脚本**并没有为分布式训练做任何准备**。这几乎是导致您训练失败的首要原因。



##### 模型注意力机制优化：

通过整合Ghost模块[[](https://www.hanspub.org/journal/paperinformation?paperid=92626#ref2)[2](https://www.hanspub.org/journal/paperinformation?paperid=92626#ref2)]和卷积块注意力机制(CBAM) [[](https://www.hanspub.org/journal/paperinformation?paperid=92626#ref3)[3](https://www.hanspub.org/journal/paperinformation?paperid=92626#ref3)]，对YOLOv8s模型进行了优化。Ghost模块通过分组卷积和线性变换的方式生成更多特征，从而在降低计算复杂度的同时保持模型的检测性能。卷积块注意力机制(CBAM)则通过引入空间和通道注意力，有效增强了模型对火灾特征的捕捉能力。



##### 损失问题、精度问题：

- 在初始训练中损失一直很大且一直降不下来。智能观察模型训练期间有没有过拟合表现，如果有就要停止训练然后再修改配置或者更换数据集重新训练。
  1. **box_loss** (边界框损失)： 衡量预测框与真实框之间的位置和形状差异（通常使用CIoU、DIoU等）。值越高，说明模型定位不准。
  2. **cls_loss** (分类损失)： 衡量预测的类别概率与真实标签之间的差异（通常使用交叉熵）。值越高，说明模型分类不准。
  3. **dfl_loss** (分布焦点损失)： 这是YOLOv8引入的改进。它不再直接回归框的宽高，而是回归一个分布，让模型更专注于学习边界附近难以区分的样本。这个值通常与其他损失在一个量级上。

- 模型经过格式转换和量化后精度太低了以至于触发了配置文件中最低置信度。修改配置文件中的置信度阈值发现对于置信度本身小于阈值的检测目标时可以正常识别的且误检率很低，但是对于高于阈值的目标置信度都是阈值归咎于模型的识别精度不够。



##### 智能体构建问题：

- 大模型回答问题缓慢：ollama部署模型导致模型运行效率不佳，这个可以考虑使用 vLLM 本地部署大模型。
- 语音回复问题：ollama不支持关闭模型思考模式导致智能体语音回复时播报思考内容，通过正则表达式过滤调带有 </think><think>标签的内容，另一方面可以在智能体的系统提示词上手动设置不显示思考内容。

​	

### 14日：

##### 火焰识别模型测试：

pytorch 模型的精度已经达到 0.86 但是经过模型转换在开发板上运行测试置信度依旧不高，现在打算从两个角度解决，首先继续训练模型根据训练日志显示模型并为完全拟合，其次对于高置信度目标检测的置信度均为 0.5 这一点没有搞懂。

- 模型转换问题

  ```bash
  # py -> onnx
  yolo export model=your_model.pt format=onnx
  ```

  ```python
  # onnx -> rknn
  rknn.load_onnx(
      model=ONNX_MODEL,
      inputs=['images'],                     # ONNX 的输入节点名
      input_size_list=[[1, 3, 640, 640]],    # 静态形状
      outputs=['/model.22/cv2.0/cv2.0.2/Conv_output_0',
               '/model.22/cv3.0/cv3.0.2/Conv_output_0',
               '/model.22/cv2.1/cv2.1.2/Conv_output_0',
               '/model.22/cv3.1/cv3.1.2/Conv_output_0',
               '/model.22/cv2.2/cv2.2.2/Conv_output_0',
               '/model.22/cv3.2/cv3.2.2/Conv_output_0']
  )
  ```

  

##### 火焰检测模型训练：

使用高精度模型做为训练模型底座在不同数据集上训练增加模型的泛化能力，避免模型过拟合新数据上的训练轮次设为100。



### 15日：

##### 验证ONNX模型精度：

根据在 pt 格式模型和 onnx 格式模型测试同一组图片的检测效果显示精度相同，检测的准确度相同。



##### YOLO 训练参数解析：

TP（True Positive）：表示将正类预测为正类的数量，即正确预测的正类样本数。

FN（False Negative）：表示将正类预测为负类的数量， 即错误预测的正类样本数。

FP（False Positive）：表示将负类预测为正类的数量，即错误预测的负类样本数。

TN（True Negative）：表示将负类预测为负类的数量，即正确预测的负类样本数。

- 精确率（Precision）：
  - 公式：Precision = TP / (TP + FP)
  - 解释：精确率是指在所有被模型预测为正例（Positive）的样本中，实际为正例的比例。它衡量了模型在正例预测中的准确性。

- 召回率（Recall）：
  - 公式：Recall = TP / (TP +  FN)
  - 解释：召回率是指在所有实际为正例的样本中，模型成功预测为正例的比例。它衡量了模型对正例的识别能力。

- F1：
  - 公式：F1 = 2TP / (2TP + FN + FP) = 2 x (precision x recall) / (precision + recall)
  - 解释：F1曲线是一种多分类问题中常用的性能评估工具，是精确率和召回率的调和平均数，取值范围介于0和1之间。1代表最佳性能，而0代表最差性能。



##### 量化精度分析：

现在面临的问题是模型转为rknn时中间的量化过程损失太多精度，如果采用混合精度开发板算力不支持。我已经测试了pt模型和onnx性能是相同的，网络上查到的资料显示可能是转rknn过程中删除了一些节点导致模型性能损失严重，通过神经网络图不够直观，我验证是否是这种情况导致的，于是想到在pc端验证节点是否缺失并同时运行onnx模型和rknn模型查看精度差异。

- 调整超参数：轮次（300）、优化器（SGD -> Adaw）、学习率（0.01 -> 0.001）

  - 学习率：在整个训练过程中学习率变化是由 lr0 递减到 lrf ，最后转为 lr0 * lrf。

  

- 更换数据集训练模型：但是模型的精度始终在 0.8 左右，即使达到 0.9 在模型进行板载测试时精度都会降低，使检测的置信度始终维持在 0.5 左右。

  

- 模型识别精度：通过检测同一验证集验证了 pt 格式模型与 onnx 格式模型识别精度，根据验证结果显示两个模型的检测精度相同。将问题定位到 onnx -> rknn 格式时量化损失过大。

  ```bash
  # 使用 YOLO 自带的模型转换指令将 pt 模型转换为 onnx
  yolo export model=path/to/model format=onnx
  
  # 验证模型检测效果
  yolo predict model=path/to/model source=path/to/fire
  ```

  

- 切换量化方法：使用 rknn 工具链支持的混合精度量化降低模型的量化损失，但是在板载测试中发现摄像头捕捉画面很卡顿并且模型并没有识别出火焰推测是算力不够导致的。工具链默认的量化格式是 INT8 ('asymmetric_quantized-8')，笔者调整为 'asymmetric_quantized-16' 。

  - 量化数据：

  ```python
      rknn.config(
          # see:ultralytics/yolo/data/utils.py
          mean_values=[[0, 0, 0]],
          std_values=[[255, 255, 255]],
          # TODO:使用下面均值、方差后，效果更差：
          # mean_values=[[123.675, 116.28, 103.53]],  # IMAGENET_MEAN = 0.485, 0.456, 0.406
          # std_values=[[58.395, 57.12, 57.375]],  # IMAGENET_STD = 0.229, 0.224, 0.225
          # quant_img_RGB2BGR=True,
          quantized_algorithm='normal',
          quantized_method='channel',
          # quantized_dtype='asymmetric_quantized-16', 
          # optimization_level=2,
          compress_weight=False,  # 压缩模型的权值，可以减小rknn模型的大小。默认值为False。
          # single_core_mode=True,
          # model_pruning=False,  # 修剪模型以减小模型大小，默认值为False。
          target_platform='rk3588'
      )
      
      rknn.load_onnx(
          model=ONNX_MODEL,
          inputs=['images'],                     # ONNX 的输入节点名
          input_size_list=[[1, 3, 640, 640]],    # 静态形状
          # 获取onnx模型中的以下六个输出，可打开https://netron.app查看节点
          outputs=['/model.22/cv2.0/cv2.0.2/Conv_output_0',
                   '/model.22/cv3.0/cv3.0.2/Conv_output_0',
                   '/model.22/cv2.1/cv2.1.2/Conv_output_0',
                   '/model.22/cv3.1/cv3.1.2/Conv_output_0',
                   '/model.22/cv2.2/cv2.2.2/Conv_output_0',
                   '/model.22/cv3.2/cv3.2.2/Conv_output_0'])
      rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET, rknn_batch_size=1)
      rknn.export_rknn(RKNN_MODEL)
  ```

  

- 增加训练数据：考虑挑选效果最好的模型使用不同数据集进行额外的训练增加模型的泛化能力，避免模型出现过拟合现象。

  

- 考虑算子失真问题：通过对比 ONNX 模型和 RKNN 模型在PC端的推理结果，来定位是节点缺失问题还是量化精度损失问题。由于在 RKNN 模型构建的过程中，对模型进行了量化，会无可避免的造成精度损失，使用精度分析接口，可以查看每一层的精度损失情况。通过混合量化，将某些损失较大的层，从量化层转为非量化层，从而提高模型的精度。



##### 版本影响：

查阅资料显示对于不同版本的 torch 和 torchvision 可能会因为随即种子参数导致训练出来的模型精度不同，并产生较大差异。



### 16日：

##### 数据集下载：

根据训练参数以及模型识别情况选出最优的模型，在选定的其他数据集上进行数据增强训练。目前已经完成数据集筛选，正在训练模型。



##### 关键词唤醒技术查看：

目前问题出在容易误检并且由于检测逻辑导致了一次关键词识别会出现输出两次的问题，后续通过修改检测逻辑或者在语音唤醒逻辑上调优。



##### 火焰检测模型训练：

在两个新的数据集上训练出新模型，目前最优的模型板载测试置信度不到 0.6。	 



### 17日：

##### 火焰检测模型训练：

训练了一个模型并进行板载测试，测试效果一般。



##### 语音助手唤醒修改：

尝试修改语音助手的唤醒代码逻辑书写。



##### 人体跟随测试：

在小车上完成了人体跟随的测试，目前可以识别但是小车跟随还有问题。



### 18日：

##### 服务器模型部署：

- 安装 vLLM ：

  ```bash
  pip install vllm -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

- 下载 modelscope：

  ```bash
  pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

- 模型下载：

  ```bash
  modelscope download --model Qwen/Qwen3-1.7B --local_dir /home/eogee/models
  ```

- 启动服务：

  ```bash
  vllm serve /home/fp/.cache/modelscope/hub/models/Qwen/Qwen3-8B \
    --served-model-name Qwen3-8B \
    --host 0.0.0.0 \
    --port 9000 \
    --dtype half \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 \
    --tensor-parallel-size 1 \
    --api-key "123456" \  	# 密钥可自行调整
  ```

- 测试模型：

  ```bash
  curl -N \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer 123456" \
    http://<your_ip>:9000/v1/chat/completions \
    -d '{
          "model": "Qwen3-8B",
          "messages": [{"role": "user", "content": "用一句话介绍自己"}],
          "stream": true,
          "max_tokens": 64
        }'
  ```

  

##### 火焰检测模型训练测试：

根据训练出来的新模型转换后放在板子上测试效果目前最优，不会出现大面积误检。

```
# 模型位置
/media/yls/1T硬盘4/model/fire/fire918/best.onnx
```



##### 语音助手优化：

- 语音助手异常终止：具体来说就是第一触发检测启动语音助手正常执行程序并关闭但是由于多次触发第一次关闭程序导致第二次执行的程序被异常中断引起程序报错。简单的来说就是子进程 A 正常跑完，子进程 B 成为孤儿，按 Ctrl-C 时只关掉 A，B 残留 → 出现“第二次唤醒进程不正常退出”的现象。代码在调用 `translator.stop()` 时，** recognizer 已经处于 stopped 状态**，而 `stop()` 方法内部没有做“重复停止”的保护，于是抛出了 `InvalidParameter` 异常。

  ```bash
  Traceback (most recent call last):
    File "/home/orangepi/fp_test/src/robot_speech_recognition/robot_speech_recognition/rknn_infer/voice_assistant.py", line 300, in <module>
      translator.stop()
    File "/home/orangepi/.local/lib/python3.10/site-packages/dashscope/audio/asr/translation_recognizer.py", line 602, in stop
      raise InvalidParameter(
  dashscope.common.error.InvalidParameter: TranslationRecognizerRealtime has stopped.
  
  # DashScope SDK 的 stop() 没有做幂等保护。用 try/except 把“重复停止”异常吃掉即可，保证进程干净退出。
  ```

  - 解决办法：在调用 `translator.stop()` 之前，先判断 recognizer 是否已经在停止状态。但 DashScope 的 SDK 并没有公开一个 `is_running()` 属性，所以我们可以用 `try/except` 包裹 `stop()`，把“重复停止”当作正常情况处理即可。

    ```python
    # 最简单修改
    finally:
        # 不再手动 stop，让 SDK 自动回调 on_close()
        callback.running = False
        if callback.process_thread.is_alive():
            callback.process_thread.join(timeout=1.0)
    ```

    

- 唤醒词触发频繁：关键词被“触发两次”并不是 sherpa-onnx 把同一帧音频判了两次，而是每来一帧 48 k 采样点就“从头”把 3 s 缓存全部喂给识别器，并且缓存里一直保留着上一次已经命中过的那段音频。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                这导致代码设置检测逻辑错误，出现一次唤醒将会执行两次语音助手脚本。

  - 解决办法：`sherpa_onnx.KeywordSpotter` 的 `create_stream()` 每次都会**重新扫描整段音频**，它本身**不带“输出延迟抑制”**，也**不会把已经触发过的关键词从缓存里抹掉**。因此必须保证：

    1. 命中后**清空缓存**（或至少把包含关键词的那一段滑掉）；
    2. 在**抑制时间窗内**（例如 5 s）不再重复启动脚本。

    ```python
    import threading
    import time
    
    class KeywordSpotter(Node):
        """
        1. 同一帧音频不会重复命中；
        2. 在5 s 内即使缓存里再次出现相同关键词，也不会重新拉起脚本；
        3. 子进程只启动一次，不会出现“第二次唤醒进程”残留。
        """
        def __init__(self, ...):
            ...
            self._trigger_lock = threading.Lock()
            self._last_trigger_time = 0          # 时间戳
            self.cooldown = 5.0                  # 抑制 5 s
    
        def audio_callback(self, msg):
            # 1. 先更新缓存
            self.frames = self.frames[-32000:]
            self.frames.extend(msg.data)
    
            # 2. 如果在冷却期内，直接 return
            with self._trigger_lock:
                if time.time() - self._last_trigger_time < self.cooldown:
                    return
    
            # 3. 正常识别
            s = self.keyword_spotter.create_stream()
            s.accept_waveform(16000, np.array(self.frames, dtype=np.float32))
            s.input_finished()
    
            while self.keyword_spotter.is_ready(s):
                self.keyword_spotter.decode_streams([s])
                r = self.keyword_spotter.get_result(s)
                if r:
                    self.get_logger().info(f"Detection keyword:{r}")
                    msg_out = String()
                    msg_out.data = r
                    self.publisher.publish(msg_out)
    
                    # 4. 成功触发，更新时间戳并清空缓存
                    with self._trigger_lock:
                        self._last_trigger_time = time.time()
                    self.frames.clear()
                    self.frames.extend([0.0] * 48000)   # 可选：填 0 或直接 []
                    self._start_assistant_script()
                    break
    ```

    

### 21日：

##### 火焰检测模型优化：

使用24服务器上 D:\fire\fire6 数据集训练的模型进行混合训练，尝试增加模型的识别准确度。



##### ChatTTS 部署优化：

之前在运行的时候为了方便看 demo 所以使用 CPU 进行部署，现在修改这一操作使用 GPU 来加速推理。在 GPU 部署条件下经过实测一段60字左右的回答内容需要经过大概20秒的转化才能输出语音内容。在同时打开大模型推理服务时同一个任务消耗时间相同。	

- 完整卸载 CPU 版本的 torch 工具（不留残余）

  ```bash
  pip uninstall -y torch torchaudio torchvision \
                  vector-quantize-pytorch vocos encodec  # 这几个会链 torch
  ```



- 确保 site-packages 里再无 `torch*lib`

  ```bash
  ls $CONDA_PREFIX/lib/python3.10/site-packages | grep torch
  ```



- 重新安装 GPU 版（CUDA 11.8 通道）

  ```bash
  # 一次装齐 GPU 版 2.1 + cu121
  pip install torch==2.1.0+cu121 torchaudio==2.1.0+cu121 \
              --index-url https://download.pytorch.org/whl/cu121
  ```

  如果遇到以下报错：pip 提示 chattts 依赖的 `vector_quantize_pytorch` 和 `vocos` 被你先前的卸载操作一起清掉了，现在补装即可，不影响 torch 本身。

  ```bash
  # 报错 1
  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
  chattts 0.2.4 requires vector_quantize_pytorch, which is not installed.
  chattts 0.2.4 requires vocos, which is not installed.
  
  # 解决办法
  pip install vector_quantize_pytorch vocos
  ```

  

  ```bash
  # 报错 2
  ChatTTS 类中缺少方法或者 infer() 函数中缺少参数等错误
  
  # 错误原因
  由于 pip 版本 的 ChatTTS 里根本没有暴露 RefineTextParams / InferCodeParams 这两个类，包括函数可能也没有暴露。需要替换成 chattts dev 版本
  ```



##### 语音助手优化：

现在出现的问题是，当语音助手回答问题语言内容过长时会被误检导致虽然还在回答问题，但是下一次关键词唤醒已经被触发导致回答问题的内容被识别成了新问题提交给大模型，等到模型回答这个问题转换为语音又会再重复上面的错误造成 “**自唤醒**” 或 “**回声误唤醒**” 。			



### 22日：

##### ChatTTS 环境部署：

运行 pip install vllm==0.4.1 希望安装其帮助 ChatTTS 运行提速但是卡在了下面这一步。vllm-nccl-cu12 这个包本身只有 **6 kB**，但它会在 `setup.py` 里 **实时去 GitHub 拉取 200 MB 的 `libnccl.so.2.18.1`**（外网文件）。国内网络 / 公司内网 / 纯离线环境 就会卡在`Downloading cu12-libnccl.so.2.18.1 ...` 这一步，**并不是 PyPI 镜像问题**，而是 **GitHub 连接超时**。

```bash
Collecting vllm-nccl-cu12<2.19,>=2.18 (from vllm==0.4.1)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/41/07/c1be8f4ffdc257646dda26470b803487150c732aa5c9f532dd789f186a54/vllm_nccl_cu12-2.18.1.0.4.0.tar.gz  (6.2 kB)
```

- 解决办法：

  - 找一台能访问 GitHub 的电脑，手动下载（200 MB 左右）

    ```html
    https://github.com/vllm-project/vllm-nccl/releases/download/v0.1.0/cu12-libnccl.so.2.18.1
    ```

  

  - 拷贝到目标机器 **固定目录**（目录不存在就新建）：

    ```bash
    mkdir -p ~/.config/vllm/nccl/cu12/
    cp cu12-libnccl.so.2.18.1 ~/.config/vllm/nccl/cu12/
    ```

    - 按照默认名字安装，这里提示系统会寻找名称为 libnccl.so.2.18.1 的文件所以需要将复制过去的文件修改为指定名称。

      ```bash
      Collecting vllm-nccl-cu12<2.19,>=2.18 (from vllm==0.4.1)
        Using cached https://pypi.tuna.tsinghua.edu.cn/packages/41/07/c1be8f4ffdc257646dda26470b803487150c732aa5c9f532dd789f186a54/vllm_nccl_cu12-2.18.1.0.4.0.tar.gz (6.2 kB)
        Preparing metadata (setup.py) ... error
        error: subprocess-exited-with-error
        
        × python setup.py egg_info did not run successfully.
        │ exit code: 1
        ╰─> [15 lines of output]
            Traceback (most recent call last):
              File "<string>", line 2, in <module>
              File "<pip-setuptools-caller>", line 35, in <module>
              File "/tmp/pip-install-23s27rjo/vllm-nccl-cu12_2ba9f5531c2c43628142d8d2f8385092/setup.py", line 83, in <module>
                if get_md5_hash(destination) != file_hash:
              File "/tmp/pip-install-23s27rjo/vllm-nccl-cu12_2ba9f5531c2c43628142d8d2f8385092/setup.py", line 43, in get_md5_hash
                with open(file_path, "rb") as f:  # Open file in binary read mode
            FileNotFoundError: [Errno 2] No such file or directory: '/home/fp/.config/vllm/nccl/cu12/libnccl.so.2.18.1'
            Downloading nccl package from https://github.com/vllm-project/vllm-nccl/releases/download/v0.1.0/cu12-libnccl.so.2.18.1
            Failed to download nccl package from https://github.com/vllm-project/vllm-nccl/releases/download/v0.1.0/cu12-libnccl.so.2.18.1
            <urlopen error retrieval incomplete: got only 327680 out of 291649520 bytes>
            md5 hash of downloaded file does not match expected hash, retrying
            Downloading nccl package from https://github.com/vllm-project/vllm-nccl/releases/download/v0.1.0/cu12-libnccl.so.2.18.1
            Failed to download nccl package from https://github.com/vllm-project/vllm-nccl/releases/download/v0.1.0/cu12-libnccl.so.2.18.1
            Remote end closed connection without response
            [end of output]
        
        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: metadata-generation-failed
      
      × Encountered error while generating package metadata.
      ╰─> See above for output.
      
      note: This is an issue with the package mentioned above, not pip.
      hint: See above for details.
      
      # 修改指令
      mv cu12-libnccl.so.2.18.1 libnccl.so.2.18.1  # 可自行检查名称是否修改正确
      ```

      

  - 重新 pip install，**发现文件已存在就会跳过下载**：

    ```bash
    pip install vllm==0.4.1 -i https://pypi.tuna.tsinghua.edu.cn/simple
    ```

    

##### 语音助手优化：

同一进程（或同一 PyAudio/ALSA 上下文）里，先打开 Monitor → 关闭 → 立即打开真实麦克风，而 ALSA 驱动没来得及释放句柄，导致第二次 open 失败，PortAudio 把它包装成 `[Errno -9999] Unanticipated host error`。简单的说**是“同一个程序太快开关两个设备，ALSA 没来得及释放”** 

- **ROS 节点启动**
  `pa.open(..., device=monitor_index)` → 成功，开始监回放。
- **关键词触发**
  `stream.stop_stream(); stream.close()` → 以为已经“释放”了，但 ALSA 底层句柄仍被标记为“in use”（尤其嵌入式驱动）。**
- **立即再开麦克风**
  `pa.open(..., device=hw_mic_index)` → ALSA 返回 `EBUSY` →PortAudio 翻译成 `-9999`。



##### 火焰检测模型训练：

在最表现最好的模型基础上进行混合训练，但是结果不好依然会出现最高置信度只有0.5的情况且存在误检的情况。



### 23日：

##### ChatTTS 部署测试：

使用 vllm 进行加速处理发现和不进行加速处理转化速度一致。目前大概是生成多少秒的音频消耗多长时间去转换音频。

- 部署步骤：

  - 离线 wheel 准备（国内镜像就能拉，不用 GitHub）

    | 包                     | 下载地址（国内镜像）                                         | 说明                          |
    | ---------------------- | ------------------------------------------------------------ | ----------------------------- |
    | torch-2.2.1+cu118      | <https://mirrors.aliyun.com/pytorch-wheels/cu118/torch-2.2.1+cu118-cp310-cp310-linux_x86_64.whl> | 主框架                        |
    | torchaudio-2.2.1+cu118 | <https://mirrors.aliyun.com/pytorch-wheels/cu118/torchaudio-2.2.1+cu118-cp310-cp310-linux_x86_64.whl> | 与 torch 同版本               |
    | vllm-0.4.1+cu118       | <https://github.com/vllm-project/vllm/releases/download/v0.4.1/vllm-0.4.1+cu118-cp310-cp310-manylinux1_x86_64.whl> | 如 GitHub 打不开，用 手动编译 |
    | vllm-nccl-cu12-2.18.1  | 用你前面已下载的 `whl` 或缓存 so 即可                        | 不再重下                      |

  - 一次性安装（顺序不能乱）

    ```bash
    # 1. 统一 cu118 版 torch / audio
    pip install torch-2.2.1+cu118-cp310-cp310-linux_x86_64.whl \
                torchaudio-2.2.1+cu118-cp310-cp310-linux_x86_64.whl
    
    # 2. 本地 nccl wheel（你已有）
    pip install vllm_nccl_cu12-2.18.1.0.4.0-cp310-cp310-linux_x86_64.whl
    
    # 3. vllm 本体（如 GitHub 打不开 → 用下方「源码编译」）
    pip install vllm-0.4.1+cu118-cp310-cp310-manylinux1_x86_64.whl
    ```

  -  继续装 ChatTTS-dev 其余依赖

    ```bash
    cd /path/ChatTTS-dev          # 你之前克隆的 dev 分支
    pip install -r requirements.txt
    pip install -e .               # 源码入口
    ```



- 测试模型加速效果：第一次运行可能会卡在模型下载上，将 ChatTTS 项目文件夹下 assert 文件夹复制到 chattts dev 项目下。

  ```python
  import ChatTTS
  import torch
  import torchaudio
  
  chat = ChatTTS.Chat()
  chat.load(compile=False) # Set to True for better performance
  
  texts = ["实时率等于识别花的时间除以语音本身的时间", "尝试持用不同版本的源码以及手动下载和脚本下载模型文件的方式进行模型部署"]
  
  wavs = chat.infer(texts)
  
  for i in range(len(wavs)):
      """
     在某些版本的torchaudio里，第一行代码能运行，但在另一些版本中，则是第二行代码能运行。
      """
      try:
          torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)
      except:
          torchaudio.save(f"basic_output{i}.wav", torch.from_numpy(wavs[i]), 24000)
  
  print(next(chat.gpt.parameters()).device, next(chat.gpt.parameters()).dtype)
  ```

  

##### 语音助手调优：

解决了一个程序打开过快导致的资源未释放问题，同时新问题出现主程序会异常退出。



##### 人体跟踪小车测试：

能绕桌子走但是存在走的不稳定的问题，基本功能实现。



### 24日：

##### ChatTTS 环境配置：

```bash
Traceback (most recent call last):
  File "/home/fp/chatts_dev/tts_test.py", line 23, in <module>
    import ChatTTS
  File "/home/fp/chatts_dev/ChatTTS/__init__.py", line 1, in <module>
    from .core import Chat
  File "/home/fp/chatts_dev/ChatTTS/core.py", line 11, in <module>
    import torch
  File "/home/fp/anaconda3/envs/chattts/lib/python3.10/site-packages/torch/__init__.py", line 237, in <module>
    from torch._C import *  # noqa: F403
ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory
```

这是 **PyTorch 能 import，但找不到 cuDNN 运行时** 的典型错误：`libcudnn.so.8` 缺失 → **CUDA 工具链不完整**

- 解决办法：

  ```bash
  # 1. 一次性添加 NVIDIA 仓库（国内镜像）
  sudo apt update
  sudo apt install -y software-properties-common
  sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub
  sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /"
  
  # 2. 装最新 8.x（无需指定小版本）
  sudo apt update
  sudo apt install -y libcudnn8 libcudnn8-dev
  ```

  

##### 烟雾检测模型训练：

训练烟雾检测模型搭配火焰检测模型尝试在精度不高的情况下联合检测失火判断情况。



##### 语音助手优化：

现在的bug都是由于基于关键词检测的唤醒逻辑造成的现在考虑换一种唤醒的方式看能不能从源头解决问题。



### 25日：

##### 实现 TTS 服务：

寻找 pico TTS 的中文语音包，中间发现 Ekho（需要项目源码但是下载不下来）可能可以还有一个 Piper TTS 项目两个均可以在嵌入式设备上运行。

- piper TTS 模型转换：`dynamic_input[0][0]=1 must be a list` 是 RKNN-Toolkit2 的典型语法限制，**不是模型不支持**，但即使格式正确，后续仍可能因结构问题失败 
  1. Piper TTS 模型结构复杂，Piper 使用的是基于 **VITS 的 TTS 模型**，包含：
     - 动态维度 (`batch_size`, `phonemes`)
     - 不支持的算子（如 `Loop`, `If`, `Gather`, `Slice` 等）
     - 非标准输入输出结构（非图像/CV 类）

- 这类模型 **不属于 RKNN 官方支持的主流 CV 模型**，转换成功率极低 

  2. **RKNN-Toolkit2 的动态输入机制限制**， 动态输入并非“任意长度”，而是**预设多组固定形状**，每组都会生成独立子图，导致：
     - 模型体积膨胀
     
     - 不支持真正变长输入
     
     - TTS 类模型难以适配 
     
       

##### 烟雾检测模型训练：

模型在服务器正常训练，昨天训练的模型检测效果一般但是不会检测抽烟造成的烟雾。另外下载数据集有问题网络不稳定造成的



### 26日：

##### piper TTS 服务搭建

使用 piper TTS 搭建了一个文字转语音服务的工具，效果还可以没有明显机械音。但是现在面临两个问题第一个会占用 CPU 资源，第二个不支持数字以及英文的转换只能转换纯中文文本。下面是部署和测试的具体流程。

- 环境准备：

  - Orange Pi 5S 通常运行的是基于 ARM64 的 Linux 系统（如 Ubuntu 或 Debian），你需要先确认系统架构：

    ```bash
    uname -m
    # 输出应为 aarch64（即 ARM64）
    ```

  - 安装依赖

    ```bash
    sudo apt update
    sudo apt install curl git unzip sox wget
    ```



- 下载并安装 Piper 可执行文件

  Piper 官方提供了预编译的 ARM64 版本，适合 Orange Pi 5S。

  ```bash
  # 下载并解压
  curl -LO https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_arm64.tar.gz
  tar -xzf piper_arm64.tar.gz
  cd piper
  
  # 确保 piper 可执行文件在当前目录下
  chmod +x piper
  ```



- 下载语音模型（中文示例）

  Piper 的模型托管在 Hugging Face，你可以手动下载或使用 `wget` 下载中文模型：

  ```bash
  # 创建模型目录
  mkdir -p models
  
  # 下载中文女声模型（示例）
  wget https://huggingface.co/rhasspy/piper-voices/resolve/main/zh/zh_CN/huayan/medium/zh_CN-huayan-medium.onnx -P models/
  wget https://huggingface.co/rhasspy/piper-voices/resolve/main/zh/zh_CN/huayan/medium/zh_CN-huayan-medium.onnx.json -P models/
  ```

  你也可以从 Hugging Face 浏览其他模型：https://huggingface.co/rhasspy/piper-voices/tree/main



- 运行 Piper TTS

  使用命令行测试语音合成并播放生成的音频（需安装 `alsa-utils`）：

  ```bash
  # 转换指令
  echo "你好，欢迎使用Piper语音合成！" | ./piper \
    --model models/zh_CN-huayan-medium.onnx \
    --config models/zh_CN-huayan-medium.onnx.json \
    --output_file output.wav
    
  #测试指令
  sudo apt install alsa-utils
  aplay output.wav
  ```

  后续如果使用可以使用下面的转换脚本

  ```python
  # 转换脚本
  from piper import Piper
  
  piper = Piper("models/zh_CN-huayan-medium.onnx")
  audio = piper.synthesize("你好，这是通过Python调用的语音合成。")
  
  with open("output.wav", "wb") as f:
      f.write(audio)
  ```



##### piper TTS 语音转换音色问题：

```bash
[2025-09-26 10:48:21.652] [piper] [info] Loaded voice in 0.392397075 second(s)
[2025-09-26 10:48:21.652] [piper] [info] Initialized piper
[2025-09-26 10:48:23.422] [piper] [warning] Missing 3 phoneme(s) from phoneme/id map!
[2025-09-26 10:48:23.422] [piper] [warning] Missing "1" (\u0031): 1 time(s)
[2025-09-26 10:48:23.422] [piper] [warning] Missing "2" (\u0032): 7 time(s)
[2025-09-26 10:48:23.422] [piper] [warning] Missing "5" (\u0035): 20 time(s)
huayan_medium_test.wav
[2025-09-26 10:48:23.424] [piper] [info] Real-time factor: 0.14969781776494565 (infer=1.762841502 sec, audio=11.776 sec)
[2025-09-26 10:48:23.424] [piper] [info] Terminated piper
```

Missing 3 phoneme(s) … Missing "1" "2" "5"说明 **文本里混进了全角数字**（①②⑤ 这类 Unicode 圆圈序号），模型词表里找不到对应音素，于是跳过发音，听起来会漏字。

```bash
echo "春天来了，微风拂面，柳树抽出嫩芽，小草从土里探出头来，大地一片生机勃勃，仿佛一切都重新开始。" \
| perl -CSD -pe 's/[^\p{Han}，。！？、]//g' \
| ./piper \
    --model models/zh_CN-huayan-medium.onnx \
    --config models/zh_CN-huayan-medium.onnx.json \
    --output_file huayan_medium_test2.wav
```

- `perl -CSD`：开启 UTF-8 输入/输出/错误流
- `\p{Han}`：Unicode 汉字属性，等价于 `\u4e00-\u9fff` 但通用
- 只保留汉字和常用中文标点，**彻底剔除隐藏字符、全角数字、emoji 等**

运行后若日志里 **不再出现 `Missing ...`**，就说明净化成功，可放心播放 `huayan_medium_test2.wav`。



##### 烟雾识别模型训练：

训练出来的烟雾检测模型经过模型转化和量化操作进行板载测试效果一般。



### 28日：

##### 语音助手优化：

考虑添加中断机制。尝试使用新的唤醒方法已经制作好用于识别的模型但是项目需要一个 github 下载的资源下载指令放在文档中了。



##### 烟雾检测模型训练：

寻找并下载合适的数据集并传入服务器上，新训练出了两个模型板载测试效果比之前的好。



### 29日：

##### 语音助手唤醒优化：

- 唤醒项目部署编译：

  唤醒功能基于 snowboy 实现的一个识别多语言自定义关键词唤醒功能，下面介绍项目的部署文档。

  - 安装pyaudio

    ```bash
    sudo apt-get install libjack-dev
    sudo apt-get install pyaudio
    ```

    

  - 配置麦克风：连接麦克风使用网页或者指令检查麦克风是否可以正常使用。

    ```bash
    rec t.wav
    ```

    

  - 下载编译 swig 和 snowboy

    ```bash
    # 下载 swig 项目
    sudo wget http://downloads.sourceforge.net/swig/swig-3.0.10.tar.gz sudo tar -xvzf swig-3.0.10.tar.gz
    
    cd swig-3.0.10/
    
    # 编译 SWIG
    /configure  --prefix= /usr \ 
    			--without-clisp \		
    			--without-maximum-compile-warnings 
    
    sudo make & sudo make install
    
    sudo install -v-m755-d /usr/share/doc/swig-3.0.10 sudo cp -v-R Doc/* /usr/share/doc/swig-3.0.10 
    
    cd ..
    
    # 下载编译 snowboy 
    sudo git clone https://github.com/Kitt-Al/snowboy cd snowboy/swig/Python3 && sudo make	# 这一步可能报错详见下 编译 snowboy 项目错误
    ```

  

  - 配置修改

    打开/snowboy/snowboy/examples/Python3/snowboydecoder.py，把其中的“from . import snowboydetect”改为“import snowboydetect”。

    ```bash
    cd /snowboy-seasalt-master/snowboy/snowboy/examples/Python3
    ```

    

  - 复制所需文件到自己的工程目录

    创建一个自己的工程目录，然后把如下文件复制到目录中：

    1. /snowboy-seasalt-master/snowboy/swig/Python3/目录下除了“Makefile”以外的所有文件。

       ```bash
       sudo cp _snowboydetect.so /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboy-detect-swig.i /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboy-detect-swig.o /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboy-detect-swigcc /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboy-detect-swig.cc /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboydetect.py /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       ```

    2. /snowboy-seasalt-master/snowboy/目录下的"resource"文件夹及其所有文件。

       ```bash
       sudo cp -r resources/ /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       ```

    3. /snowboy-seasalt-master/snowboy/examples/Python3/目录下的“snowboydecoder.py”文件和”demo.py”。

       ```bash
       sudo cp demo.py /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboydecoder.py /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       ```

       

  - demo测试

    进入工程目录，输入指令进行测试。

    ```bash
    # 测试 demo 
    python3 demo.py resources/models/snowboy.umdl
    ```

    如果需要替换关键词可以按照下面的操作进行。

    1. [浏览器](https://snowboy.hahack.com/)进入模型生成网页按照要求完成语音输入并生成模型放在项目 /resources/models 文件夹下

    2. 修改上述指令进行测试

       ```
       python3 demo.py resources/models/xiaoming.umdl
       ```

       

- 编译 snowboy 项目错误

  ```bash
  g++ -I../../ -O3 -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++0x  -shared snowboy-detect-swig.o \
  ../..//lib/ubuntu64/libsnowboy-detect.a -L/usr/lib/python3.10/config-3.10-x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu  -lcrypt -ldl  -lm -lm  -lm -ldl -lf77blas -lcblas -llapack -latlas -o _snowboydetect.so
  /usr/bin/ld: 找不到 -lf77blas: 没有那个文件或目录
  /usr/bin/ld: 找不到 -lcblas: 没有那个文件或目录
  /usr/bin/ld: 找不到 -latlas: 没有那个文件或目录
  collect2: error: ld returned 1 exit status
  make: *** [Makefile:73：_snowboydetect.so] 错误 1
  ```

  - 错误原因：系统**缺少 ATLAS 及其相关开发包**（`libatlas-dev`、`libf77blas-dev` 等），导致链接器找不到这些数学库，直接安装缺失的开发包即可。

    ```bash
    sudo apt update
    sudo apt install libatlas-base-dev libf77blas-dev libcblas-dev
    
    # 在新版本新版本 Ubuntu 中已被整合进 libatlas-base-dev 或其他包。
    sudo apt update
    sudo apt install libatlas-base-dev
    
    # 重新编译并检查是否已经有编译文件产生
    cd snowboy/swig/Python3
    sudo make clean
    sudo make
    ls -lh _snowboydetect.so     # 应该输出类似 -rwxr-xr-x 1 root root 1.1M  9月 29 09:04 _snowboydetect.so
    ```

  

  ```bash
  "fatal error: Python.h: No such file or directory"
  ```

  - 错误原因：可能是 Python 开发包没有安装

    ```bash
    sudo apt-get install python3-dev
    
    sudo apt-get install sox		# 不安装可能不能正常唤醒
    ```



##### 烟雾检测模型训练：

- 训练模型正常进行，但是精度都不高并且奇怪的是有一个数据集开始训练的100左右效果最好，超过100轮后效果越来越差但是精确度和召回率却很高。

- 在 kaggle 上找到两个开源模型检测效果还可以但是转换为 rknn 后放在开发板上测试发现模型推理输出格式不一致导致功能包不能正常检测。

  ```bash
    File "/home/orangepi/fire_detect/install/fire_detect/lib/python3.10/site-packages/fire_detect/utils/rknn_utils.py", line 167, in box_process
      grid_h, grid_w = position.shape[2:4]
  ValueError: not enough values to unpack (expected 2, got 1)
  [ros2run]: Process exited with failure 1
  ```



##### 新开发板功能包测试：

- SSH 远程连接报错：

  ```bash
  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
  @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
  IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
  Someone could be eavesdropping on you right now (man-in-the-middle attack)!
  It is also possible that a host key has just been changed.
  The fingerprint for the ED25519 key sent by the remote host is
  SHA256:P3zq/DvQqlKdwlRDib9H9g8o2Y4Q2e5SLDhAAczw3fA.
  Please contact your system administrator.
  Add correct host key in /home/yls/.ssh/known_hosts to get rid of this message.
  Offending ECDSA key in /home/yls/.ssh/known_hosts:18
    remove with:
    ssh-keygen -f "/home/yls/.ssh/known_hosts" -R "192.168.1.240"
  Host key for 192.168.1.240 has changed and you have requested strict checking.
  Host key verification failed.
  lost connection
  ```

  说明本地 `.ssh/known_hosts` 文件中记录的 `192.168.1.240` 的主机密钥，与远程主机当前提供的密钥不一致。表明尝试通过 `scp`（或 `ssh`）连接的主机（`192.168.1.240`）的 **SSH 主机密钥发生了变化**，而你的客户端配置了严格的主机密钥检查，因此连接被拒绝。这通常是出于安全考虑，防止中间人攻击（Man-in-the-Middle Attack）。

  - 解决办法：运行以下命令，删除 `known_hosts` 中关于 `192.168.1.240` 的旧记录，然后重新连接。

    ```bash
    # 删除旧记录
    ssh-keygen -f "/home/yls/.ssh/known_hosts" -R "192.168.1.240"
    
    scp your_file yls@192.168.1.240:/path/to/destination
    ```

    

- 环境安装完成但是可能是网络原因导致不能正常安装 ROS2 导致不能测试功能包。

  ```bash
  pip install cv-bridge==3.2.1
  Defaulting to user installation because normal site-packages is not writeable
  ERROR: Could not find a version that satisfies the requirement cv-bridge==3.2.1 (from versions: 1.13.0.post0)
  ERROR: No matching distribution found for cv-bridge==3.2.1
  ```

  `cv_bridge==3.2.1` 这个版本在 PyPI（Python 官方包仓库）上根本不存在，你看到的唯一版本是 `1.13.0.post0`，而 3.2.1 是 ROS（Robot Operating System）里随 `ros-melodic-cv-bridge` 等 deb 包一起发布的版本号，并不是给 `pip` 用的。

  

- 安装 ROS2 环境：

  - 在科学上网机器上克隆仓库

    ```bash
    git clone https://github.com/fishros/install.git
    ```

  - 用局域网 scp（两台机器在同一局域网）

    ```bash
    scp -r install/ ubt@192.168.1.240:/home/ubt
    ```

  - 安装：

    ```bash
    # 根据你放的路径调整
    cd ~/fishros_install
    
    python3 install.py		# 按照提示选择合适源和版本
    ```

    注意如果这里报了类似的错误：中间选择完源后有一个选项记得先选 "n" 再选 "y"。

    ```bash
    --2025-09-29 12:00:07--  https://raw.githubusercontent.com/fishros/install/master/tools/tool_config_system_source.py
    Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 0.0.0.0, ::
    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|0.0.0.0|:443... failed: Connection refused.
    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|::|:443... failed: Connection refused.
    ```



- 测试过程中有两个问题：

  这两个问题在旧的开发板上运行都没有问题，在新的开发板上运行报错。问了 AI 说0.9.8 与 0.9.6 不完全兼容，Rockchip 在 0.9.8 里默认关闭/移除了对某些 Slice 算子的支持，导致你的模型 fallback 失败。不过这里不确定在新的开发板中建议还是装 0.9.6 版本的 NPU 驱动。

  - 在测试语音检测时遇到如下报错：

    ```bash
    [ WARN:0] global ./modules/videoio/src/cap_gstreamer.cpp (1100) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1
    ```

  - 在测试表情检测时遇到如下问题：

    ```bash
    num_frames: 103
    E RKNN: [15:46:45.151] Meet unsupported slice
    E RKNN: [15:46:45.151] Op type:Slice, name: Slice:/Gather_194_2sl, fallback cpu failed. please try updating to the latest version of the toolkit2 and runtime from: https://console.zbox.filez.com/l/I00fc3 (PWD: rknn)
    [ros2run]: Segmentation fault
    ```



### 30日：

##### 优化语音助手：

- 模型数字输出：优化了模型回答问题包含阿拉伯数字的情况，这样转语音的时候不会出现错误转换的情况。已经在服务器上测试完成。

- 测试多人关键词唤醒性能：

- 整合语音助手：在开发板上集成 Piper TTS，实现回复的内容不使用接口使用本地离线服务。



##### 烟雾检测模型训练：

- 模型正常训练：尝试将两个模型合并，另外有几个数据集是同时检测的。测试和分开模型检测的效果哪个更好。

- 量化精度损失问题：通过增加量化 image 文件夹中的图片尝试解决，可以增加模型检测的精度但是很有限。

- 模型转换输出不一致问题：

  ```bash
  Traceback (most recent call last):
    File "/home/yls/YOLOV8-on-RK3588/rk3588/python/fire_detect/rknn_infer_x86.py", line 344, in <module>
      boxes, scores, classes = post_process(input_data=outputs, ratio=ratio, dw=dw, dh=dh, shape=(orig_h, orig_w))
    File "/home/yls/YOLOV8-on-RK3588/rk3588/python/fire_detect/rknn_infer_x86.py", line 218, in post_process
      boxes.append(box_process(input_data[i]))
    File "/home/yls/YOLOV8-on-RK3588/rk3588/python/fire_detect/rknn_infer_x86.py", line 190, in box_process
      grid_h, grid_w = position.shape[2:4]
  ValueError: not enough values to unpack (expected 2, got 1)
  ```

  

## 十月

### 9日：

##### 烟雾检测模型训练：

服务器上使用火焰 + 烟雾的数据集进行新模型训练，目前都能检测出来但是精度不高。数据集是 smoke-fire1，训练结果如下：

```
Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 194/194 [00:35<00:00,  5.49it/s]
                   all       3099       3932      0.772      0.708      0.778       0.46
                 smoke       1550       1756      0.829       0.77      0.836      0.527
                  fire        879       2176      0.715      0.647      0.719      0.393
```



##### Piper TTS 集成测试：

已经成功集成在语音助手上，现在问题在结束一轮对话后会自动触发第二次对话这和设计的交互逻辑有偏差，目标是一次唤醒一次交互解决 “ 回声 ” 问题。



##### 关检测唤醒集成：

尝试将 snowboy 关键词唤醒项目集成进 ROS2 功能包中，作为语音助手唤醒的新机制代替 sherpa-onnx 的关键词唤醒。



### 10日：

##### 烟雾检测模型训练：

服务器上使用火焰 + 烟雾的数据集进行新模型训练，昨天训练的模型火焰检测精度 0.715 ，烟雾检测精度 0.829 。数据集是 smoke-fire2，训练结果如下：

```
Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 125/125 [00:34<00:00,  3.62it/s]
                   all       2000       4312      0.846      0.704      0.784      0.489
                 smoke       1895       3627      0.897      0.804      0.871      0.581
                  fire        462        685      0.796      0.604      0.696      0.397
```



##### 关键词唤醒集成：

成功将关键词唤醒逻辑进行修改并测试完成，目前解决了一次唤醒会输出两次关键词的问题。	

- 出现词表中关键词都可以触发语音助手的异常情况

  排查关键词检测代码发现是在触发代码上没有加检测机制，导致一旦检测到关键词就会触发语音助手，代码修改如下：	

  ```python
  def audio_callback(self, msg):
      # 只送入新数据
      self.stream.accept_waveform(16000, np.array(msg.data, dtype=np.float32))
  
      # 尝试解码
      while self.keyword_spotter.is_ready(self.stream):
          self.keyword_spotter.decode_streams([self.stream])
  
      # 获取结果
      result = self.keyword_spotter.get_result(self.stream)
      if result and result == "小明你好":          # 只响应这一句
          self.get_logger().info(f"{result} is detected.")
          # 重置流，防止重复触发
          self.stream = self.keyword_spotter.create_stream()
          # 启动助手
          self._start_assistant_script()         # 其它关键词直接丢弃，不做任何处理
  ```

  

##### 语音助手优化：

运行第一次执行语音助手功能时是正常的，但是后面会出现交互逻辑错误体现在本义是当用户结束语音输入后3秒将用户问题传给服务器上的智能体进行回答然后接收回答然后经由Piper TTS 进行文字转语音服务然后退出当前程序。现在程序不会正常终止并且还会一直识别语音。

- 问题分析：

  1. 主线程 `while not callback.should_exit:` 是唯一决定进程生死的循环。
  2. `_send_collected_data()` 里 TTS 播放成功后把 `self.should_exit = True` 设为 True。
  3. 但 TTS 播放是同步阻塞的（`subprocess.run(...)` 直到播放完才返回），
     而 `_send_collected_data()` 又是在 子线程（`_process_loop`）里跑的。
     于是出现：
     - 子线程：播放完 → `should_exit = True` → 线程函数结束。
     - 主线程：还在 `while not callback.should_exit:` 里读麦克风，完全不知道标志已被改掉，继续循环。这会导致第一次语音→服务器→TTS 完成后，主线程没退出，麦克风继续读数据；`Callback.on_event` 仍然收到识别结果，继续往 `self.transcriptions` 里追加；3 秒静默又到 → `_process_loop` 再次触发 `_send_collected_data()` → 第二次请求……于是看起来“逻辑错乱”。

  结论：子线程改标志，主线程没机会检查，进程退不出去。

  

- 解决方法：

  只要让 主线程 在播放结束后及时退出即可。最简单的办法：把“播放完就退出”这一动作从子线程挪到主线程，也就是 子线程只负责把最终文字塞进一个队列，主线程播放完立即 `break`，顺便把 `translator` 停掉。代码修改如下：

  ```python
  # 1. 在 Callback.__init__ 里再开一个队列
  self.tts_text_queue = queue.Queue()   # 只放最后一次要读的文字
  
  # 2. 修改 _send_collected_data()
  def _send_collected_data(self):
      """发送收集的语音数据到API"""
      with self.lock:
          if not self.transcriptions:
              return 
          self.is_processing = True		# 设置处理中状态
          all_text = [item["text"].strip() for item in self.transcriptions if item["text"].strip()]		# 获取所有转录文本
          if not all_text:
              self.is_processing = False
              return 
          query = " ".join(all_text)		# 合并所有文本
          if len(query) < 2:
              self.is_processing = False
              return  	# 忽略太短的输入
          print(f"[INFO] 发送到服务器的内容: {query}")
          self.transcriptions = []		# 清空转录数据
  
      # 发送请求到服务器（不在锁内执行，避免阻塞）
      try:
          payload = {
              "inputs": {},
              "query": query,
              "response_mode": "streaming",
              "conversation_id": "",
              "user": "decade",
              "files": [
                  {
                      "type": "image",
                      "transfer_method": "remote_url",
                      "url": "https://cloud.dify.ai/logo/logo-site.png"
                  }
              ]
          }
  
          # 启用流式请求
          response = requests.post(
              url="http://192.168.1.28/v1/chat-messages",
              headers=headers,
              json=payload,
              timeout=30,
              stream=True
          )
          print(f"[DEBUG] API 响应状态码：{response.status_code}")
  
          if response.status_code != 200:
              print(f"[ERROR] API 请求失败: {response.status_code} - {response.text}")
              self.is_processing = False
              return
  
          answer_parts = []  # 存储流式返回的 answer 片段
          for line in response.iter_lines():
              if line:
                  text = line.decode('utf-8')
                  if text.startswith("data: "):
                      try:
                          event = json.loads(text[6:])  # 去除 "data: " 前缀
                          if event.get("event") in ["agent_message", "message"]:
                              ans = event.get("answer", "")
                              if ans:
                                  # 去除<think>标签及其内容
                                  ans = self._remove_think_tags(ans)
                                  answer_parts.append(ans)
                                  print(f"[INFO] 流式回答片段: {ans}")
                          elif event.get("event") == "message_end":
                              final_answer = ''.join(answer_parts)
                              # 最终再次检查并去除可能残留的<think>标签
                              final_answer = self._remove_think_tags(final_answer)
                              print(f"[INFO] 完整回答: {final_answer}")
  
                              # 把最终文字扔给主线程，由主线程播放并退出
                              self.tts_text_queue.put(final_answer)
                      except json.JSONDecodeError as e:
                          print(f"[ERROR] JSON 解析失败: {e}, 原始数据: {text}")
                          continue
      except Exception as e:
          print(f"[ERROR] 请求失败：{e}")
      finally:
          self.is_processing = False
  
  # 3. 修改主循环
  try:
      while not callback.should_exit:
          # 先看有没有要读的文字
          try:
              text_to_speak = callback.tts_text_queue.get_nowait()
              # 主线程里播放
              success = synthesizer.generate_and_play_speech(text_to_speak)
              # 播放完立即退出
              callback.should_exit = True
              break
          except queue.Empty:
              pass
  
          # 正常读麦克风
          if stream and not callback.is_processing:
              data = stream.read(3200, exception_on_overflow=False)
              translator.send_audio_frame(data)
          else:
              time.sleep(0.1)
  except KeyboardInterrupt:
      print("程序已终止")
  ```



### 11日：

##### 烟雾检测模型训练：

服务器上使用火焰 + 烟雾的数据集进行新模型训练，昨天训练的模型火焰检测精度 0.897 ，烟雾检测精度 0.796 。数据集是 smoke-fire2，训练结果如下：

```
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
    110/300      1.38G     0.8645     0.7048      1.087         41        640: 100%|██████████| 839/839 [04:02<00:00,  3.46it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 80/80 [00:22<00:00,  3.58it/s]
                   all       1277       3931      0.564      0.527      0.535      0.219
```



##### 语音助手测试完成：

目前语音助手开发完成并且测试了正常唤醒使用一次问答时间在10s内，另外测试了程序运行在常规环境下会不会被误触的情况，结果显示关键词唤醒表现良好。待优化部分是模型回答内容带有数字时还是有问题不够清晰。



### 13日：

##### 烟雾检测模型训练：

服务器上使用火焰 + 烟雾的数据集进行新模型训练，昨天训练的模型火焰检测精度 0.587 ，烟雾检测精度 0.215 。数据集是 smoke-fire3，训练结果如下：

```
Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 80/80 [00:23<00:00,  3.35it/s]
                   all       1277       3931      0.469      0.401      0.398      0.154
                  fire        944       2121       0.43      0.587       0.51      0.198
                 smoke        980       1810      0.508      0.215      0.285      0.109
```

- YOLO 模型结构简介：Backbone、neck 和 Head。
  - Backbone：模型的主干网络，负责提取图像的特征。YOLOv8的Backbone在结构上进行了重要的优化，比如引入了CSP（Cross Stage Partial networks）设计，它通过部分连接多个网络阶段，减少了计算量的同时保持了特征的丰富性。此外，YOLOv8的Backbone可能还整合了SPP（Spatial Pyramid Pooling）和C2F（Coarse-to-Fine）结构，使模型能够捕获从粗到细的多尺度特征。
  - neck：连接Backbone和Head，它在特征传递过程中起到增强和过滤的作用。YOLOv8可能采用了PANet（Path Aggregation Network）或者BiFPN（Bidirectional Feature Pyramid Network）这样的结构，以促进不同尺度的特征图之间的信息流动，强化了检测器对于不同尺寸目标的检测能力。
  - Head：模型的预测器，负责最终的目标检测任务。它通常包含多个并行的卷积层，用于预测边界框的位置、尺寸和目标的类别。YOLOv8的Head可能通过改进的anchor机制来预测边界框，该机制能够更精准地匹配目标的形状和大小，从而提高了检测的精度。



- 聚类：聚类(Clustering) 是按照某个特定标准(如距离)把一个数据集分割成不同的类或簇，使得**同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大**。也即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。
  - 聚类(Clustering)：是指把相似的数据划分到一起，具体划分的时候并不关心这一类的标签，目标就是把相似的数据聚合到一起，聚类是一种**无监督学习(Unsupervised Learning)**方法。
  - 分类(Classification)：是把不同的数据划分开，其过程是通过训练数据集获得一个分类器，再通过分类器去预测未知数据，分类是一种监督学习(Supervised Learning)方法。



- 噪声：所谓噪音，就是指在原始信号中出现了我们不希望的信号，或者干扰。了解噪音的生成方法，是为了方便我们更好的评估去噪函数。通常，在图像学领域，由于传感器的原因，会出现3种比较常见的噪音。分别是椒盐噪音、高斯噪音以及泊松噪音。现在就来分别了解下这些噪音的产生原因，以及手工实现噪音产生的方法。

  - 椒盐噪声：椒盐噪声(salt-and-pepper noise)又称脉冲噪声，它随机改变一些像素值，在二值图像上表现为使一些像素点变白，一些像素点变黑。 是由图像传感器，传输信道，解码处理等产生的黑白相间的亮暗点噪声，也就是老人们比较熟悉的所谓“雪花”。

  - 高斯噪声：假设某一种有效信号它表示为 μ \mu μ，而随着时间的延续，比方说这个信号是来自某种温度传感器，由于传感器老化或者传输信号出现了某种干扰，这些干扰的信号范围在 [ − 3 σ , 3 σ ] [-3\sigma, 3\sigma] [−3σ,3σ]之间，然后我们把这段时间内噪音的振幅收集并且整理后，发现它符合正态分布曲线，亦或者称为高斯曲线，那么这样的噪音就称为高斯噪音。

    ![](/media/yls/1T硬盘4/picture/高斯噪音.jpeg)

  - 泊松噪声：泊松噪音存在的根本原因是因为光是由离散的光子构成（光的粒子性）。光源发出的光子打在CMOS上，从而形成一个可见的光点。光源每秒发射的光子到达CMOS的越多，则该像素的灰度值越大。但是因为光源发射和CMOS接收之间都有可能存在一些因素导致单个光子并没有被CMOS接收到或者某一时间段内发射的光子特别多，所以这就导致了灰度值会有波动，也就是所谓的散粒噪声。举例而言，在光源强度比较低的时候，比如说设定光强为每秒5个光子的时候，那么每秒实际CMOS接受到的光子数可能从0到10（服从泊松分布）。简单的说就是满足泊松分布的噪音，你会觉得它和正态分布很相似，其实如果我们采集的数据越多，精度越密，其形态上它越发接近高斯分布函数，也就是正态分布，是常见的一种满足指数函数分布的离散模型。



##### 烟雾火焰检测算法：

目前想的是在火焰检测模型和烟雾检测模型混合检测，当火焰检测模型检测的置信度高于 0.5 时视为可能发生火情就通过话题发布着火信息给主控；当烟雾检测模型检测的置信度高于 0.7 时视为可能发生火情就通过话题发布着火信息给主控；最后一种情况当两个模型检测的置信度都不高时采用混合检测的方式执行，首先火焰检测模型检测阈值设为 0.3 低于 0.3 视为误检，同理烟雾检测模型阈值设为 0.5 ，当火焰检测模型 置信度在 0.3 至 0.5 之间同时烟雾检测模型置信度在 0.5 至 0.7 之间时视为有火情发生发布信息给主控。



### 14日：

##### 烟雾检测模型训练：

还有一个训练集，准备训练结束挑选表现最好的两个模型进行算法开发，融合模型和分开检测的模型均尝试检测效果最后选择效果好的用。以下是昨天训练的火焰-烟雾检测模型数据，数据集采用的 smoke-fire4 ：

```
Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 106/106 [00:27<00:00,  3.89it/s]
                   all       1681       3926      0.806      0.788       0.85      0.585
                  fire       1202       1777      0.843      0.823      0.896      0.644
                 other        244        399      0.703      0.699      0.743      0.474
                 smoke       1477       1750      0.873      0.842       0.91      0.635
```



smoke-fire5 数据集训练结果如下：

```
Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:06<00:00,  3.73it/s]
                   all        387        748      0.873      0.859        0.9       0.67
                  fire        342        586      0.857      0.787      0.848      0.535
                 smoke        115        162      0.888      0.932      0.952      0.805
```



##### 烟雾火焰检测模型功能包开发：

完成功能包以及测试模型转化，目前有一个综合模型检测火焰表现良好但是不能检测烟雾正在排查原因。可能是某个驱动原因，因为之前的猫狗多种类识别也出现相同情况。



### 15日：

##### 语音助手开发文档书写：

完成整个项目的开发文档书写，目前在测试机器人控制部分，等所有功能完成后再进行对应的文档补全和源码上传。



##### 语音控制机器人开发：

目前可以控制机器人进行前景和后退操作但是延迟较高，后面要考虑实现具体的功能有哪些。



##### 烟雾火焰检测模型功能包开发：

新 bug 现在模型识别出问题只能识别一类目标，考虑使用两个模型进行混合识别提升识别精度。



### 16日：

##### 开发板功能包测试：

这是 OpenCV 的 GUI 模块（cv2.imshow 等） 试图使用 Qt 插件失败，通常是因为用的是 `opencv-python-headless`（无 GUI 支持），但代码里调用了 `cv2.imshow` 或其他 GUI 函数；或者 Qt 插件路径不对，缺少 `libqxcb.so`。

```bash
qt.qpa.plugin: Could not find the Qt platform plugin "xcb" in "/home/ubt/.local/lib/python3.10/site-packages/cv2/qt/plugins"
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

[ros2run]: Aborted
```

- 解决办法：卸载 opencv 包然后重新下载基本可以解决。

  ```bash
  pip uninstall opencv-python
  pip install opencv-python==4.10.0.84 -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

  

##### 开发板模型版本修复：

模型是用 RKNN Toolkit 2.3.0 导出的；但运行环境（RKNN Runtime）是 2.0.0b0，版本不匹配；虽然日志说“init succeed”，但可能存在兼容性风险，比如推理结果异常或崩溃。

```bash
I RKNN: [09:45:22.516] RKNN Runtime Information, librknnrt version: 2.0.0b0 (35a6907d79@2024-03-24T10:31:14)
I RKNN: [09:45:22.516] RKNN Driver Information, version: 0.9.8
I RKNN: [09:45:22.517] RKNN Model Information, version: 6, toolkit version: 2.3.0(compiler version: 2.3.0 (c949ad889d@2024-11-07T11:39:30)), target: RKNPU v2, target platform: rk3588, framework name: ONNX, framework layout: NCHW, model inference type: static_shape
W RKNN: [09:45:22.517] RKNN Model version: 2.3.0 not match with rknn runtime version: 2.0.0
W RKNN: [09:45:22.525] query RKNN_QUERY_INPUT_DYNAMIC_RANGE error, rknn model is static shape type, please export rknn with dynamic_shapes
W Query dynamic range failed. Ret code: RKNN_ERR_MODEL_INVALID. (If it is a static shape RKNN model, please ignore the above warning message.)
```

- 解决办法：升级板端 RKNN Runtime 到 2.3.0

  - 从 [Rockchip 官方 GitHub](https://github.com/rockchip-linux/rknn-toolkit2) 下载对应版本的 `librknnrt.so`；

  - 替换 `/usr/lib/librknnrt.so` 或 `/lib/librknnrt.so`；

  - 或者重新刷一个包含 RKNN Runtime 2.3.0 的固件。

    ```
    # /home/orangepi/zipformer librknnrt.so 本地位置
    ```

    

##### 火情监测功能包开发：

目前选取了效果最好的火焰和烟雾检测模型测试了单个模型的检测效果没有问题，烟雾检测模型会出现误检情况。  



### 19日：

##### 火情监测功能包开发：

目前构建了功能包整合和模型以及模型推理代码，正在测试选取效果好的参数让混合检测的效果最优。



##### 陌生人检测功能包开发：

检测逻辑是依据拍到的人的人脸特征与预录入的特征图进行对比如果发现该人脸特征未录入则发出警报，此时通过 ROS2 话题发布陌生人闯入。这里要加入计数以及截图分别通过 ROS2 话题发布人数以及将截图存入本地的方式记录。

- 项目难点：目前推测人脸检测需要人正对摄像头，如果这项功能是常开的话可能出现大规模的误检。



##### 功能包整合：

完成血液检测功能包集成并放在新开发板上测试，需要敲定血液检测截图存放位置，暂时放在测试功能包同级目录 blood_detect 下。



##### 开发板语音包问题检测：

排除代码以及驱动问题，重新安装了 rknn 工具链但报错如下：

```bash
# 错误一
W rknn-toolkit-lite2 version: 2.3.0
2025-10-19 17:16:37.757 | INFO     | robot_speech_recognition.rknn_infer.rknn_utils:load_rknn:21 - load rknn model succeed
I RKNN: [17:16:37.833] RKNN Runtime Information, librknnrt version: 2.0.0b0 (35a6907d79@2024-03-24T10:31:14)
I RKNN: [17:16:37.833] RKNN Driver Information, version: 0.9.8
I RKNN: [17:16:37.836] RKNN Model Information, version: 6, toolkit version: 2.3.0(compiler version: 2.3.0 (c949ad889d@2024-11-07T11:39:30)), target: RKNPU v2, target platform: rk3588, framework name: ONNX, framework layout: NCHW, model inference type: static_shape
W RKNN: [17:16:37.837] RKNN Model version: 2.3.0 not match with rknn runtime version: 2.0.0
W RKNN: [17:16:37.944] query RKNN_QUERY_INPUT_DYNAMIC_RANGE error, rknn model is static shape type, please export rknn with dynamic_shapes
W Query dynamic range failed. Ret code: RKNN_ERR_MODEL_INVALID. (If it is a static shape RKNN model, please ignore the above warning message.)
2025-10-19 17:16:37.944 | INFO     | robot_speech_recognition.rknn_infer.rknn_utils:load_rknn:36 - init rknn model succeed
W rknn-toolkit-lite2 version: 2.3.0
2025-10-19 17:16:37.953 | INFO     | robot_speech_recognition.rknn_infer.rknn_utils:load_rknn:21 - load rknn model succeed
I RKNN: [17:16:37.999] RKNN Runtime Information, librknnrt version: 2.0.0b0 (35a6907d79@2024-03-24T10:31:14)
I RKNN: [17:16:37.999] RKNN Driver Information, version: 0.9.8
I RKNN: [17:16:37.999] RKNN Model Information, version: 6, toolkit version: 2.3.0(compiler version: 2.3.0 (c949ad889d@2024-11-07T11:39:30)), target: RKNPU v2, target platform: rk3588, framework name: ONNX, framework layout: NCHW, model inference type: static_shape
W RKNN: [17:16:37.999] RKNN Model version: 2.3.0 not match with rknn runtime version: 2.0.0
E RKNN: [17:16:38.005] Unsupport CPU op: Clip in this librknnrt.so, please try to register custom op by callingrknn_register_custom_ops or please try updating to the latest version of the toolkit2 and runtime from: https://console.zbox.filez.com/l/I00fc3 (PWD: rknn)
W RKNN: [17:16:38.005] query RKNN_QUERY_INPUT_DYNAMIC_RANGE error, rknn model is static shape type, please export rknn with dynamic_shapes
W Query dynamic range failed. Ret code: RKNN_ERR_MODEL_INVALID. (If it is a static shape RKNN model, please ignore the above warning message.)
2025-10-19 17:16:38.005 | INFO     | robot_speech_recognition.rknn_infer.rknn_utils:load_rknn:36 - init rknn model succeed
W rknn-toolkit-lite2 version: 2.3.0
2025-10-19 17:16:38.014 | INFO     | robot_speech_recognition.rknn_infer.rknn_utils:load_rknn:21 - load rknn model succeed
I RKNN: [17:16:38.061] RKNN Runtime Information, librknnrt version: 2.0.0b0 (35a6907d79@2024-03-24T10:31:14)
I RKNN: [17:16:38.061] RKNN Driver Information, version: 0.9.8
I RKNN: [17:16:38.061] RKNN Model Information, version: 6, toolkit version: 2.3.0(compiler version: 2.3.0 (c949ad889d@2024-11-07T11:39:30)), target: RKNPU v2, target platform: rk3588, framework name: ONNX, framework layout: NCHW, model inference type: static_shape
W RKNN: [17:16:38.061] RKNN Model version: 2.3.0 not match with rknn runtime version: 2.0.0
W RKNN: [17:16:38.065] query RKNN_QUERY_INPUT_DYNAMIC_RANGE error, rknn model is static shape type, please export rknn with dynamic_shapes
W Query dynamic range failed. Ret code: RKNN_ERR_MODEL_INVALID. (If it is a static shape RKNN model, please ignore the above warning message.)
2025-10-19 17:16:38.065 | INFO     | robot_speech_recognition.rknn_infer.rknn_utils:load_rknn:36 - init rknn model succeed
num_frames: 103
E RKNN: [17:16:38.639] Meet unsupported slice
E RKNN: [17:16:38.639] Op type:Slice, name: Slice:/Gather_194_2sl, fallback cpu failed. please try updating to the latest version of the toolkit2 and runtime from: https://console.zbox.filez.com/l/I00fc3 (PWD: rknn)
[ros2run]: Segmentation fault


# 错误二
2025-10-19 17:32:13.901 | ERROR    | robot_speech_recognition.rknn_infer.speech_recognition:recognize:278 - 语音识别发生错误'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/ubt/fp_test/install/robot_speech_recognition/lib/robot_speech_recognition/robot_speech_recognition", line 33, in <module>
    sys.exit(load_entry_point('robot-speech-recognition==0.0.0', 'console_scripts', 'robot_speech_recognition')())
  File "/home/ubt/fp_test/install/robot_speech_recognition/lib/python3.10/site-packages/robot_speech_recognition/robot_speech_recognition.py", line 46, in main
    rclpy.shutdown()
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/__init__.py", line 130, in shutdown
    _shutdown(context=context)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/utilities.py", line 58, in shutdown
    return context.shutdown()
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/context.py", line 102, in shutdown
    self.__context.shutdown()
rclpy._rclpy_pybind11.RCLError: failed to shutdown: rcl_shutdown already called on the given context, at ./src/rcl/init.c:241
[ros2run]: Process exited with failure 1
```



### 20日：

##### 火情监测功能包开发：

开发板测试通过火焰 + 烟雾模型检测的火灾预防功能包测试，目前发现一个问题对闪烁的红色灯光存在误检。



##### 陌生人检测功能包开发：

检测逻辑是依据拍到的人的人脸特征与预录入的特征图进行对比如果发现该人脸特征未录入则发出警报，此时通过 ROS2 话题发布陌生人闯入。这里要加入计数以及截图分别通过 ROS2 话题发布人数以及将截图存入本地的方式记录。

- 项目难点：
  - 目前推测人脸检测需要人正对摄像头，如果这项功能是常开的话可能出现大规模的误检。
  - 外部光线因素以及人脸遮挡导致人脸特征提取不到。
- 检测逻辑：
  - 结合关键词检测技术做到记录合法人脸特征，假设指令内容 "记住我" ，接收到指令后响应并在记录完成后语音提示。



##### 功能包整合：

完成火灾预防功能包集成并放在新开发板上测试。



##### 开发板语音识别问题：

在新开发板上成功测试语音助手，问题出在麦克风音量上。另外新的功能包需要下载额外几个依赖已经注明并放在 61 服务器上。



### 21日：

##### 宠物识别模型优化：

发现功能虽然正常但是如果同时出现猫狗就检测不出来了，可能是配置问题也有可能是模型问题都尝试看能不能优化，另外此功能暂时不与功能包整合。



##### 陌生人检测：

测试发现特征提取的功能在离人稍远的地方就不能做人脸特征提取，导致无法判断是否是陌生人。	



### 22日：

##### 室内积水数据集：

网上开源的数据集很少且需要收费，识别的具体类别也需要确认是识别水渍还是针对大面积失水。



##### 陌生人检测：

基本放弃使用人脸关键点来做人脸识别，因为检测要求苛刻实际引用满足不了。考虑使用其他方法来做人脸识别。



##### 宠物检测模型训练：

服务器上训练了宠物识别的模型并在开发板上完成了测试。



### 23日： 

##### 陌生人检测：

开始构建识别功能包，现在完成了距离 1.5 米左右的人脸采集以及特征处理部分，验证了距离远近对人脸特征采集会造成较大影响实际应用会造成误测。



##### 宠物识别模型训练：

服务器上训练了宠物识别的模型并在开发板上完成了测试。



##### 室内积水数据集：

下载了几个用于训练的数据集在宠物识别的模型训练结束后进行训练。



### 26日：

##### 宠物识别模型优化：

使用YOLO进行宠物识别模型训练，现在的问题是旧板子上对于模型识别类别多的情况无法正常显示



##### 室内积水模型训练：

室内积水检测模型训练, 数据集准备完毕开始训练



##### 陌生人检测功能包开发:

考虑采集同一个人远近两种距离的人脸特征，再进行识别测试 



### 27日：

##### 室内积水检测模型训练：

完成两个室内积水检测模型训练但是效果不太好



##### 室内积水检测功能包创建：

完成功能包创建并放在开发板上测试目前出现的问题是模型检测出来但是话题没有发到的类别



##### 陌生人检测：

测试了远近两种人脸特征的对比发现精度依旧低



### 28日：

##### 水渍检测数据集：

寻找更多的水渍检测数据集，购买了一个付费的数据集已将下载连接转发



##### 水渍检测模型测试：

把昨天训练的模型装入开发好的功能包综合评测现在最好的模型可以做到检测但是检测效率不高。



##### 陌生人检测：

寻找新的解决方案思考怎么升级新的检测算法。



### 29日：

##### 水渍检测模型测试：

处理新数据集训练出来的模型，现在调整识别目标对水渍进行识别，测试过程发现无法识别水浸场景，但是对渗水和泼水检测较为准确。



##### 模型识别问题：

解决了模型只能识别第0类目标的问题，现在模型能够正常识别。



##### 陌生人检测：

使用新的技术方案尝试实现陌生人识别功能包开发，现在进行模型的转化阶段。需要数据集进行I INT8 量化。下面是量化重要性：

- 性能提升：INT8 量化可将模型推理速度提升 2-4 倍

- 内存减少：模型大小减少约 75%

- 功耗降低：NPU 运行 INT8 模型功耗更低

  数据集再量化中的作用：

  1. 校准动态范围：通过真实数据确定每层激活值的动态范围
  2. 减少精度损失：使用代表性数据确保量化后的模型保持较高精度
  3. 适应实际分布：让量化参数匹配您实际应用中的数据分布



### 30日：

##### 水渍检测模型训练：

服务器完成模型训练并放在开发板上测试，现在模型能够检测出水渍且精度较高但是存在比较明显的误测现象。



##### 陌生人检测数据集整理：

整理用于转换模型所需要的数据集，并成功完成转换。



##### 陌生人检测测试：

测试了新的检测方法但是检测不准确还不清楚问题出在哪里。

MobileFaceNet人脸特征提取 arcface识别模型 RetinaFace人脸特征提取



## 十一月

### 2日：

##### YOLO 算法的优缺点：

**优点：**

1. 实时性高：YOLO采用单次前向传递即可完成目标检测，无需复杂的候选区域生成过程，因此具有极高的检测速度。这使得YOLO在需要实时响应的场景中表现出色。
2. 全局感知能力强：YOLO在整个图像上进行全局优化，能够捕捉到物体的全局上下文信息，从而提高检测的准确性和鲁棒性。
3. 易于部署：YOLO算法结构简洁明了，易于理解和实现。同时，由于其高效的计算性能，YOLO可以在移动设备或嵌入式系统上轻松部署。

**缺点：**

1. 小目标检测能力弱：尽管YOLO在整体目标检测方面表现出色，但对于小尺寸或密集排列的目标仍存在挑战。这是因为小目标在图像中的占比较小，难以被准确检测出来。
2. 定位精度略低：相比两阶段的目标检测算法（如Faster R-CNN），YOLO在某些情况下的定位精度可能稍逊一筹。这主要是由于YOLO直接从图像中回归出边界框坐标，而没有经过区域建议网络的精细调整。
3. 训练数据要求高：YOLO需要大量的标注数据才能取得较好的检测效果。这在一定程度上增加了数据准备的难度和成本。



##### 水渍检测模型训练测试：

完成模型训练但是功能包测试容易出现误测，尝试优化训练方法使训练的模型表现更好。



##### 水渍检测：

从网上找了很多数据集但是要不就是针对室外环境要不就是训练结果一般。目前主流的检测室内水浸的方法还时使用水浸检测传感器，如果使用模型检测误差太大。



##### 陌生人检测：

模型转换完毕但是经过验证检测效果一般，准备再尝试多一点的样本检验是不是方法用错了。



### 3日：

##### 水渍检测模型训练：

完成水浸检测功能包开发并将功能包上传至svn



##### 陌生人检测：

模型还是不能正常工作，尝试使用开源项目使用的方法先实现对人脸的检测。



### 4日：

##### 陌生人检测：

调通了一个陌生人检测的开源项目但是一运行就会强制关机，推测时识别模型在CPU上运行不了导致的。尝试使用更轻量级的模型进行检测。使用retinaFace模型进行检测但是检测出来的人脸位置不准确。



### 5日：

##### SSH 断连问题：

```
2025-11-05 10:00:26.450 [error] [窗口] 远程主机密钥已更改，端口转发已禁用: CodeExpectedError: 远程主机密钥已更改，端口转发已禁用
    at Wkt.sb (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3820:15716)
    at async Wkt.rb (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3820:15082)
    at async Wkt.fc (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3826:3611)
    at async vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:82978
```

- 解决办法：如果你知道远程主机的地址（比如 `192.168.1.144`），可以直接运行清除就密码然后重新连接

  ```bash
  ssh-keygen -R 192.168.1.144
  ```




##### 水浸检测开发板测试：

在新开发板上完成新开发板的测试，现在实现了水浸环境监测以及对无水环境的检测。由于功能还有提升空间所以不将此功能包与总功能包集成。



##### 陌生人检测：

尝试使用python的人脸检测模块进行人脸检测，还是需要模型训练在服务器上测试方法可行性。



### 6日：

##### 陌生人检测：

- ONNX 格式 retinaface 模型人脸检测使用 ONNX Runtime进行推理会遇到识别错误的情况，如果使用 pytorch 进行推理需要管理 torch 版本原因如下：

  ```bash
  使用自定义模型: C:/face_detect/model/RetinaFace_mobile320.onnx
  加载自定义模型失败: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
  Please file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error:
  
  Unsupported operand 8
  
  Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
  ```

  

- mediapipe 人脸检测：

  ```bash
  # 创建虚拟环境
  conda create -n meidapipe python=3.8 -y
  
  # 安装包，自动下载模型
  pip install mediapipe
  ```

  - 测试脚本：针对人少的情况可以测试，但是人多的情况会检测不到。考虑设置对低置信度目标不进行关键点提取。

    ```python
    import cv2
    import os
    import sys
    
    def check_dependencies():
        """检查并安装必要的依赖"""
        try:
            import mediapipe as mp
            return mp
        except ImportError as e:
            print(f"导入错误: {e}")
            print("正在尝试安装兼容的 protobuf 版本...")
            os.system("pip install protobuf==3.20.3")
            import mediapipe as mp
            return mp
    
    def mediapipe_face_detection(image_path, output_path="mediapipe_result.jpg"):
        """
        使用 MediaPipe 进行人脸检测
        - 模型大小: ~9.4MB (自动下载)
        - 模型格式: 内置，无需单独管理
        """
        try:
            # 检查依赖
            mp = check_dependencies()
            
            # 初始化 MediaPipe 人脸检测
            mp_face_detection = mp.solutions.face_detection
            mp_drawing = mp.solutions.drawing_utils
            
            # 读取图像
            image = cv2.imread(image_path)
            if image is None:
                print(f"错误：无法从 {image_path} 读取图像")
                return
            
            print("使用 MediaPipe 人脸检测...")
            
            # 转换为 RGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:
                # 进行检测
                results = face_detection.process(image_rgb)
                
                if results.detections:
                    for detection in results.detections:
                        # 获取边界框
                        bboxC = detection.location_data.relative_bounding_box
                        h, w, _ = image.shape
                        
                        # 计算实际坐标
                        x1 = int(bboxC.xmin * w)
                        y1 = int(bboxC.ymin * h)
                        x2 = int((bboxC.xmin + bboxC.width) * w)
                        y2 = int((bboxC.ymin + bboxC.height) * h)
                        
                        # 绘制边界框
                        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        
                        # 显示置信度
                        confidence = detection.score[0]
                        label = f"Face: {confidence:.2f}"
                        cv2.putText(image, label, (x1, y1-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                        
                        print(f"检测到人脸: 位置 [{x1}, {y1}, {x2}, {y2}], 置信度: {confidence:.3f}")
                    
                    print(f"MediaPipe 检测到 {len(results.detections)} 个人脸")
                else:
                    print("MediaPipe 未检测到人脸")
            
            # 保存结果
            cv2.imwrite(output_path, image)
            print(f"结果保存为: {output_path}")
            
            return image
            
        except Exception as e:
            print(f"发生错误: {e}")
            print("\n请尝试以下解决方案:")
            print("1. 运行: pip install protobuf==3.20.3")
            print("2. 运行: pip install --upgrade mediapipe")
            print("3. 创建新的虚拟环境")
            return None
    
    # 使用示例
    if __name__ == "__main__":
        image_path = "C:/face_detect/images/peoples.jpg"
        
        # 检查文件是否存在
        if not os.path.exists(image_path):
            print(f"错误：图像文件不存在: {image_path}")
            print("请确保路径正确且文件存在")
        else:
            result = mediapipe_face_detection(image_path)
            if result is not None:
                print("人脸检测完成！")
    ```

    

-  ArcFace 测试人脸检测： 

   

  

##### 表情检测开发板测试： 

测试发现新开发板模型加载没有问题但是接受不到摄像头数据，导致没有检测结果。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            



### 9日：

##### 陌生人检测：

尝试整合人脸检测以及识别的demo，但是遇到效果不佳小模型检测的结果显示不正确。表现为使用模型检测的结果显示检测到太多虚假人脸调整代码有效减少但是检测结果依旧不正确。



##### 语音助手文档书写上传：

增加机器人控制相关的功能介绍和开发文档。



### 10日：

##### 语音助手优化：

尝试强制退出功能，具体是针对关键词误触模型在回答过程中强制退出的方式。

1. 一次唤醒后持续对话，直到
   a) 用户说出特定结束关键词（如“退出助手”），或
   b) 静默超时 60 s 自动结束；
2. 在任何时刻（包括 TTS 播放回答时）说出结束关键词可立即强制退出；
3. 回答阶段 TTS 的声音不会被重新采集、也不会被当成新 query 发回模型。



##### 陌生人检测：

排查为什么陌生人检测只进行了人脸检测即检测有多少个人脸但是并没有进行陌生人检测功能。



### 11日：

##### 陌生人检测：

已经实现了基于 retinaface 库、mediaPipe库人脸检测但是前者使用的默认模型 100MB 开发板可能不支持后者检测效果一般， 并且库默认使用 CPU 进行推理。尝试使用本地模型并用  GPU 加速推理。另外可以调通 arcface 对基于人脸数据库的陌生人脸检测脚本，如果 retinaface 检测调试不通过尝试使用 yolo 实现。



### 12日：

##### 语音助手提示词修改：

提示词已修改为小伴小伴并且已完成新开发板的测试。



##### 陌生人检测：

尝试使用mediaPipe或者yolo模型检测人脸过滤高置信度目标，下一步进行人脸识别功能开发目前问题出在距离远就检测不了的问题。



### 13日：

##### 家庭火灾逃生指南：

完成逃生指南语音生成并将语音上传至svn。



##### 陌生人检测：

训练了人脸检测的YOLO模型在开发板测试发现人站在离摄像头2m的位置置信度达到0.8以上达到检测要求且模型的大小合适可以使用NPU加速推理，考虑对置信度达到0.8以上的目标做陌生人检测。



### 16日：

##### 人脸检测：

训练最后一个YOLO人脸检测模型，目前有两种检测方法均可做到两米左右对人脸进行检测满足陌生人检测的基本条件。

- 模型测试问题：

  ```shell
  yolo predict model=C:\yolov8-face\yolov8n-face-lindevs.pt source=C:\ultralytics\images\face
  Traceback (most recent call last):
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\runpy.py", line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\runpy.py", line 87, in _run_code
      exec(code, run_globals)
    File "C:\Users\fp\miniconda3\envs\YOLO\Scripts\yolo.exe\__main__.py", line 7, in <module>
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\ultralytics\cfg\__init__.py", line 956, in entrypoint
      model = YOLO(model, task=task)
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\ultralytics\models\yolo\model.py", line 79, in __init__        
      super().__init__(model=model, task=task, verbose=verbose)
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\ultralytics\engine\model.py", line 151, in __init__
      self._load(model, task=task)
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\ultralytics\engine\model.py", line 295, in _load
      self.model, self.ckpt = attempt_load_one_weight(weights)
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\ultralytics\nn\tasks.py", line 1549, in attempt_load_one_weight
      ckpt, weight = torch_safe_load(weight)  # load ckpt
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\ultralytics\nn\tasks.py", line 1447, in torch_safe_load
      ckpt = torch_load(file, map_location="cpu")
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\ultralytics\utils\patches.py", line 118, in torch_load
      return torch.load(*args, **kwargs)
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\torch\serialization.py", line 1028, in load
      return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
    File "C:\Users\fp\miniconda3\envs\YOLO\lib\site-packages\torch\serialization.py", line 1246, in _legacy_load
      magic_number = pickle_module.load(f, **pickle_load_args)
  EOFError: Ran out of input
  ```

  - 解决办法：这个 `EOFError: Ran out of input` 错误表明你的模型文件 **损坏或不完整**。这通常发生在模型文件下载中断或没有正确保存的情况下。

  - 问题原因

    文件 `C:\yolov8-face\yolov8n-face-lindevs.pt` 可能：

    - 下载不完整（文件大小为0KB或远小于正常值）

    - 文件内容损坏

    - 文件为空

      

##### 陌生人检测：

构建陌生人检测测试代码，目前遇到本地模型加载不成功的问题或者显示模型结构不正确。需要使用inightface默认模型但是下载速度太慢尝试修改代码实现加载本地模型进行推理。

- 显示模型结构不正确具体日志如下：

  ```bash
  正在加载模型...
    加载 state_dict，初始化 MobileFaceNet...
  Traceback (most recent call last):
    File "C:\face_detect\face_detect_test.py", line 79, in _load_arcface_model
      arcface_model.load_state_dict(model)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\torch\nn\modules\module.py", line 2629, in load_state_dict
      raise RuntimeError(
  RuntimeError: Error(s) in loading state_dict for MobileFaceNet:
          Missing key(s) in state_dict: "conv1.weight", "bn1.weight", "bn1.bias", "bn1.running_mean", "bn1.running_var", "conv2.weight", "bn2.weight", "bn2.bias", "bn2.running_mean", "bn2.running_var", "conv3.weight", "bn3.weight", "bn3.bias", "bn3.running_mean", "bn3.running_var", "fc.weight", "fc.bias", "features.weight", "features.bias", "features.running_mean", "features.running_var".
          Unexpected key(s) in state_dict: "arcface.conv1.conv.weight", "arcface.conv1.bn.weight", "arcface.conv1.bn.bias", "arcface.conv1.bn.running_mean", "arcface.conv1.bn.running_var", "arcface.conv1.bn.num_batches_tracked", "arcface.conv1.prelu.weight", "arcface.conv2_dw.conv.weight", "arcface.conv2_dw.bn.weight", "arcface.conv2_dw.bn.bias", "arcface.conv2_dw.bn.running_mean", "arcface.conv2_dw.bn.running_var", "arcface.conv2_dw.bn.num_batches_tracked", "arcface.conv2_dw.prelu.weight", "arcface.conv_23.conv.conv.weight", "arcface.conv_23.conv.bn.weight", "arcface.conv_23.conv.bn.bias", "arcface.conv_23.conv.bn.running_mean", "arcface.conv_23.conv.bn.running_var", "arcface.conv_23.conv.bn.num_batches_tracked", "arcface.conv_23.conv.prelu.weight", "arcface.conv_23.conv_dw.conv.weight", "arcface.conv_23.conv_dw.bn.weight", "arcface.conv_23.conv_dw.bn.bias", "arcface.conv_23.conv_dw.bn.running_mean", "arcface.conv_23.conv_dw.bn.running_var", "arcface.conv_23.conv_dw.bn.num_batches_tracked", "arcface.conv_23.conv_dw.prelu.weight", "arcface.conv_23.project.conv.weight", "arcface.conv_23.project.bn.weight", "arcface.conv_23.project.bn.bias", "arcface.conv_23.project.bn.running_mean", "arcface.conv_23.project.bn.running_var", "arcface.conv_23.project.bn.num_batches_tracked", "arcface.conv_3.model.0.conv.conv.weight", "arcface.conv_3.model.0.conv.bn.weight", "arcface.conv_3.model.0.conv.bn.bias", "arcface.conv_3.model.0.conv.bn.running_mean", "arcface.conv_3.model.0.conv.bn.running_var", "arcface.conv_3.model.0.conv.bn.num_batches_tracked", "arcface.conv_3.model.0.conv.prelu.weight", "arcface.conv_3.model.0.conv_dw.conv.weight", "arcface.conv_3.model.0.conv_dw.bn.weight", "arcface.conv_3.model.0.conv_dw.bn.bias", "arcface.conv_3.model.0.conv_dw.bn.running_mean", "arcface.conv_3.model.0.conv_dw.bn.running_var", "arcface.conv_3.model.0.conv_dw.bn.num_batches_tracked", "arcface.conv_3.model.0.conv_dw.prelu.weight", "arcface.conv_3.model.0.project.conv.weight", "arcface.conv_3.model.0.project.bn.weight", "arcface.conv_3.model.0.project.bn.bias", "arcface.conv_3.model.0.project.bn.running_mean", "arcface.conv_3.model.0.project.bn.running_var", "arcface.conv_3.model.0.project.bn.num_batches_tracked", "arcface.conv_3.model.1.conv.conv.weight", "arcface.conv_3.model.1.conv.bn.weight", "arcface.conv_3.model.1.conv.bn.bias", "arcface.conv_3.model.1.conv.bn.running_mean", "arcface.conv_3.model.1.conv.bn.running_var", "arcface.conv_3.model.1.conv.bn.num_batches_tracked", "arcface.conv_3.model.1.conv.prelu.weight", "arcface.conv_3.model.1.conv_dw.conv.weight", "arcface.conv_3.model.1.conv_dw.bn.weight", "arcface.conv_3.model.1.conv_dw.bn.bias", "arcface.conv_3.model.1.conv_dw.bn.running_mean", "arcface.conv_3.model.1.conv_dw.bn.running_var", "arcface.conv_3.model.1.conv_dw.bn.num_batches_tracked", "arcface.conv_3.model.1.conv_dw.prelu.weight", "arcface.conv_3.model.1.project.conv.weight", "arcface.conv_3.model.1.project.bn.weight", "arcface.conv_3.model.1.project.bn.bias", "arcface.conv_3.model.1.project.bn.running_mean", "arcface.conv_3.model.1.project.bn.running_var", "arcface.conv_3.model.1.project.bn.num_batches_tracked", "arcface.conv_3.model.2.conv.conv.weight", "arcface.conv_3.model.2.conv.bn.weight", "arcface.conv_3.model.2.conv.bn.bias", "arcface.conv_3.model.2.conv.bn.running_mean", "arcface.conv_3.model.2.conv.bn.running_var", "arcface.conv_3.model.2.conv.bn.num_batches_tracked", "arcface.conv_3.model.2.conv.prelu.weight", "arcface.conv_3.model.2.conv_dw.conv.weight", "arcface.conv_3.model.2.conv_dw.bn.weight", "arcface.conv_3.model.2.conv_dw.bn.bias", "arcface.conv_3.model.2.conv_dw.bn.running_mean", "arcface.conv_3.model.2.conv_dw.bn.running_var", "arcface.conv_3.model.2.conv_dw.bn.num_batches_tracked", "arcface.conv_3.model.2.conv_dw.prelu.weight", "arcface.conv_3.model.2.project.conv.weight", "arcface.conv_3.model.2.project.bn.weight", "arcface.conv_3.model.2.project.bn.bias", "arcface.conv_3.model.2.project.bn.running_mean", "arcface.conv_3.model.2.project.bn.running_var", "arcface.conv_3.model.2.project.bn.num_batches_tracked", "arcface.conv_3.model.3.conv.conv.weight", "arcface.conv_3.model.3.conv.bn.weight", "arcface.conv_3.model.3.conv.bn.bias", "arcface.conv_3.model.3.conv.bn.running_mean", "arcface.conv_3.model.3.conv.bn.running_var", "arcface.conv_3.model.3.conv.bn.num_batches_tracked", "arcface.conv_3.model.3.conv.prelu.weight", 
  "arcface.conv_3.model.3.conv_dw.conv.weight", "arcface.conv_3.model.3.conv_dw.bn.weight", "arcface.conv_3.model.3.conv_dw.bn.bias", "arcface.conv_3.model.3.conv_dw.bn.running_mean", "arcface.conv_3.model.3.conv_dw.bn.running_var", "arcface.conv_3.model.3.conv_dw.bn.num_batches_tracked", "arcface.conv_3.model.3.conv_dw.prelu.weight", "arcface.conv_3.model.3.project.conv.weight", "arcface.conv_3.model.3.project.bn.weight", "arcface.conv_3.model.3.project.bn.bias", "arcface.conv_3.model.3.project.bn.running_mean", "arcface.conv_3.model.3.project.bn.running_var", "arcface.conv_3.model.3.project.bn.num_batches_tracked", "arcface.conv_34.conv.conv.weight", "arcface.conv_34.conv.bn.weight", "arcface.conv_34.conv.bn.bias", "arcface.conv_34.conv.bn.running_mean", "arcface.conv_34.conv.bn.running_var", "arcface.conv_34.conv.bn.num_batches_tracked", "arcface.conv_34.conv.prelu.weight", "arcface.conv_34.conv_dw.conv.weight", "arcface.conv_34.conv_dw.bn.weight", "arcface.conv_34.conv_dw.bn.bias", "arcface.conv_34.conv_dw.bn.running_mean", "arcface.conv_34.conv_dw.bn.running_var", "arcface.conv_34.conv_dw.bn.num_batches_tracked", "arcface.conv_34.conv_dw.prelu.weight", "arcface.conv_34.project.conv.weight", "arcface.conv_34.project.bn.weight", "arcface.conv_34.project.bn.bias", "arcface.conv_34.project.bn.running_mean", "arcface.conv_34.project.bn.running_var", "arcface.conv_34.project.bn.num_batches_tracked", "arcface.conv_4.model.0.conv.conv.weight", "arcface.conv_4.model.0.conv.bn.weight", "arcface.conv_4.model.0.conv.bn.bias", "arcface.conv_4.model.0.conv.bn.running_mean", "arcface.conv_4.model.0.conv.bn.running_var", "arcface.conv_4.model.0.conv.bn.num_batches_tracked", "arcface.conv_4.model.0.conv.prelu.weight", "arcface.conv_4.model.0.conv_dw.conv.weight", "arcface.conv_4.model.0.conv_dw.bn.weight", "arcface.conv_4.model.0.conv_dw.bn.bias", "arcface.conv_4.model.0.conv_dw.bn.running_mean", "arcface.conv_4.model.0.conv_dw.bn.running_var", "arcface.conv_4.model.0.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.0.conv_dw.prelu.weight", "arcface.conv_4.model.0.project.conv.weight", "arcface.conv_4.model.0.project.bn.weight", "arcface.conv_4.model.0.project.bn.bias", "arcface.conv_4.model.0.project.bn.running_mean", "arcface.conv_4.model.0.project.bn.running_var", "arcface.conv_4.model.0.project.bn.num_batches_tracked", "arcface.conv_4.model.1.conv.conv.weight", "arcface.conv_4.model.1.conv.bn.weight", "arcface.conv_4.model.1.conv.bn.bias", "arcface.conv_4.model.1.conv.bn.running_mean", "arcface.conv_4.model.1.conv.bn.running_var", "arcface.conv_4.model.1.conv.bn.num_batches_tracked", "arcface.conv_4.model.1.conv.prelu.weight", "arcface.conv_4.model.1.conv_dw.conv.weight", "arcface.conv_4.model.1.conv_dw.bn.weight", "arcface.conv_4.model.1.conv_dw.bn.bias", "arcface.conv_4.model.1.conv_dw.bn.running_mean", "arcface.conv_4.model.1.conv_dw.bn.running_var", "arcface.conv_4.model.1.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.1.conv_dw.prelu.weight", "arcface.conv_4.model.1.project.conv.weight", "arcface.conv_4.model.1.project.bn.weight", "arcface.conv_4.model.1.project.bn.bias", "arcface.conv_4.model.1.project.bn.running_mean", "arcface.conv_4.model.1.project.bn.running_var", "arcface.conv_4.model.1.project.bn.num_batches_tracked", "arcface.conv_4.model.2.conv.conv.weight", "arcface.conv_4.model.2.conv.bn.weight", "arcface.conv_4.model.2.conv.bn.bias", "arcface.conv_4.model.2.conv.bn.running_mean", "arcface.conv_4.model.2.conv.bn.running_var", "arcface.conv_4.model.2.conv.bn.num_batches_tracked", "arcface.conv_4.model.2.conv.prelu.weight", "arcface.conv_4.model.2.conv_dw.conv.weight", "arcface.conv_4.model.2.conv_dw.bn.weight", "arcface.conv_4.model.2.conv_dw.bn.bias", "arcface.conv_4.model.2.conv_dw.bn.running_mean", "arcface.conv_4.model.2.conv_dw.bn.running_var", "arcface.conv_4.model.2.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.2.conv_dw.prelu.weight", "arcface.conv_4.model.2.project.conv.weight", "arcface.conv_4.model.2.project.bn.weight", "arcface.conv_4.model.2.project.bn.bias", "arcface.conv_4.model.2.project.bn.running_mean", "arcface.conv_4.model.2.project.bn.running_var", "arcface.conv_4.model.2.project.bn.num_batches_tracked", "arcface.conv_4.model.3.conv.conv.weight", "arcface.conv_4.model.3.conv.bn.weight", "arcface.conv_4.model.3.conv.bn.bias", "arcface.conv_4.model.3.conv.bn.running_mean", "arcface.conv_4.model.3.conv.bn.running_var", "arcface.conv_4.model.3.conv.bn.num_batches_tracked", "arcface.conv_4.model.3.conv.prelu.weight", "arcface.conv_4.model.3.conv_dw.conv.weight", "arcface.conv_4.model.3.conv_dw.bn.weight", "arcface.conv_4.model.3.conv_dw.bn.bias", "arcface.conv_4.model.3.conv_dw.bn.running_mean", "arcface.conv_4.model.3.conv_dw.bn.running_var", "arcface.conv_4.model.3.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.3.conv_dw.prelu.weight", "arcface.conv_4.model.3.project.conv.weight", "arcface.conv_4.model.3.project.bn.weight", "arcface.conv_4.model.3.project.bn.bias", "arcface.conv_4.model.3.project.bn.running_mean", "arcface.conv_4.model.3.project.bn.running_var", "arcface.conv_4.model.3.project.bn.num_batches_tracked", "arcface.conv_4.model.4.conv.conv.weight", "arcface.conv_4.model.4.conv.bn.weight", "arcface.conv_4.model.4.conv.bn.bias", "arcface.conv_4.model.4.conv.bn.running_mean", "arcface.conv_4.model.4.conv.bn.running_var", "arcface.conv_4.model.4.conv.bn.num_batches_tracked", "arcface.conv_4.model.4.conv.prelu.weight", "arcface.conv_4.model.4.conv_dw.conv.weight", "arcface.conv_4.model.4.conv_dw.bn.weight", "arcface.conv_4.model.4.conv_dw.bn.bias", "arcface.conv_4.model.4.conv_dw.bn.running_mean", "arcface.conv_4.model.4.conv_dw.bn.running_var", "arcface.conv_4.model.4.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.4.conv_dw.prelu.weight", "arcface.conv_4.model.4.project.conv.weight", "arcface.conv_4.model.4.project.bn.weight", "arcface.conv_4.model.4.project.bn.bias", "arcface.conv_4.model.4.project.bn.running_mean", "arcface.conv_4.model.4.project.bn.running_var", "arcface.conv_4.model.4.project.bn.num_batches_tracked", "arcface.conv_4.model.5.conv.conv.weight", "arcface.conv_4.model.5.conv.bn.weight", "arcface.conv_4.model.5.conv.bn.bias", "arcface.conv_4.model.5.conv.bn.running_mean", "arcface.conv_4.model.5.conv.bn.running_var", "arcface.conv_4.model.5.conv.bn.num_batches_tracked", "arcface.conv_4.model.5.conv.prelu.weight", "arcface.conv_4.model.5.conv_dw.conv.weight", "arcface.conv_4.model.5.conv_dw.bn.weight", "arcface.conv_4.model.5.conv_dw.bn.bias", "arcface.conv_4.model.5.conv_dw.bn.running_mean", "arcface.conv_4.model.5.conv_dw.bn.running_var", "arcface.conv_4.model.5.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.5.conv_dw.prelu.weight", "arcface.conv_4.model.5.project.conv.weight", "arcface.conv_4.model.5.project.bn.weight", "arcface.conv_4.model.5.project.bn.bias", "arcface.conv_4.model.5.project.bn.running_mean", "arcface.conv_4.model.5.project.bn.running_var", "arcface.conv_4.model.5.project.bn.num_batches_tracked", "arcface.conv_45.conv.conv.weight", "arcface.conv_45.conv.bn.weight", "arcface.conv_45.conv.bn.bias", "arcface.conv_45.conv.bn.running_mean", "arcface.conv_45.conv.bn.running_var", "arcface.conv_45.conv.bn.num_batches_tracked", "arcface.conv_45.conv.prelu.weight", "arcface.conv_45.conv_dw.conv.weight", "arcface.conv_45.conv_dw.bn.weight", "arcface.conv_45.conv_dw.bn.bias", "arcface.conv_45.conv_dw.bn.running_mean", "arcface.conv_45.conv_dw.bn.running_var", "arcface.conv_45.conv_dw.bn.num_batches_tracked", "arcface.conv_45.conv_dw.prelu.weight", "arcface.conv_45.project.conv.weight", "arcface.conv_45.project.bn.weight", "arcface.conv_45.project.bn.bias", "arcface.conv_45.project.bn.running_mean", "arcface.conv_45.project.bn.running_var", "arcface.conv_45.project.bn.num_batches_tracked", "arcface.conv_5.model.0.conv.conv.weight", "arcface.conv_5.model.0.conv.bn.weight", "arcface.conv_5.model.0.conv.bn.bias", "arcface.conv_5.model.0.conv.bn.running_mean", "arcface.conv_5.model.0.conv.bn.running_var", "arcface.conv_5.model.0.conv.bn.num_batches_tracked", "arcface.conv_5.model.0.conv.prelu.weight", "arcface.conv_5.model.0.conv_dw.conv.weight", "arcface.conv_5.model.0.conv_dw.bn.weight", "arcface.conv_5.model.0.conv_dw.bn.bias", "arcface.conv_5.model.0.conv_dw.bn.running_mean", "arcface.conv_5.model.0.conv_dw.bn.running_var", "arcface.conv_5.model.0.conv_dw.bn.num_batches_tracked", "arcface.conv_5.model.0.conv_dw.prelu.weight", "arcface.conv_5.model.0.project.conv.weight", "arcface.conv_5.model.0.project.bn.weight", "arcface.conv_5.model.0.project.bn.bias", "arcface.conv_5.model.0.project.bn.running_mean", "arcface.conv_5.model.0.project.bn.running_var", "arcface.conv_5.model.0.project.bn.num_batches_tracked", "arcface.conv_5.model.1.conv.conv.weight", "arcface.conv_5.model.1.conv.bn.weight", "arcface.conv_5.model.1.conv.bn.bias", "arcface.conv_5.model.1.conv.bn.running_mean", "arcface.conv_5.model.1.conv.bn.running_var", "arcface.conv_5.model.1.conv.bn.num_batches_tracked", "arcface.conv_5.model.1.conv.prelu.weight", "arcface.conv_5.model.1.conv_dw.conv.weight", "arcface.conv_5.model.1.conv_dw.bn.weight", "arcface.conv_5.model.1.conv_dw.bn.bias", "arcface.conv_5.model.1.conv_dw.bn.running_mean", "arcface.conv_5.model.1.conv_dw.bn.running_var", "arcface.conv_5.model.1.conv_dw.bn.num_batches_tracked", "arcface.conv_5.model.1.conv_dw.prelu.weight", "arcface.conv_5.model.1.project.conv.weight", "arcface.conv_5.model.1.project.bn.weight", "arcface.conv_5.model.1.project.bn.bias", "arcface.conv_5.model.1.project.bn.running_mean", "arcface.conv_5.model.1.project.bn.running_var", "arcface.conv_5.model.1.project.bn.num_batches_tracked", "arcface.sep.weight", "arcface.sep_bn.weight", "arcface.sep_bn.bias", "arcface.sep_bn.running_mean", "arcface.sep_bn.running_var", "arcface.sep_bn.num_batches_tracked", "arcface.prelu.weight", "arcface.GDC_dw.weight", "arcface.GDC_bn.weight", "arcface.GDC_bn.bias", "arcface.GDC_bn.running_mean", "arcface.GDC_bn.running_var", "arcface.GDC_bn.num_batches_tracked", "arcface.features.weight", "arcface.last_bn.weight", "arcface.last_bn.bias", "arcface.last_bn.running_mean", "arcface.last_bn.running_var", "arcface.last_bn.num_batches_tracked".
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\face_detect\face_detect_test.py", line 381, in <module>
      result_img, detections = process_image(
    File "C:\face_detect\face_detect_test.py", line 279, in process_image
      system = FaceRecognitionSystem(
    File "C:\face_detect\face_detect_test.py", line 56, in __init__
      self.arcface_model = self._load_arcface_model(arcface_model_path)
    File "C:\face_detect\face_detect_test.py", line 96, in _load_arcface_model
      raise Exception(f"ArcFace模型加载失败: {e}")
  Exception: ArcFace模型加载失败: Error(s) in loading state_dict for MobileFaceNet:
          Missing key(s) in state_dict: "conv1.weight", "bn1.weight", "bn1.bias", "bn1.running_mean", "bn1.running_var", "conv2.weight", "bn2.weight", "bn2.bias", "bn2.running_mean", "bn2.running_var", "conv3.weight", "bn3.weight", "bn3.bias", "bn3.running_mean", "bn3.running_var", "fc.weight", "fc.bias", "features.weight", "features.bias", "features.running_mean", "features.running_var".
          Unexpected key(s) in state_dict: "arcface.conv1.conv.weight", "arcface.conv1.bn.weight", "arcface.conv1.bn.bias", "arcface.conv1.bn.running_mean", "arcface.conv1.bn.running_var", "arcface.conv1.bn.num_batches_tracked", "arcface.conv1.prelu.weight", "arcface.conv2_dw.conv.weight", "arcface.conv2_dw.bn.weight", "arcface.conv2_dw.bn.bias", "arcface.conv2_dw.bn.running_mean", "arcface.conv2_dw.bn.running_var", "arcface.conv2_dw.bn.num_batches_tracked", "arcface.conv2_dw.prelu.weight", "arcface.conv_23.conv.conv.weight", "arcface.conv_23.conv.bn.weight", "arcface.conv_23.conv.bn.bias", "arcface.conv_23.conv.bn.running_mean", "arcface.conv_23.conv.bn.running_var", "arcface.conv_23.conv.bn.num_batches_tracked", "arcface.conv_23.conv.prelu.weight", "arcface.conv_23.conv_dw.conv.weight", "arcface.conv_23.conv_dw.bn.weight", "arcface.conv_23.conv_dw.bn.bias", "arcface.conv_23.conv_dw.bn.running_mean", "arcface.conv_23.conv_dw.bn.running_var", "arcface.conv_23.conv_dw.bn.num_batches_tracked", "arcface.conv_23.conv_dw.prelu.weight", "arcface.conv_23.project.conv.weight", "arcface.conv_23.project.bn.weight", "arcface.conv_23.project.bn.bias", "arcface.conv_23.project.bn.running_mean", "arcface.conv_23.project.bn.running_var", "arcface.conv_23.project.bn.num_batches_tracked", "arcface.conv_3.model.0.conv.conv.weight", "arcface.conv_3.model.0.conv.bn.weight", "arcface.conv_3.model.0.conv.bn.bias", "arcface.conv_3.model.0.conv.bn.running_mean", "arcface.conv_3.model.0.conv.bn.running_var", "arcface.conv_3.model.0.conv.bn.num_batches_tracked", "arcface.conv_3.model.0.conv.prelu.weight", "arcface.conv_3.model.0.conv_dw.conv.weight", "arcface.conv_3.model.0.conv_dw.bn.weight", "arcface.conv_3.model.0.conv_dw.bn.bias", "arcface.conv_3.model.0.conv_dw.bn.running_mean", "arcface.conv_3.model.0.conv_dw.bn.running_var", "arcface.conv_3.model.0.conv_dw.bn.num_batches_tracked", "arcface.conv_3.model.0.conv_dw.prelu.weight", "arcface.conv_3.model.0.project.conv.weight", "arcface.conv_3.model.0.project.bn.weight", "arcface.conv_3.model.0.project.bn.bias", "arcface.conv_3.model.0.project.bn.running_mean", "arcface.conv_3.model.0.project.bn.running_var", "arcface.conv_3.model.0.project.bn.num_batches_tracked", "arcface.conv_3.model.1.conv.conv.weight", "arcface.conv_3.model.1.conv.bn.weight", "arcface.conv_3.model.1.conv.bn.bias", "arcface.conv_3.model.1.conv.bn.running_mean", "arcface.conv_3.model.1.conv.bn.running_var", "arcface.conv_3.model.1.conv.bn.num_batches_tracked", "arcface.conv_3.model.1.conv.prelu.weight", "arcface.conv_3.model.1.conv_dw.conv.weight", "arcface.conv_3.model.1.conv_dw.bn.weight", "arcface.conv_3.model.1.conv_dw.bn.bias", "arcface.conv_3.model.1.conv_dw.bn.running_mean", "arcface.conv_3.model.1.conv_dw.bn.running_var", "arcface.conv_3.model.1.conv_dw.bn.num_batches_tracked", "arcface.conv_3.model.1.conv_dw.prelu.weight", "arcface.conv_3.model.1.project.conv.weight", "arcface.conv_3.model.1.project.bn.weight", "arcface.conv_3.model.1.project.bn.bias", "arcface.conv_3.model.1.project.bn.running_mean", "arcface.conv_3.model.1.project.bn.running_var", "arcface.conv_3.model.1.project.bn.num_batches_tracked", "arcface.conv_3.model.2.conv.conv.weight", "arcface.conv_3.model.2.conv.bn.weight", "arcface.conv_3.model.2.conv.bn.bias", "arcface.conv_3.model.2.conv.bn.running_mean", "arcface.conv_3.model.2.conv.bn.running_var", "arcface.conv_3.model.2.conv.bn.num_batches_tracked", "arcface.conv_3.model.2.conv.prelu.weight", "arcface.conv_3.model.2.conv_dw.conv.weight", "arcface.conv_3.model.2.conv_dw.bn.weight", "arcface.conv_3.model.2.conv_dw.bn.bias", "arcface.conv_3.model.2.conv_dw.bn.running_mean", "arcface.conv_3.model.2.conv_dw.bn.running_var", "arcface.conv_3.model.2.conv_dw.bn.num_batches_tracked", "arcface.conv_3.model.2.conv_dw.prelu.weight", "arcface.conv_3.model.2.project.conv.weight", "arcface.conv_3.model.2.project.bn.weight", "arcface.conv_3.model.2.project.bn.bias", "arcface.conv_3.model.2.project.bn.running_mean", "arcface.conv_3.model.2.project.bn.running_var", "arcface.conv_3.model.2.project.bn.num_batches_tracked", "arcface.conv_3.model.3.conv.conv.weight", "arcface.conv_3.model.3.conv.bn.weight", "arcface.conv_3.model.3.conv.bn.bias", "arcface.conv_3.model.3.conv.bn.running_mean", "arcface.conv_3.model.3.conv.bn.running_var", "arcface.conv_3.model.3.conv.bn.num_batches_tracked", "arcface.conv_3.model.3.conv.prelu.weight", 
  "arcface.conv_3.model.3.conv_dw.conv.weight", "arcface.conv_3.model.3.conv_dw.bn.weight", "arcface.conv_3.model.3.conv_dw.bn.bias", "arcface.conv_3.model.3.conv_dw.bn.running_mean", "arcface.conv_3.model.3.conv_dw.bn.running_var", "arcface.conv_3.model.3.conv_dw.bn.num_batches_tracked", "arcface.conv_3.model.3.conv_dw.prelu.weight", "arcface.conv_3.model.3.project.conv.weight", "arcface.conv_3.model.3.project.bn.weight", "arcface.conv_3.model.3.project.bn.bias", "arcface.conv_3.model.3.project.bn.running_mean", "arcface.conv_3.model.3.project.bn.running_var", "arcface.conv_3.model.3.project.bn.num_batches_tracked", "arcface.conv_34.conv.conv.weight", "arcface.conv_34.conv.bn.weight", "arcface.conv_34.conv.bn.bias", "arcface.conv_34.conv.bn.running_mean", "arcface.conv_34.conv.bn.running_var", "arcface.conv_34.conv.bn.num_batches_tracked", "arcface.conv_34.conv.prelu.weight", "arcface.conv_34.conv_dw.conv.weight", "arcface.conv_34.conv_dw.bn.weight", "arcface.conv_34.conv_dw.bn.bias", "arcface.conv_34.conv_dw.bn.running_mean", "arcface.conv_34.conv_dw.bn.running_var", "arcface.conv_34.conv_dw.bn.num_batches_tracked", "arcface.conv_34.conv_dw.prelu.weight", "arcface.conv_34.project.conv.weight", "arcface.conv_34.project.bn.weight", "arcface.conv_34.project.bn.bias", "arcface.conv_34.project.bn.running_mean", "arcface.conv_34.project.bn.running_var", "arcface.conv_34.project.bn.num_batches_tracked", "arcface.conv_4.model.0.conv.conv.weight", "arcface.conv_4.model.0.conv.bn.weight", "arcface.conv_4.model.0.conv.bn.bias", "arcface.conv_4.model.0.conv.bn.running_mean", "arcface.conv_4.model.0.conv.bn.running_var", "arcface.conv_4.model.0.conv.bn.num_batches_tracked", "arcface.conv_4.model.0.conv.prelu.weight", "arcface.conv_4.model.0.conv_dw.conv.weight", "arcface.conv_4.model.0.conv_dw.bn.weight", "arcface.conv_4.model.0.conv_dw.bn.bias", "arcface.conv_4.model.0.conv_dw.bn.running_mean", "arcface.conv_4.model.0.conv_dw.bn.running_var", "arcface.conv_4.model.0.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.0.conv_dw.prelu.weight", "arcface.conv_4.model.0.project.conv.weight", "arcface.conv_4.model.0.project.bn.weight", "arcface.conv_4.model.0.project.bn.bias", "arcface.conv_4.model.0.project.bn.running_mean", "arcface.conv_4.model.0.project.bn.running_var", "arcface.conv_4.model.0.project.bn.num_batches_tracked", "arcface.conv_4.model.1.conv.conv.weight", "arcface.conv_4.model.1.conv.bn.weight", "arcface.conv_4.model.1.conv.bn.bias", "arcface.conv_4.model.1.conv.bn.running_mean", "arcface.conv_4.model.1.conv.bn.running_var", "arcface.conv_4.model.1.conv.bn.num_batches_tracked", "arcface.conv_4.model.1.conv.prelu.weight", "arcface.conv_4.model.1.conv_dw.conv.weight", "arcface.conv_4.model.1.conv_dw.bn.weight", "arcface.conv_4.model.1.conv_dw.bn.bias", "arcface.conv_4.model.1.conv_dw.bn.running_mean", "arcface.conv_4.model.1.conv_dw.bn.running_var", "arcface.conv_4.model.1.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.1.conv_dw.prelu.weight", "arcface.conv_4.model.1.project.conv.weight", "arcface.conv_4.model.1.project.bn.weight", "arcface.conv_4.model.1.project.bn.bias", "arcface.conv_4.model.1.project.bn.running_mean", "arcface.conv_4.model.1.project.bn.running_var", "arcface.conv_4.model.1.project.bn.num_batches_tracked", "arcface.conv_4.model.2.conv.conv.weight", "arcface.conv_4.model.2.conv.bn.weight", "arcface.conv_4.model.2.conv.bn.bias", "arcface.conv_4.model.2.conv.bn.running_mean", "arcface.conv_4.model.2.conv.bn.running_var", "arcface.conv_4.model.2.conv.bn.num_batches_tracked", "arcface.conv_4.model.2.conv.prelu.weight", "arcface.conv_4.model.2.conv_dw.conv.weight", "arcface.conv_4.model.2.conv_dw.bn.weight", "arcface.conv_4.model.2.conv_dw.bn.bias", "arcface.conv_4.model.2.conv_dw.bn.running_mean", "arcface.conv_4.model.2.conv_dw.bn.running_var", "arcface.conv_4.model.2.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.2.conv_dw.prelu.weight", "arcface.conv_4.model.2.project.conv.weight", "arcface.conv_4.model.2.project.bn.weight", "arcface.conv_4.model.2.project.bn.bias", "arcface.conv_4.model.2.project.bn.running_mean", "arcface.conv_4.model.2.project.bn.running_var", "arcface.conv_4.model.2.project.bn.num_batches_tracked", "arcface.conv_4.model.3.conv.conv.weight", "arcface.conv_4.model.3.conv.bn.weight", "arcface.conv_4.model.3.conv.bn.bias", "arcface.conv_4.model.3.conv.bn.running_mean", "arcface.conv_4.model.3.conv.bn.running_var", "arcface.conv_4.model.3.conv.bn.num_batches_tracked", "arcface.conv_4.model.3.conv.prelu.weight", "arcface.conv_4.model.3.conv_dw.conv.weight", "arcface.conv_4.model.3.conv_dw.bn.weight", "arcface.conv_4.model.3.conv_dw.bn.bias", "arcface.conv_4.model.3.conv_dw.bn.running_mean", "arcface.conv_4.model.3.conv_dw.bn.running_var", "arcface.conv_4.model.3.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.3.conv_dw.prelu.weight", "arcface.conv_4.model.3.project.conv.weight", "arcface.conv_4.model.3.project.bn.weight", "arcface.conv_4.model.3.project.bn.bias", "arcface.conv_4.model.3.project.bn.running_mean", "arcface.conv_4.model.3.project.bn.running_var", "arcface.conv_4.model.3.project.bn.num_batches_tracked", "arcface.conv_4.model.4.conv.conv.weight", "arcface.conv_4.model.4.conv.bn.weight", "arcface.conv_4.model.4.conv.bn.bias", "arcface.conv_4.model.4.conv.bn.running_mean", "arcface.conv_4.model.4.conv.bn.running_var", "arcface.conv_4.model.4.conv.bn.num_batches_tracked", "arcface.conv_4.model.4.conv.prelu.weight", "arcface.conv_4.model.4.conv_dw.conv.weight", "arcface.conv_4.model.4.conv_dw.bn.weight", "arcface.conv_4.model.4.conv_dw.bn.bias", "arcface.conv_4.model.4.conv_dw.bn.running_mean", "arcface.conv_4.model.4.conv_dw.bn.running_var", "arcface.conv_4.model.4.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.4.conv_dw.prelu.weight", "arcface.conv_4.model.4.project.conv.weight", "arcface.conv_4.model.4.project.bn.weight", "arcface.conv_4.model.4.project.bn.bias", "arcface.conv_4.model.4.project.bn.running_mean", "arcface.conv_4.model.4.project.bn.running_var", "arcface.conv_4.model.4.project.bn.num_batches_tracked", "arcface.conv_4.model.5.conv.conv.weight", "arcface.conv_4.model.5.conv.bn.weight", "arcface.conv_4.model.5.conv.bn.bias", "arcface.conv_4.model.5.conv.bn.running_mean", "arcface.conv_4.model.5.conv.bn.running_var", "arcface.conv_4.model.5.conv.bn.num_batches_tracked", "arcface.conv_4.model.5.conv.prelu.weight", "arcface.conv_4.model.5.conv_dw.conv.weight", "arcface.conv_4.model.5.conv_dw.bn.weight", "arcface.conv_4.model.5.conv_dw.bn.bias", "arcface.conv_4.model.5.conv_dw.bn.running_mean", "arcface.conv_4.model.5.conv_dw.bn.running_var", "arcface.conv_4.model.5.conv_dw.bn.num_batches_tracked", "arcface.conv_4.model.5.conv_dw.prelu.weight", "arcface.conv_4.model.5.project.conv.weight", "arcface.conv_4.model.5.project.bn.weight", "arcface.conv_4.model.5.project.bn.bias", "arcface.conv_4.model.5.project.bn.running_mean", "arcface.conv_4.model.5.project.bn.running_var", "arcface.conv_4.model.5.project.bn.num_batches_tracked", "arcface.conv_45.conv.conv.weight", "arcface.conv_45.conv.bn.wu.weight", "arcface.conv_45.conv_dw.conv.weight", "arcface.conv_45.conv_dw.bn.weight", "arcface.conv_45.conv_dw.bn.bias", "arcface.conv_45.conv_dw.bn.running_mean", "arcface.conv_45.conv_dw.bn.running_var", "arcface.conv_45.conv_dw.bn.num_batches_tracked", "arcface.conv_45.conv_dw.prelu.weight", "arcface.conv_45.project.conv.weight", "arcface.conv_45.project.bn.weight", "arcface.conv_45.project.bn.bias", "arcface.conv_45.project.bn.running_mean", "arcface.conv_45.project.bn.running_var", "arcface.conv_45.project.bn.num_batches_tracked", "arcface.conv_5.model.0.conv.conv.weight", "arcface.conv_5.model.0.conv.bn.weight", "arcface.conv_5.model.0.conv.bn.bias", "arcface.conv_5.model.0.conv.bn.running_mean", "arcface.conv_5.model.0.conv.bn.running_var", "arcface.conv_5.model.0.conv.bn.num_batches_tracked", "arcface.conv_5.model.0.conv.prelu.weight", "arcface.conv_5.model.0.conv_dw.conv.weight", "arcface.conv_5.model.0.conv_dw.bn.weight", "arcface.conv_5.model.0.conv_dw.bn.bias", "arcface.conv_5.model.0.conv_dw.bn.running_mean", "arcface.conv_5.model.0.conv_dw.bn.running_var", "arcface.conv_5.model.0.conv_dw.bn.num_batches_tracked", "arcface.conv_5.model.0.conv_dw.prelu.weight", "arcface.conv_5.model.0.project.conv.weight", "arcface.conv_5.model.0.project.bn.weight", "arcface.conv_5.model.0.project.bn.bias", "arcface.conv_5.model.0.project.bn.running_mean", "arcface.conv_5.model.0.project.bn.running_var", "arcface.conv_5.model.0.project.bn.num_batches_tracked", "arcface.conv_5.model.1.conv.conv.weight", "arcface.conv_5.model.1.conv.bn.weight", "arcface.conv_5.model.1.conv.bn.bias", "arcface.conv_5.model.1.conv.bn.running_mean", "arcface.conv_5.model.1.conv.bn.running_var", "arcface.conv_5.model.1.conv.bn.num_batches_tracked", "arcface.conv_5.model.1.conv.prelu.weight", "arcface.conv_5.model.1.conv_dw.conv.weight", "arcface.conv_5.model.1.conv_dw.bn.weight", "arcface.conv_5.model.1.conv_dw.bn.bias", "arcface.conv_5.model.1.conv_dw.bn.running_mean", "arcface.conv_5.model.1.conv_dw.bn.running_var", "arcface.conv_5.model.1.conv_dw.bn.num_batches_tracked", "arcface.conv_5.model.1.conv_dw.prelu.weight", "arcface.conv_5.model.1.project.conv.weight", "arcface.conv_5.model.1.project.bn.weight", "arcface.conv_5.model.1.project.bn.bias", "arcface.conv_5.model.1.project.bn.running_mean", "arcface.conv_5.model.1.project.bn.running_var", "arcface.conv_5.model.1.project.bn.num_batches_tracked", "arcface.sep.weight", "arcface.sep_bn.weight", "arcface.sep_bn.bias", "arcface.sep_bn.running_mean", "arcface.sep_bn.running_var", "arcface.sep_bn.num_batches_tracked", "arcface.prelu.weight", "arcface.GDC_dw.weight", "arcface.GDC_bn.weight", "arcface.GDC_bn.bias", "arcface.GDC_bn.running_mean", "arcface.GDC_bn.running_var", "arcface.GDC_bn.num_batches_tracked", "arcface.features.weight", "arcface.last_bn.weight", "arcface.last_bn.bias", "arcface.last_bn.running_mean", "arcface.last_bn.running_var", "arcface.last_bn.num_batches_tracked".
  ```

  

- 本地模型加载不成功的问题报错日志如下：

  ```bash
  ==================================================
  人脸识别系统测试
  ==================================================
  
  ===== 初始化人脸识别系统 =====
  
  1. 加载 YOLOv8-face 检测模型...
     ✓ YOLO 模型加载成功
  
  2. 设备配置: cpu
  
  3. 加载 ArcFace 特征提取模型...
     使用 insightface 内置模型...
  
  3. 加载 ArcFace 特征提取模型...
  C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\albumentations\check_version.py:147: UserWarning: Error fetching version info The read operation timed out
    data = fetch_version_info()
    使用 insightface 内置模型...
  download_path: C:\Users\fp/.insightface\models\buffalo_l
  Downloading C:\Users\fp/.insightface\models\buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...
  Traceback (most recent call last):
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
      response = self._make_request(
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
      response = conn.getresponse()
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\connection.py", line 565, in getresponse
      httplib_response = super().getresponse()
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\http\client.py", line 1375, in getresponse
      response.begin()
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\http\client.py", line 318, in begin
      version, status, reason = self._read_status()
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\http\client.py", line 287, in _read_status
      raise RemoteDisconnected("Remote end closed connection without"
  http.client.RemoteDisconnected: Remote end closed connection without response
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\requests\adapters.py", line 644, in send
      resp = conn.urlopen(
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
      retries = retries.increment(
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\util\retry.py", line 474, in increment
      raise reraise(type(error), error, _stacktrace)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\util\util.py", line 38, in reraise
      raise value.with_traceback(tb)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
      response = self._make_request(
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
      response = conn.getresponse()
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\urllib3\connection.py", line 565, in getresponse
      httplib_response = super().getresponse()
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\http\client.py", line 1375, in getresponse
      response.begin()
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\http\client.py", line 318, in begin
      version, status, reason = self._read_status()
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\http\client.py", line 287, in _read_status
      raise RemoteDisconnected("Remote end closed connection without"
  urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\face_detect\test.py", line 164, in __init__
      self.app = FaceAnalysis(providers=['CPUExecutionProvider'])
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\insightface\app\face_analysis.py", line 27, in __init__
      self.model_dir = ensure_available('models', name, root=root)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\insightface\utils\storage.py", line 28, in ensure_available
      return download(sub_dir, name, force=False, root=root)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\insightface\utils\storage.py", line 17, in download
      download_file(model_url,
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\insightface\utils\download.py", line 71, in download_file
      r = requests.get(url, stream=True)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\requests\api.py", line 73, in get
      return request("get", url, params=params, **kwargs)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\requests\api.py", line 59, in request
      return session.request(method=method, url=url, **kwargs)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\requests\sessions.py", line 589, in request
      resp = self.send(prep, **send_kwargs)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\requests\sessions.py", line 703, in send
      r = adapter.send(request, **kwargs)
    File "C:\Users\fp\miniconda3\envs\facedetect\lib\site-packages\requests\adapters.py", line 659, in send
      raise ConnectionError(err, request=request)
  requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\face_detect\test.py", line 538, in <module>
      result_img, detections = process_image(
    File "C:\face_detect\test.py", line 428, in process_image
      system = FaceRecognitionSystem(
    File "C:\face_detect\test.py", line 228, in __init__
      self.arcface_extractor = InsightFaceExtractor()
    File "C:\face_detect\test.py", line 171, in __init__
      raise Exception(f"insightface 加载失败: {e}")
  Exception: insightface 加载失败: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
  ```

  

### 17日：

##### 陌生人检测：

综合测试脚本成功检测人脸但是不能提取人脸特征，调整人脸数据库结构依旧没有作用不能进行陌生人检测，怀疑是图片格式的问题。选取小体型的模型进行检测方便后续在开发板上测试。



### 18日：

##### 陌生人检测：

服务器测试demo成功，测试了远距离和近距离的基于人脸检测陌生人检测。结果显示检测率均达到0.5左右且模型使用的均为10MB左右的小模型。下一步准备放在orangepi上测试效果。如果没问题将模型转为rknn格式再接着测试。



### 19日：

##### 陌生人检测：

解决旧开发板测试包版本冲突导致的环境问题，测试发现YOLO模型可以正确识别人脸但是arcface检测不到人脸无法进行陌生人判断。怀疑是两种模型需要输入的图片比例不一致导致arcface不能正常接收人脸数据进行特征提取。



### 20日：

##### 陌生人检测：

测试了arcface检测异常问题是图片转换问题但是检测相似度不对，替换了识别模型效果可以。现在可以进行基于本地人脸数据库的陌生人检测功能。设置了人脸在画面比例中占到0.45才会进行检测结合YOLO识别人脸置信度0.8一起过滤常见情况避免频繁误测。

 

### 23日：

##### 陌生人检测：

创建了陌生人检测功能包并将检测代码重构运行了功能包，解决摄像头初始化以及模型位置声明问题。测试了功能包对陌生人检测功能发现测试效果没问题。后面加测添加人像看会不会检测为陌生人。



### 24日：

##### 人脸数据获取：

通过调用本地摄像头的方式采集人脸数据并存储在开发板上，目前缺少必要的引导客户使用的界面。



##### 陌生人检测：

测试陌生人检测功能包，目前存在的问题是采集到的人脸数据放在功能包中不能被正确识别，考虑是人脸特征读取失败。



### 25日：

##### 陌生人检测：

陌生人检测功能包失败的原因是初始化人脸数据库失败是由人脸图片拍摄对焦问题导致的，现在解决了这个问题目前功能包可以正常运行了。



### 26日：

##### 开发文档书写：

完成陌生人检测开发文档的书写并将功能包和开发文档上传至svn上。



### 27日：

##### 声音识别：

从网上查找采集声音并识别是否为尖叫、哭喊，目前看到又可以识别是否为婴儿哭喊的实现思路。



##### 手势识别：

考虑使用基础手势如手掌向上、向下、向左以及向右移动控制小车前进、后退、向左以及向右搭配人体跟随的功能实现规避跟随失控的情况。



### 30日：

##### 手势识别：

服务器配置开发环境，测试基于图片的手势识别是否能够跑通。发现图片中若存在多个手掌只能识别一个，考虑实现左右方向控制以及前进后退四个功能。

pip show opencv-python
Name: opencv-python
Version: 4.10.0.84



## 十二月

### 1日：

##### 手势识别：

服务器上尝试使用 mediapipe 实现对手掌方向的识别但是测试过程发现文件识别错误，查看官方文档学习怎么使用模型进行识别。



### 2日：

##### 手势识别：

在旧开发板上部署手势识别开发环境发现有两个包的环境冲突寻找解决办法或者不冲突的版本。



##### 陌生人识别：

在小车上配置环境修改为功能包相对路径寻找方式成功部署，打算和建图一起测试室内环境的检测调整检测阈值。

- 远程连接

  - 根本原因：SSH 拒绝连接，因为远程主机的 host key 已更改。Remote-SSH 出于安全考虑拒绝启用端口转发（遥测原因：MitmPortForwardingDisabled）。日志显示 “WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!” 并建议运行以下命令：
  
  ```bash
  # Fetch and show the fingerprint:
  ssh-keyscan -t ed25519 192.168.1.141 2>/dev/null | ssh-keygen -lf -
  
  # Remove the old known_hosts entry (as suggested by the log)
  ssh-keygen -f ~/.ssh/known_hosts -R "192.168.1.141"
  
  # Add/accept the new key
  ssh orangepi@192.168.1.141
  ssh-keyscan 192.168.1.141 >> ~/.ssh/known_hosts
  ```
  
  

### 3日：

##### 陌生人检测测试：

小车上运行功能包一段时间会强制关机，原因可能是弱电模式下 CPU 过载会强制关机。尝试通过转换模型格式使用 NPU 加速推理让 CPU 不被跑满解决这个问题。完成人脸检测模型（YOLO、 arcface）的转换。



### 4日：

##### 陌生人检测功能包：

将模型修改为rknn格式，修改功能包后发现两种模型输出不一样导致检测到的人脸数量不对并且人脸特征提取有问题。



##### 手势识别：

模型提供的可识别手型包括6种可以提供六种功能，服务器准备测试识别视频检测功能正常再转到旧开发板测试摄像头实时识别。                                                                                                                                                                                                                                                                                                                               

```
周结：
一、陌生人检测功能开发与优化
   在小车上完成功能包部署，采用相对路径方式优化环境配置
   发现弱电模式下CPU过载导致强制关机问题
   完成YOLO、arcface模型转换为rknn格式以使用NPU加速推理
   模型格式转换后出现输出不一致问题，导致人脸数量检测异常和特征提取错误

二、手势识别功能开发
   服务器端配置开发环境，验证基于图片的手势识别功能
   在旧开发板部署发现环境包冲突问题，正在寻找兼容版本解决方案

下周计划：
陌生人检测功能包优化：功能包修改小车测试
手势识别开发板移植：计划完成视频识别测试后转移到开发板进行实时摄像头识别
```



### 7日：

##### 陌生人检测功能包：

测试了都替换为rknn格式模型但是报错解决不了输入问题，选择yolo模型使用rknn格式arcface模型使用onnx格式进行识别，重构功能包中。



### 8日：

##### 陌生人检测功能包：

尝试将功能包重新封装遇到功能包编译失败解决了这个问题，根据日志显示加载了错误的模型但是修改配置文件和启动文件都没有效果目前还在寻找解决办法。

- 模型启动失败：

  ```bash
  [face_detect_node-1] W rknn-toolkit-lite2 version: 2.3.0
  [face_detect_node-1] W Query dynamic range failed. Ret code: RKNN_ERR_MODEL_INVALID. (If it is a static shape RKNN model, please ignore the above warning message.)
  [face_detect_node-1] [ERROR] [1765186050.290603470] [face_detect_node]: 人脸识别系统初始化失败: 'RKNNLite' object has no attribute 'get_input_attrs'
  ```

  

### 9日：

##### 陌生人检测功能包：

解决配置文件编译不生效问题，还是遇到启动模型失败的问题涉及检测脚本已解决这个问题。运行功能包报错提取人脸特征失败。

* RKNN Lite 2.3.0 的API极度简化**：该版本**完全不提供 `get_inputs()`, `get_outputs()`, `input_hapes`, `output_shapes` 等常见接口。所有形状信息都需要通过**硬编码**或**运行时推断**获得。

  ```python
  # 检测脚本
  #!/usr/bin/env python3
  
  from rknnlite.api import RKNNLite
  import os
  
  # 加载RKNN模型
  model_path = "/home/orangepi/Final_test/face_detect/robot_face_detect/install/robot_face_detect/share/robot_face_detect/model/yolov8n-face-lindevs.rknn"
  
  print("="*60)
  print("RKNN Lite API 调试工具")
  print("模型路径:", model_path)
  print("="*60)
  
  # 初始化RKNN
  rknn = RKNNLite()
  
  # 加载模型
  ret = rknn.load_rknn(model_path)
  print(f"\n[1] 加载模型返回码: {ret}")
  
  # 初始化运行时
  ret = rknn.init_runtime(core_mask=RKNNLite.NPU_CORE_0)
  print(f"[2] 初始化运行时返回码: {ret}")
  
  # ==================== 探索API结构 ====================
  
  print("\n[3] --- 探索RKNN对象的所有属性和方法 ---")
  all_attrs = dir(rknn)
  print(f"rknn对象共有 {len(all_attrs)} 个属性和方法:")
  
  # 过滤掉魔术方法
  public_attrs = [a for a in all_attrs if not a.startswith('__')]
  for attr in public_attrs:
      print(f"  - {attr}")
  
  # ==================== 尝试获取输入/输出信息 ====================
  
  print("\n[4] --- 尝试获取输入信息 ---")
  
  # 方法1: 尝试get_inputs()
  try:
      inputs = rknn.get_inputs()
      print(f"✅ get_inputs() 成功: {inputs}")
      print(f"   类型: {type(inputs)}")
      if inputs:
          print(f"   第一个元素类型: {type(inputs[0])}")
          print(f"   第一个元素dir: {dir(inputs[0])}")
          if hasattr(inputs[0], 'shape'):
              print(f"   第一个元素shape: {inputs[0].shape}")
  except Exception as e:
      print(f"❌ get_inputs() 失败: {e}")
  
  # 方法2: 尝试get_input_shapes()
  try:
      input_shapes = rknn.get_input_shapes()
      print(f"✅ get_input_shapes() 成功: {input_shapes}")
  except Exception as e:
      print(f"❌ get_input_shapes() 失败: {e}")
  
  # 方法3: 尝试直接访问inputs属性
  try:
      inputs = rknn.inputs
      print(f"✅ rknn.inputs 成功: {inputs}")
      print(f"   类型: {type(inputs)}")
  except Exception as e:
      print(f"❌ rknn.inputs 失败: {e}")
  
  # 方法4: 尝试直接访问input_shapes属性
  try:
      input_shapes = rknn.input_shapes
      print(f"✅ rknn.input_shapes 成功: {input_shapes}")
  except Exception as e:
      print(f"❌ rknn.input_shapes 失败: {e}")
  
  # ==================== 尝试获取输出信息 ====================
  
  print("\n[5] --- 尝试获取输出信息 ---")
  
  # 方法1: 尝试get_outputs()
  try:
      outputs = rknn.get_outputs()
      print(f"✅ get_outputs() 成功: {outputs}")
      print(f"   类型: {type(outputs)}")
      if outputs:
          print(f"   第一个元素类型: {type(outputs[0])}")
          if hasattr(outputs[0], 'shape'):
              print(f"   第一个元素shape: {outputs[0].shape}")
  except Exception as e:
      print(f"❌ get_outputs() 失败: {e}")
  
  # 方法2: 尝试get_output_shapes()
  try:
      output_shapes = rknn.get_output_shapes()
      print(f"✅ get_output_shapes() 成功: {output_shapes}")
  except Exception as e:
      print(f"❌ get_output_shapes() 失败: {e}")
  
  # 方法3: 尝试直接访问outputs属性
  try:
      outputs = rknn.outputs
      print(f"✅ rknn.outputs 成功: {outputs}")
  except Exception as e:
      print(f"❌ rknn.outputs 失败: {e}")
  
  # 方法4: 尝试直接访问output_shapes属性
  try:
      output_shapes = rknn.output_shapes
      print(f"✅ rknn.output_shapes 成功: {output_shapes}")
  except Exception as e:
      print(f"❌ rknn.output_shapes 失败: {e}")
  
  # ==================== 查看RKNN版本 ====================
  
  print("\n[6] --- RKNN Lite版本信息 ---")
  print(f"RKNNLite类: {RKNNLite}")
  print(f"模块路径: {RKNNLite.__module__}")
  print(f"版本: {getattr(RKNNLite, '__version__', '未知')}")
  
  # 尝试查看__init__方法的文档
  if hasattr(RKNNLite.__init__, '__doc__'):
      print(f"RKNNLite.__init__文档: {RKNNLite.__init__.__doc__}")
  
  # 释放资源
  rknn.release()
  
  print("\n" + "="*60)
  print("调试完成")
  print("="*60)
  ```

  

- 核心矛盾：您的RKNN模型输出是**特征图 (1, 64, 80, 80) ，而非 检测框 (1, 84, 8400) **。原因是转换RKNN时导出了 错误的输出节点（可能选择了某个卷积层的输出，而非检测头的输出）。

  > ONNX模型输出：(1, 84, 8400) —— 8400个候选框，每个框84维（4坐标+1置信度+79类）
  > RKNN模型输出：(1, 64, 80, 80) —— 80x80的特征图，64通道（这是主干网络的特征层）

  - 解决办法：重新转换YOLO模型到RKNN，放进功能包中人脸特征提取失败并且检测不到人脸。问题出在模型转换（ONNX -> RKNN）

    ```python
    from rknn.api import RKNN
    import os
    
    def convert_model():
        rknn = RKNN()
        rknn.config(
            mean_values=[[0, 0, 0]],          # 归一化均值
            std_values=[[255, 255, 255]],     # 归一化标准差
            target_platform='rk3588',         # 目标平台
            output_optimize=True,             # ✅ 正确：使用布尔值
        )
        
        # 加载ONNX模型（不指定outputs，自动检测）
        print("加载ONNX模型...")
        ret = rknn.load_onnx(model='yolov8n-face-lindevs.onnx')
        if ret != 0:
            print(f"加载ONNX失败: {ret}")
            return
        
        # 构建RKNN模型
        # 需要准备dataset.txt文件，包含至少10张校准图片路径
        print("构建RKNN模型...")
        ret = rknn.build(
            do_quantization=True,             # 启用量化
            dataset='dataset.txt'             # 校准数据集
        )
        if ret != 0:
            print(f"构建失败: {ret}")
            return
        
        # 导出RKNN模型
        print("导出RKNN模型...")
        ret = rknn.export_rknn('yolov8n-face-lindevs.rknn')
        if ret != 0:
            print(f"导出失败: {ret}")
            return
        
        print("✅ RKNN模型转换成功!")
        rknn.release()
    
    if __name__ == '__main__':
        convert_model()
    ```

  - 重新转换模型：添加 init_runtime() 核心修复，推理前必须初始化。

    ```python
    #!/usr/bin/env python3
    from rknn.api import RKNN
    import cv2
    import numpy as np
    
    def convert_with_dataset():
        """使用人脸数据集重新转换YOLOv8-face模型"""
        
        # 1. 创建RKNN实例
        rknn = RKNN(verbose=True)
        
        # 2. 配置参数（针对人脸检测优化）
        print("配置RKNN参数...")
        rknn.config(
            mean_values=[[0, 0, 0]],          # 输入归一化
            std_values=[[255, 255, 255]],
            target_platform='rk3588',
            # 人脸检测关键：启用后处理优化
            output_optimize=True,
            # 量化参数（减少精度损失）
            quantized_dtype='w8a8',            # 8位量化
            optimization_level=3,            # 最高优化级别
            # 不指定outputs，自动检测所有检测头
        )
        
        # 3. 加载ONNX模型
        print("加载ONNX模型...")
        onnx_path = 'model/yolov8n-face-lindevs.onnx'
        ret = rknn.load_onnx(model=onnx_path)
        if ret != 0:
            print(f"❌ 加载ONNX失败: {ret}")
            exit(ret)
        
        # 4. 准备量化校准数据集（必须包含人脸图片）
        print("准备校准数据集...")
        '''
        # 如何创建dataset.txt:
        cd /home/orangepi/Final_test/face_detect/robot_face_detect/src/robot_face_detect/known_faces
        find . -name "*.jpg" | head -50 > ../../dataset.txt
        '''
        
        # 5. 构建RKNN模型（关键步骤）
        print("开始构建RKNN模型（这可能需要3-5分钟）...")
        ret = rknn.build(
            do_quantization=True,
            dataset='image/imageslist.txt',
        )
        if ret != 0:
            print(f"❌ 构建失败: {ret}")
            exit(ret)
        
        # 6. 验证模型输出
        print("验证模型输出...")
        # 加载一张测试图片
        test_img = cv2.imread('image/people.jpg')
        img_resized = cv2.resize(test_img, (640, 640))
        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
        img_input = img_rgb.astype(np.float32) / 255.0
        img_input = np.expand_dims(img_input, 0)
        
        # 推理测试
        # outputs = rknn.inference(inputs=[img_input])
        # print(f"✅ 输出形状: {outputs[0].shape}")
        # print(f"✅ 置信度范围: {outputs[0][0,4,:].min():.3f} ~ {outputs[0][0,4,:].max():.3f}")
        
        # 7. 导出RKNN模型
        output_path = 'model/yolov8n-face-lindevs-correct.rknn'
        ret = rknn.export_rknn(output_path)
        if ret != 0:
            print(f"❌ 导出失败: {ret}")
            exit(ret)
        
        print(f"✅ 转换成功！新模型: {output_path}")
        rknn.release()
    
    if __name__ == '__main__':
        convert_with_dataset()
    ```

    

##### 训练手势识别模型：



### 10日：

##### 陌生人识别：

功能包重新封装现在集和检测以及发布检测内容，包括时间戳以及监测结果。



##### 手势识别：

目前训练检测出六种手势，出前进后退控制左右外还有两个手势可以指定功能。



### 11日：

##### 手势识别：

服务器功能验证完成，在开发板创建功能包验证测试单个文件没有问题且不会造成过多资源消耗但是存在环境冲突目前想把功能包完善再解决环境问题。

```
周结：
1. 陌生人检测功能包
     修复了编译失败问题。
     通过修改检测脚本，解决了模型配置文件不生效和启动模型失败的顽固错误。
     对功能包进行了整体重构，集成了人脸检测、识别和结果发布流程，能够发布包含时间戳和检测结果的完整信息。

2. 手势识别功能
	模型训练取得初步成果：在服务器上完成了手势识别模型的训练。
	已实现功能定义：模型能够检测出六种手势。其中四种手势映射为前进、后退、左转、右转的移动控制命令，另外两种手势可作为自定义功能的触发信号。

下周计划：
1.  核心攻坚：确保陌生人检测核心功能稳定可用。解决人脸特征提取失败问题。
2.  功能集成与测试：将训练好的手势识别模型部署到开发环境中，开始与小车控制逻辑的集成与测试工作。
```



### 14日：

##### 手势识别：

功能包创建完毕完成测试并且已经可以在本地获取摄像头数据进行手势识别。现在的需要解决开发板中mediapipe包中版本冲突问题。



### 15日：

##### 手势识别：

解决了环境冲突的问题，源头是远程连接造成的环境冲突误判。项目环境中的包与mediapipe所需的两个包是向下兼容的并且完成功能包的开发板测试上传至SVN。



### 16日：

##### 手势识别：

书写手势识别开发文档并放在svn上。



##### 新闻总结助手：

考虑早晨启动后自动总结然后用户选择是否读出，或者用户主动选择使用此功能（小伴小伴，告诉我今天有哪些值得关注的新闻）。



### 17日：

##### 新闻总结助手：

确定开发架构以及需要使用的技术。写出基础demo显示内容不理想，内容质量层次不齐信息不可溯源。



##### 服务器部署文档：

写了启动指令怎么指定端口怎么设置密钥以及怎么调用，虚拟机部署的话需要使用容器安装合适的显卡驱动。



### 18日：

##### 新闻总结助手：

修改热点新闻搜集整理代码现在生成的文档比昨天的要好并且可以正确溯源。



##### 功能包测试：

查看功能包测试文档指定功能包测试计划。



### 21日：

##### 新闻总结助手：

通过测试发现原来使用的是硬编码，分析微博热搜页面结构，发现需要通过API才能获取实时数据最终确定使用XXAPI提供的微博热搜接口。现在可以获取基础信息下一步获取内容总结。



### 22日：

##### 摔倒检测测试：

| 用例编号   | 模块     | 子模块       | 测试场景            | 前置条件                                    | 测试步骤                                          | 预期结果                                                     | 优先级 | 实际结果 | 备注                                                         |
| ---------- | -------- | ------------ | ------------------- | ------------------------------------------- | ------------------------------------------------- | ------------------------------------------------------------ | ------ | -------- | ------------------------------------------------------------ |
| TC-FAL-001 | 跌倒检测 | 视觉检测     | 正常摔倒            | 摄像头无遮挡                                | 测试人员在地垫上正常摔倒                          | 1.10s内准确识别，并发出警报（灯光和音效）<br />2.准确通知家属和客服台 | P0     |          | 需测试骨骼追踪算法                                           |
| TC-FAL-001 | 跌倒检测 | 视觉+IMU融合 | 70%身体被遮挡下跌倒 | 1. 在沙发后模拟跌倒<br>2. 遮挡摄像头50%视野 | 测试人员在障碍物后正常摔倒                        | 1.机器突然失去目标，发出语音提示并接受对方回复<br />2. 备用方案检测准确率≥80%<br/>3. 报警延迟≤15秒<br />4.通知家属和客服台 | p0     |          | 1.需测试骨骼追踪算法<br />2.备用方案（音频检测跌倒声撞击声音）<br />3.需测试本地AI芯片的故障切换逻辑 |
| TC-FAL-003 | 跌倒检测 | 误报过滤     | 老人缓慢坐下/弯腰   | 摄像头无遮挡                                | 1. 连续模拟10次坐姿<br />2.连续模拟10次弯腰       | 1. 误报次数≤1次                                              | p0     |          | 需测试骨骼追踪算法                                           |
| TC-FAL-011 | 跌倒检测 | 复杂动作干扰 | 瑜伽/太极动作模拟   | 1. 测试员连续做10组高难度动作（如倒立）     | 1. 记录误报次数 <br />2. 检查骨骼关键点追踪稳定性 | 1. 误报率≤1次/小时 <br />2. 动作结束后自动恢复监控           | P2     |          | 需建立常见运动动作白名单（太极，倒立等）                     |

---

#### 

##### 火灾检测测试：

| 用例编号   | 模块     | 子模块         | 测试场景                               | 前置条件                                                    | 测试步骤                                           | 预期结果                                                     | 优先级 | 实际结果 | 备注                     |
| ---------- | -------- | -------------- | -------------------------------------- | ----------------------------------------------------------- | -------------------------------------------------- | ------------------------------------------------------------ | ------ | -------- | ------------------------ |
| TC-FIR-001 | 火灾检测 | 视觉火焰识别   | 不同燃烧物质识别（纸张/布料/油脂火灾） | 1. 在安全舱内点燃标准火源<br>2. 调整摄像头角度              | 1. 观察火焰识别<br>2. 记录响应时间                 | 1. 火焰形状识别准确率≥98%<br>2. 灯光语音等报警延迟≤10秒<br />3.通知家属和客服台 | P0     |          |                          |
| TC-FIR-002 | 火灾检测 | 烟雾传感器校准 | 高湿度环境误报测试（浴室蒸汽干扰）     | 1. 在湿度>85%环境中释放水蒸气                               | 1. 监测MQ-2传感器数值<br>2. 验证是否触发误报警     | 1. 误报率≤1次/24小时<br>                                     | P1     |          |                          |
| TC-FIR-003 | 火灾检测 | 光学干扰       | 强光反射模拟火焰（阳光照射玻璃瓶）     | 1. 环境光照强度>80000lux<br>2. 设置反光物体<br />3.测试10次 | 1. 监测火焰识别误报次数<br>                        | 1. 误报率≤1次<br>                                            | P1     |          | 需测试红外或温度传感器   |
| TC-FIR-004 | 火灾检测 | 图片干扰       | 视觉识别图片中的火焰                   | 1.在室内墙壁摆放多张火焰图片<br />2.测试10次                | 1. 监测火焰识别误报次数<br>                        | 1. 误报率≤1次<br>                                            | P1     |          | 需测试红外或温度传感器   |
| TC-FIR-005 | 火灾检测 | 化学干扰       | 酒精喷雾导致的误报                     | 1. 喷洒75%浓度酒精<br />2.测试10次                          | 1. 记录传感器响应时间和数据<br>2. 检查是否触发警报 | 1. 酒精喷雾误报率≤1次<br>                                    | P2     |          | 需区分可燃气体与火灾特征 |



##### 语音交互测试：

| 用例编号   | 模块 | 子模块         | 测试场景                      | 前置条件                                            | 测试步骤                                                 | 预期结果                                       | 优先级 | 实际结果 | 备注                        |
| ---------- | ---- | -------------- | ----------------------------- | --------------------------------------------------- | -------------------------------------------------------- | ---------------------------------------------- | ------ | -------- | --------------------------- |
| TC-VOI-001 | 语音 | 正常环境       | 正常环境下指令识别            | 1.普通室内环境识别语音<br /><br />2.普通话指令      | 1. 说“播放京剧”<br />2.发出10次指令                      | 1. 识别准确率≥90%<br/>2. 响应时间≤2秒<br/>     | P1     |          | 需测试麦克风阵列波束成形    |
| TC-VOI-002 | 语音 | 噪声环境       | 噪声下指令识别                | 1.嘈杂的环境<br /><br />2.普通话指令                | 1. 播放白噪声<br/>2. 说“播放京剧”<br />3.发出10次指令    | 1. 识别准确率≥85%<br/>2. 响应时间≤3秒<br/>     | P1     |          | 需测试麦克风阵列波束成形    |
| TC-VOI-003 | 语音 | 多语言混合指令 | 中英文混杂指令（"播放music"） | 1. 设置语言为中文优先<br>2. 语音库包含1000+英文单词 | 1. 发送混合指令<br>2. 验证执行准确性<br />3.发送10次指令 | 1. 混合指令识别率≥75%<br>2. 自动追问确认机制   | P2     |          | 需测试自然语言处理(NLP)模型 |
| TC-VOI-004 | 语音 | 方言指令       | 粤语指令（"播放music"）       | 1. 内置方言语音模型<br>2. 正常环境                  | 1. 发送方言指令<br>2. 验证执行准确性<br />3.发送10次指令 | 1. 方言识别率≥75%<br>2. 自动追问确认机制<br /> | P2     |          | 需测试自然语言处理(NLP)模型 |



##### 情绪检测测试：

测试了常规环境下的检测，目前需要测试强光环境下的情况。

| 用例编号   | 模块 | 子模块       | 测试场景              | 前置条件                     | 测试步骤                                        | 预期结果                                   | 优先级 | 实际结果 | 备注               |
| ---------- | ---- | ------------ | --------------------- | ---------------------------- | ----------------------------------------------- | ------------------------------------------ | ------ | -------- | ------------------ |
| TC-EMO-001 | 情绪 | 微表情识别   | 多种表情快速测试      | 0.5秒悲伤表情                | 1. 快速切换多种表情<br />2.每种表情连续测试10次 | 1. 识别准确率≥90%<br/>2.数据精准存储       | P2     |          |                    |
| TC-EMO-002 | 情绪 | 极端光照条件 | 强光/弱光下的情绪识别 | 1. 光照强度>10000lux或<50lux | 1. 模拟5种表情<br>2. 记录识别准确率             | 1. 强光下准确率≥85%<br>2. 弱光下准确率≥80% | P1     |          | 需测试红外补光效果 |



##### 语音唤醒测试：

| 用例编号    | 模块     | 子模块       | 测试场景               | 前置条件                                          | 测试步骤                                                     | 预期结果                                      | 优先级 | 实际结果 | 备注                   |
| ----------- | -------- | ------------ | ---------------------- | ------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------- | ------ | -------- | ---------------------- |
| TC-WAKE-001 | 语音唤醒 | 低功耗响应   | 待机30分钟后唤醒       | 1. 机器人待机≥30分钟<br>2. 环境噪音≤30dB          | 1. 距3米处说20次唤醒词<br>2. 测量从唤醒到响应时间<br>3. 记录待机功耗 | 1. 响应时间≤1.5秒<br>2. 唤醒成功率≥80%        | P0     |          | 需测试自定义唤醒词支持 |
| TC-WAKE-002 | 语音唤醒 | 噪声抗干扰   | 75dB电视背景音下唤醒   | 1. 播放持续电视声（新闻节目）<br>2. 环境噪音>70dB | 1. 在噪声峰值时说出20次唤醒词<br>2. 记录唤醒次数             | 1. 唤醒成功率≥80%<br>                         | P0     |          |                        |
| TC-WAKE-003 | 语音唤醒 | 模糊发音容错 | 方言/口音唤醒词        | 1. 正常环境<br>2. 用户带地方口音                  | 1. 用地方口音说唤醒词<br>2. 重复测试20次                     | 1. 唤醒成功率≥80%<br>                         | P1     |          | 需支持方言库扩展       |
| TC-WAKE-004 | 语音唤醒 | 多语言混合   | 中英文混合唤醒词       | 1. 设置语言为中文优先<br>2. 唤醒词包含英文单词    | 1. 说"Hi小助手"唤醒<br>2. 验证语音引擎切换逻辑               | 1. 混合唤醒成功率≥75%<br>2. 自动追问确认机制  | P2     |          | 需测试语种自动检测功能 |
| TC-WAKE-05  | 语音唤醒 | 紧急打断     | 唤醒后立即发出紧急指令 | 1. 成功唤醒机器人<br>2. 间隔≤0.3秒说"拨打120"     | 1. 记录指令执行成功率<br>2.连续测试10次                      | 1. 紧急指令响应时间≤0.5秒<br>2. 成功率大于8次 | P0     |          |                        |



##### 宠物检测测试：

| 用例编号   | 模块 | 子模块   | 测试场景                 | 前置条件               | 测试步骤                                                     | 预期结果                                   | 优先级 | 实际结果 | 备注 |
| ---------- | ---- | -------- | ------------------------ | ---------------------- | ------------------------------------------------------------ | ------------------------------------------ | ------ | -------- | ---- |
| TC-PET-001 | 宠物 | 单类宠物 | 针对环境中的一类宠物图片 | 明亮无遮挡环境下的测试 | 1. 打开功能包并测试<br />2.频繁切换检测是否可以正常检测      | 1. 识别准确率≥90%<br/>2.数据精准存储       | P2     |          |      |
| TC-PET-002 | 宠物 | 多类宠物 | 针对环境中的两类宠物图片 | 明亮无遮挡环境下的测试 | 1. 打开功能包并测试<br>2. 选取包含两类目标的图片记录识别准确率 | 1. 强光下准确率≥85%<br>2. 弱光下准确率≥80% | P1     |          |      |



##### 血液检测测试：

| 用例编号     | 模块 | 子模块       | 测试场景              | 前置条件                           | 测试步骤                                                     | 预期结果                                   | 优先级 | 实际结果 | 备注 |
| ------------ | ---- | ------------ | --------------------- | ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------ | ------ | -------- | ---- |
| TC-BLOOD-001 | 血液 | 理想条件     | 光亮充足且无遮挡      | 开启功能包并保证摄像头可以正常工作 | 1. 测试含有血液的图片<br />2.在环境中切换测试正确率          | 1. 识别准确率≥50%<br/>2.数据精准存储       | P2     |          |      |
| TC-BLOOD-002 | 血液 | 极端光照条件 | 强光/弱光下的血液识别 | 理想条件下可以正确识别血液且无误测 | 1. 测试强光条件下以及弱光条件下测试结果<br>2. 记录识别准确率 | 1. 强光下准确率≥30%<br>2. 弱光下准确率≥40% | P1     |          |      |



##### 失水检测测试：

| 用例编号     | 模块 | 子模块       | 测试场景         | 前置条件                           | 测试步骤                                        | 预期结果                                   | 优先级 | 实际结果 | 备注 |
| ------------ | ---- | ------------ | ---------------- | ---------------------------------- | ----------------------------------------------- | ------------------------------------------ | ------ | -------- | ---- |
| TC-WATER-001 | 失水 | 理想条件     | 光亮充足且无遮挡 | 开启功能包并保证摄像头可以正常工作 | 1. 快速切换多种环境<br />2.每种情况连续测试10次 | 1. 识别准确率≥80%<br/>2.数据精准存储       | P2     |          |      |
| TC-WATER-002 | 失水 | 极端光照条件 | 强光下的情绪识别 | 1. 光照强度>10000lux或<50lux       | 1. 模拟高光环境<br>2. 记录识别准确率            | 1. 强光下准确率≥50%<br>2. 弱光下准确率≥70% | P1     |          |      |



##### 陌生人检测测试：

| 用例编号        | 模块   | 子模块       | 测试场景          | 前置条件                     | 测试步骤                                            | 预期结果                                   | 优先级 | 实际结果 | 备注 |
| --------------- | ------ | ------------ | ----------------- | ---------------------------- | --------------------------------------------------- | ------------------------------------------ | ------ | -------- | ---- |
| TC-STRANGER-001 | 陌生人 | 理想条件     | 光亮充足且无遮挡  | 收集好用户照片               | 1. 快速切换多个人和环境<br />2.每种情况连续测试10次 | 1. 识别准确率≥50%<br/>2.数据精准存储       | P2     |          |      |
| TC-STRANGER-002 | 陌生人 | 极端光照条件 | 强光/弱光下的识别 | 1. 光照强度>10000lux或<50lux | 1. 模拟高光环境<br>2. 记录识别准确率                | 1. 强光下准确率≥50%<br>2. 弱光下准确率≥40% | P1     |          |      |







##### 功能包测试:

 完成所有功能包测试案例的书写包括摔倒检测、火灾检测、语音助手、情绪检测、语音唤醒、宠物检测、血液检测、失水检测以及陌生人检测。并且将检测的excel表格上传至svn上。







```
日结：
整理测试文件，准备工作交接。
书写服务器环境配置文档和模型部署文档已经放在61和svn上

周结：
总结所有功能包启动指令并放在交接文档中
模型整理并放在61
服务器环境配置文档和模型部署文档书写

下周计划：
整理交接文档和文件
新板子环境设置好后将功能包在新开发板上测试

月结：
陌生人检测功能开发与优化：对功能包进行了整体重构，集成了人脸检测、识别和结果发布流程，能够发布包含时间戳和检测结果的完整信息。
手势识别功能开发：模型能够检测出六种手势。其中四种手势映射为前进、后退、左转、右转的移动控制命令，另外两种手势可作为自定义功能的触发信号。
新闻总结助手：完成了对新闻的提取以及文档生成，下一步需要对内容读取并压缩方便用户了解。会在交接中说明开发方向。
测试文档书写：按照测试要求整理测试excel表格，并在旧开发板完成完整的测试任务。

下月计划：
整理交接文档和文件
新板子环境设置好后将功能包在新开发板上测试
```

















































