功能概述：语音指令分为两个模块，分别是联网时提供实时语音助手功能；本地控制机器人移动

------

## 一、整体架构设计 

#### 逻辑框架图

```
1.		      [用户语音]  
		          ↓
2.     [本地语音识别模型检测关键词]
		          ↓
3.	 	      [误触机制]		->			   [机器人控制] 
		          ↓						 		 ↓
4. 	   		[启动语音助手]  			    [通过ROS2节点控制小车移动]
		    	  ↓
5.	[ASR接口、服务器模型部署、本地TTS]
```

------



#### 硬件平台：

##### 1. orangepi 5

- CPU : 八核 ARM Cortex-A76 + A55
- GPU : Mali-G76 MC4
- NPU : 支持 INT8/FP16，算力达 6TOPS，适合运行轻量级 AI 模型
- 操作系统 : 推荐使用 Linux（Ubuntu ）
- 内存 : 至少 4GB RAM（建议 8GB）

##### 2. 服务器

- 显卡：A100
- 内存：64G

------



## 二、功能模块划分 

|    类别    |               执行模块                |
| :--------: | :-----------------------------------: |
|  语音唤醒  |       关键词识别使用sherpa-onnx       |
|  语音识别  |     使用阿里云百炼的接口调用服务      |
|  智能助手  | 调用服务器部署Dify构建agent和相应工具 |
|  语音转换  |   使用板载部署的 Piper TTS 实时转录   |
| 机器人控制 |               ROS2节点                |



### 1. 语音唤醒

使用本地部署的 sherpa-onnx 编码、解码、全连接模型实现集于关键词的语音助手唤醒。Sherpa-onnx 是一个专为离线环境设计的语音处理工具箱，它非常适合在算力和内存有限的嵌入式设备上运行，不依赖网络避免了数据上传云端的隐私泄露风险能让你在本地实现高质量的语音交互。读者须知需要联合词表一起使用，功能实现模型以及词表文件如下：



##### 1.1 模型列表：

```yaml
keyword_decoder_model: '/rknn_model/decoder-epoch-12-avg-2-chunk-16-left-64.onnx'		# 编码
keyword_encoder_model: '/rknn_model/encoder-epoch-12-avg-2-chunk-16-left-64.onnx'		# 解码
keyword_joiner_model: '/rknn_model/joiner-epoch-12-avg-2-chunk-16-left-64.onnx'			# 全连接
```



##### 1.2  词表：

```yaml
keywords: '/rknn_model/keywords.txt'		# 定义需要识别的关键词
keyword_tokens: '/rknn_model/keyword_tokens.txt'		# 模型识别需要的 token 表
```



##### 1.3 开发流程：

功能介绍：基于onnx的轻量级语音识别框架，支持自动语音识别。使用sherpa-onnx达到本地语音识别功能，识别成功时将识别内容发布给主控，主控根据内容判断做机器人控制或语音助手功能。[本地部署sherpa-onnx](/home/yls/YOLOV8-on-RK3588/文档/关键词唤醒/关键词唤醒.md)

- 识别内容传输

  创建ROS功能包，整合配置文件包括创建rknn-model文件夹，在该文件下写入keyword.txt文件用于设置触发需要识别的关键词以及语音识别需要使用的模型

  ```
  # 目录结构
  ├──model
  	├── decoder-epoch-12-avg-2-chunk-16-left-64.onnx
  	├── encoder-epoch-12-avg-2-chunk-16-left-64.onnx
  	├── joiner-epoch-12-avg-2-chunk-16-left-64.onnx
  	├── keyword.txt
  ```

  

- 使用指令生成 keyword.txt ：

  - 准备生词表：

    ```
    # 暂定唤醒词
    小明你好
    
    # 暂定机器人控制指令
    向前
    向后
    向左
    向右
    前进
    后退
    跟我走
    ```

  - 使用模型生成词表：

    ```bash
    # 指令生成拼音
    sherpa-onnx-cli text2token   --tokens keyword_tokens.txt   --tokens-type ppinyin   keywords_raw.txt keywords.txt
    ```

    生成词表如下：

    ```
    # 需手动添加@xxxx
    x iǎo m íng n ǐ h ǎo @小明你好
    
    x iàng q ián @向前
    x iàng h òu @向后
    x iàng z uǒ @向左
    x iàng y òu @向右
    q ián j ìn @前进
    h òu t uì @后退
    g ēn w ǒ z ǒu @跟我走
    
    d ǎ g ěi ér z i @打给儿子
    d ǎ g ěi n ǚ ér @打给女儿
    
    j iù m ìng @救命
    y ǒu r én x ū y ào b āng zh ù @有人需要帮助
    k uài b ào j ǐng @快报警
    zh áo h uǒ @着火
    w ǒ b ù x íng l e k uài j iào j iù h ù ch ē @我不行了，快叫救护车
    ```

  - 控制信号传输：初始化 sherpa_onnx ，获取识别关键词以及语音识别模型等参数，创建订阅者`robot_audio_data`和发布者`asr_exacute`用于接收语音检查特定关键词然后发布关键词触发信号。

    - 创建订阅者和发布者话题：

      ```python
      # 创建订阅者，订阅话题
      self.subscription = self.create_subscription(
          Float32MultiArray,
          'robot_audio_data',
          self.audio_callback,
          10,
      )
      # 创建发布者
      self.publisher = self.create_publisher(String, 'asr_exactue', 10)
      ```

    - 关键词识别：检查音频流并对特定关键词进行识别传输启动语音助手脚本信号，识别的音频流采用只送入新数据如果使用滑动窗口原因时会重复识别导致一次关键词识别会返回两次结果导致语音助手唤醒两次，进而造成语音识别重复启动。对特定关键词进行识别并启动语音助手。

      ```python
      def audio_callback(self, msg):
          # 只送入新数据
          self.stream.accept_waveform(16000, np.array(msg.data, dtype=np.float32))
      
          # 尝试解码
          while self.keyword_spotter.is_ready(self.stream):
              self.keyword_spotter.decode_streams([self.stream])
      
          # 获取结果
          result = self.keyword_spotter.get_result(self.stream)
          if result and result == "小明你好":          # 只响应这一句
              self.get_logger().info(f"{result} is detected.")
              # 重置流，防止重复触发
              self.stream = self.keyword_spotter.create_stream()
              # 启动助手
              self._start_assistant_script()         # 其它关键词直接丢弃，不做任何处理
              
      # 语音助手脚本启动模块
      def _start_assistant_script(self):
          def run():
              try:
                  script = "src/robot_speech_recognition/robot_speech_recognition/rknn_infer/voice_assistant_test.py"
                  subprocess.Popen(["python3", script])
                  self.get_logger().info("已启动语音助手脚本")
              except Exception as e:
                  self.get_logger().error(f"启动语音助手失败：{e}")
      
          threading.Thread(target=run, daemon=True).start()
      ```

      

##### 1.4 识别硬件：

- 使用麦克风阵列或 USB 麦克风

- 通过 `pyaudio`实时录音

  ```shell
  pip install pyaudio
  ```

注意：开发过程中遇到开发板连接显示屏时声卡选择不正确，程序启动时程序默认使用显示器的声卡设备所以音频线需要连接显示器。



##### 1.5 其他选择：

使用 snowboy 作为关键词识别工具，用 ROS2 集成进功能包实现语音唤醒智能助手。Snowboy 是一款离线、轻量级的自定义热词检测引擎。"热词"也被称为"唤醒词"，就是像"Hey Siri"、"OK Google"或"小爱同学"这样的特定触发词。它的核心功能非常专注：持续监听音频流，当识别到预设的唤醒词时，触发一个动作，它本身不负责复杂的语音识别或自然语言理解。不过这个项目目前已经停止运维。



###### 语音助手唤醒优化：

- 唤醒项目部署编译：

  唤醒功能基于 snowboy 实现的一个识别多语言自定义关键词唤醒功能，下面介绍项目的部署文档。

  - 安装pyaudio

    ```bash
    sudo apt-get install libjack-dev
    sudo apt-get install pyaudio
    ```

  - 配置麦克风：连接麦克风使用网页或者指令检查麦克风是否可以正常使用。

    ```bash
    rec t.wav
    ```

  - 下载编译 swig 和 snowboy

    ```bash
    # 下载 swig 项目
    sudo wget http://downloads.sourceforge.net/swig/swig-3.0.10.tar.gz sudo tar -xvzf swig-3.0.10.tar.gz
    
    cd swig-3.0.10/
    
    # 编译 SWIG
    /configure  --prefix= /usr \ 
    			--without-clisp \		
    			--without-maximum-compile-warnings 
    
    sudo make & sudo make install
    
    sudo install -v-m755-d /usr/share/doc/swig-3.0.10 sudo cp -v-R Doc/* /usr/share/doc/swig-3.0.10 
    
    cd ..
    
    # 下载编译 snowboy 
    sudo git clone https://github.com/Kitt-Al/snowboy cd snowboy/swig/Python3 && sudo make	# 这一步可能报错详见下 编译 snowboy 项目错误
    ```

  - 配置修改

    打开/snowboy/snowboy/examples/Python3/snowboydecoder.py，把其中的“from . import snowboydetect”改为“import snowboydetect”。

    ```bash
    cd /snowboy-seasalt-master/snowboy/snowboy/examples/Python3
    ```

  - 复制所需文件到自己的工程目录

    创建一个自己的工程目录，然后把如下文件复制到目录中：

    1. /snowboy-seasalt-master/snowboy/swig/Python3/目录下除了“Makefile”以外的所有文件。

       ```bash
       sudo cp _snowboydetect.so /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboy-detect-swig.i /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboy-detect-swig.o /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboy-detect-swigcc /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboy-detect-swig.cc /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboydetect.py /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       ```

    2. /snowboy-seasalt-master/snowboy/目录下的"resource"文件夹及其所有文件。

       ```bash
       sudo cp -r resources/ /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       ```

    3. /snowboy-seasalt-master/snowboy/examples/Python3/目录下的“snowboydecoder.py”文件和”demo.py”。

       ```bash
       sudo cp demo.py /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       sudo cp snowboydecoder.py /media/yls/1T硬盘4/code/snowboy-seasalt-master/snowboy/examples/Python3/awake_keywords/
       ```

  - demo测试

    进入工程目录，输入指令进行测试。

    ```bash
    # 测试 demo 
    python3 demo.py resources/models/snowboy.umdl
    ```

    如果需要替换关键词可以按照下面的操作进行。

    1. [浏览器](https://snowboy.hahack.com/)进入模型生成网页按照要求完成语音输入并生成模型放在项目 /resources/models 文件夹下

    2. 修改上述指令进行测试

       ```
       python3 demo.py resources/models/xiaoming.umdl
       ```

       

- 编译 snowboy 项目错误

  ```bash
  g++ -I../../ -O3 -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++0x  -shared snowboy-detect-swig.o \
  ../..//lib/ubuntu64/libsnowboy-detect.a -L/usr/lib/python3.10/config-3.10-x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu  -lcrypt -ldl  -lm -lm  -lm -ldl -lf77blas -lcblas -llapack -latlas -o _snowboydetect.so
  /usr/bin/ld: 找不到 -lf77blas: 没有那个文件或目录
  /usr/bin/ld: 找不到 -lcblas: 没有那个文件或目录
  /usr/bin/ld: 找不到 -latlas: 没有那个文件或目录
  collect2: error: ld returned 1 exit status
  make: *** [Makefile:73：_snowboydetect.so] 错误 1
  ```

  - 错误原因：系统**缺少 ATLAS 及其相关开发包**（`libatlas-dev`、`libf77blas-dev` 等），导致链接器找不到这些数学库，直接安装缺失的开发包即可。

    ```bash
    sudo apt update
    sudo apt install libatlas-base-dev libf77blas-dev libcblas-dev
    
    # 在新版本新版本 Ubuntu 中已被整合进 libatlas-base-dev 或其他包。
    sudo apt update
    sudo apt install libatlas-base-dev
    
    # 重新编译并检查是否已经有编译文件产生
    cd snowboy/swig/Python3
    sudo make clean
    sudo make
    ls -lh _snowboydetect.so     # 应该输出类似 -rwxr-xr-x 1 root root 1.1M  9月 29 09:04 _snowboydetect.so
    ```

  ```bash
  "fatal error: Python.h: No such file or directory"
  ```

  - 错误原因：可能是 Python 开发包没有安装

    ```bash
    sudo apt-get install python3-dev
    
    sudo apt-get install sox		# 不安装可能不能正常唤醒
    ```



### 2. 语音识别

使用阿里云百炼的接口实现 ASR 自动语音转文字的功能，模型选择 gummy-realtime-v1，后续可以在硬件充足的情况下考虑开源部署在服务器。核心代码如下：

```python
# 启动语音识别
translator = TranslationRecognizerRealtime(
    model="gummy-realtime-v1",
    format="pcm",
    sample_rate=16000,
    transcription_enabled=True,
    translation_enabled=False,  # 关闭翻译功能
    callback=callback,
)
translator.start()

def on_open(self) -> None:
    global mic, stream
    print("TranslationRecognizerCallback open.")
    mic = pyaudio.PyAudio()
    default_device_index = mic.get_default_input_device_info()['index']
    stream = mic.open(
        format=pyaudio.paInt16, 
        channels=1, 
        rate=16000, 
        input=True,
        input_device_index=default_device_index
        # input_device_index=3
    )

def on_close(self) -> None:
    global mic, stream
    print("TranslationRecognizerCallback close.")
    if stream:
        stream.stop_stream()
        stream.close()
    if mic:
        mic.terminate()
    stream = None
    mic = None

def on_event(
    self,
    request_id,
    transcription_result: TranscriptionResult,
    translation_result: TranslationResult,
    usage,
) -> None:
    # 如果正在处理响应，忽略新的识别结果
    if self.is_processing:
        return

    if transcription_result is not None:
        with self.lock:
            text = transcription_result.text
            print(f"[DEBUG] 识别内容：{text}")

            # 更新最后语音时间
            self.last_voice_time = time.time()

            # 保存识别结果
            self.transcriptions.append({
                "sentence_id": transcription_result.sentence_id,    
                "text": transcription_result.text,
                "timestamp": time.time()
            })
```



### 3. 智能助手

智能体模型部署使用的 vLLM ，它是一个专为大语言模型设计的高性能推理引擎，它能显著提升模型服务的吞吐量和效率，同时优化内存使用。通过PagedAttention技术高效管理内存，并结合连续批处理，vLLM能同时处理大量请求，吞吐量可达传统框架的24倍。PagedAttention技术像操作系统管理内存一样处理模型的KV缓存，基本消除内存碎片，使GPU显存利用率高达99.8%，让大模型用更少的资源服务更多用户。支持流式输出，并允许新请求无需等待整个批次完成即可加入处理，大幅降低用户等待时间。下面是部署操作：



##### 3.1  Vllm 部署 Qwen3-8B 模型：

- 安装 vLLM ：安装

  ```bash
  pip install vllm -i https://pypi.tuna.tsinghua.edu.cn/simple
  
  # 检查是否有以下依赖
  vllm==0.6.6.post1
  torch==2.5.1
  transformers==4.48.0
  huggingface-hub==0.27.1
  ```

  

- 下载 modelscope：用于下载模型

  ```bash
  pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple
  ```

  

- 模型下载：

  ```bash
  modelscope download --model Qwen/Qwen3-1.7B --local_dir /home/eogee/models
  ```

  

- 启动服务：

  ```bash
  vllm serve /home/fp/.cache/modelscope/hub/models/Qwen/Qwen3-8B \
    --served-model-name Qwen3-8B \
    --host 0.0.0.0 \
    --port 9000 \
    --dtype half \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 \
    --tensor-parallel-size 1 \
    --api-key "123456" \  	# 密钥可自行调整
  ```

  

- 测试模型：

  ```bash
  curl -N \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer 123456" \
    http://<your_ip>:9000/v1/chat/completions \
    -d '{
          "model": "Qwen3-8B",
          "messages": [{"role": "user", "content": "用一句话介绍自己"}],
          "stream": true,
          "max_tokens": 64
        }'
  ```

  

##### 3.2 agent 开发和工具部署

Dify 是一个开源的 LLM 应用开发平台，其核心目标是让开发者甚至非技术人员都能快速构建和部署生成式 AI 应用。它通过整合 AI 应用开发、测试与上线的全流程，提供了一套高效、低代码的闭环解决方案。

- 使用docker compose本地部署dify：

  - docker compose工具：Linux中如果已安装 Docker Engine 20.10.13 及以上，`docker compose` 已作为子命令集成。

    ```shell
    # 查看docker版本
    docker version
    ```

  - 克隆Dify代码仓库：

    ```shell
    # 假设当前最新版本为 0.15.3
    git clone https://github.com/langgenius/dify.git --branch 0.15.3
    ```

  - 启动 Dify ：

    1. 进入 Dify 源代码的 Docker 目录

       ```shell
       cd dify/docker
       ```

    2. 复制环境配置文件

       ```shell
       cp .env.example .env
       ```

    3. 启动 Docker 容器，详细细节见Dify[官方文档](https://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose)

       ```shell
       docker compose up -d
       
       # 运行命令后，你应该会看到类似以下的输出，显示所有容器的状态和端口映射：
       [+] Running 11/11
        ✔ Network docker_ssrf_proxy_network  Created                                   
        ✔ Network docker_default             Created                                   
        ✔ Container docker-redis-1           Started                                   
        ✔ Container docker-ssrf_proxy-1      Started                                   
        ✔ Container docker-sandbox-1         Started                                   
        ✔ Container docker-web-1             Started                                   
        ✔ Container docker-weaviate-1        Started                                   
        ✔ Container docker-db-1              Started                                   
        ✔ Container docker-api-1             Started                                   
        ✔ Container docker-worker-1          Started                                  
        ✔ Container docker-nginx-1           Started     
        
        # 检查所有容器是不是都正常运行
        docker compose ps
       ```

  - 更新Dify：进入 dify 源代码的 docker 目录，按顺序执行以下命令：

    ```shell
    cd dify/docker
    docker compose down
    git pull origin main
    docker compose pull
    docker compose up -d
    ```

  - 访问Dify：进入下列的地址，设置帐号密码登陆就可以开始愉快的模型应用创作之旅了。

    ```shell
    # 本地环境
    http://localhost/install
    ```

  

- Dify从右上头像—>设置—>模型供应商—>安装模型供应商设置模型，例如使用ollama拉取deepseek-r1:1.5b

![image-20250623161146787](/home/yls/.config/Typora/typora-user-images/image-20250623161146787.png)



- 搜索引擎部署：使用Dify本地部署searXNG搜索引擎

  - 克隆github代码仓库

    ```shell
    git clone https://github.com/searxng/searxng-docker.git
    ```

  - 进入searxng-docker/searxng文件夹并修改settings.yml配置文件并更改密钥

    ```yaml
    # 密钥生成：使用openssl生成密钥
    Set-Content -Path "searxng/settings.yml" -Value ((Get-Content -Path "searxng/settings.yml" -Raw) -replace "ultrasecretkey", (openssl rand -hex 32))
    
    # 配置文件修改
    # see https://docs.searxng.org/admin/settings/settings.html#settings-use-default-settings
    use_default_settings: true
    server:
      # base_url is defined in the SEARXNG_BASE_URL environment variable, see .env and docker-compose.yml
      secret_key: "R+PzvVHeg97zDPWbGZKK4RfeVu+ZjZQPhkqsqysoUYU="  # change this!
      limiter: false  # enable this when running the instance for a public usage on the internet
      image_proxy: true
    ui:
      static_use_hash: true
    redis:
      url: redis://redis:6379/0
    search:
      formats:
        - html
        - json
        - rss
    ```

  - 修改searxng-docker/docker-compose.yaml配置文件：删除 caddy 部分并修改默认端口号

    |  Name   |             Description              |                         Docker image                         |                          Dockerfile                          |
    | :-----: | :----------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
    |  Caddy  | 反向代理（自动创建Lets Encrypt证书） | [docker.io/library/caddy:2-alpine](https://hub.docker.com/_/caddy) | [Dockerfile](https://github.com/caddyserver/caddy-docker/blob/master/Dockerfile.tmpl) |
    | SearXNG |             SearXNG本身              | [docker.io/searxng/searxng:latest](https://hub.docker.com/r/searxng/searxng) | [Dockerfile](https://github.com/searxng/searxng/blob/master/Dockerfile) |
    | Valkey  |              内存数据库              | [docker.io/valkey/valkey:8-alpine](https://hub.docker.com/r/valkey/valkey) | [Dockerfile](https://github.com/valkey-io/valkey-container/blob/mainline/Dockerfile.template) |

    ```yaml
    # 修改后配置文件
    version: "3.7"
    
    services:
      redis:
        container_name: redis
        image: docker.io/valkey/valkey:8-alpine
        command: valkey-server --save 30 1 --loglevel warning
        restart: unless-stopped
        networks:
          - searxng
        volumes:
          - valkey-data2:/data
        cap_add:
          - SETGID
          - SETUID
          - DAC_OVERRIDE
        logging:
          driver: "json-file"
          options:
            max-size: "1m"
            max-file: "1"
    
      searxng:
        container_name: searxng
        image: docker.io/searxng/searxng:latest
        restart: unless-stopped
        networks:
          - searxng
        ports:
          - "8081:8080"
        volumes:
          - ./searxng:/etc/searxng:rw
        environment:
          - SEARXNG_BASE_URL=http://${SEARXNG_HOSTNAME:-localhost}/
          - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
          - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
        cap_add:
          - CHOWN
          - SETGID
          - SETUID
        logging:
          driver: "json-file"
          options:
            max-size: "1m"
            max-file: "1"
    
    networks:
      searxng:
    
    volumes:
      valkey-data2:
    
    ```

  - 配置agent工具填写searXNG base url

    ```
    http://192.168.1.137:8081
    ```



- 天气查询工具：用 Dify + Ollama + DuckDuckGo 的方式做联网搜索，之所以可以搜新闻却“搜不到天气”，大概率是因为 DuckDuckGo 的搜索结果里对“天气”这类查询返回的是**卡片化/结构化数据**（例如只给出“今日天气请见 weather.com”并附一个外链），而不是一段可直接拿来当答案的文本。LLM 收到这种结果后，只能把链接抛给用户，于是看起来就像“搜索失败”。

  - 解决方案：通过自定义工具形式形式访问

    - 申请 API Key ：可以选择 高徳、和风以及彩云三家均提供免费查询额度，笔者试了和风和彩云分别提供50000以及10000额度每月其中彩云支持设置额度提醒功能好评。

      <img src="/home/yls/.config/Typora/typora-user-images/image-20250721100353532.png" alt="image-20250721100353532" style="zoom: 50%;" />

  

  - 创建自定义天气工具：

    - 选择自定义工具新建工具根据模板修改结构，这一步可以使用 [Swagger](https://editor.swagger.io/) 或者直接让AI生成，注意不要泄漏接口密钥，生成示例如下：

      ```json
      {
        "openapi": "3.1.0",
        "info": {
          "title": "Weather Query",
          "description": "根据城市名返回实时天气",
          "version": "v1.0.0"
        },
        "servers": [{ "url": "https://wttr.in" }],
        "paths": {
          "/{location}": {
            "get": {
              "operationId": "getWeather",
              "summary": "获取天气",
              "parameters": [
                {
                  "name": "location",
                  "in": "path",
                  "required": true,
                  "schema": { "type": "string" },
                  "description": "城市名，例如 Beijing"
                },
                {
                  "name": "format",
                  "in": "query",
                  "schema": { "type": "string", "default": "%l:+%c+%t+%h" },
                  "description": "返回格式"
                }
              ],
              "responses": {
                "200": {
                  "description": "纯文本天气信息"
                }
              }
            }
          }
        }
      }
      ```

      - 如接口需鉴权，点击「设置鉴权」选择 API Key 通常选择 Bearer 填写 token
      - 测试接口功能是否正常：点击右下角 「测试」输入参数 `location = Shanghai`能看到返回 `Shanghai: ⛅ +33°C 湿度 59%` 即成功

      - 更换浏览器工具：替换为 Google 可以做到浏览器信息查询得到正确数据。



##### 4.3 示例对话流程

```
用户说：“明天北京天气怎么样？”

→ ASR转成文本 → "明天北京天气怎么样？"

→ 大模型分析 → {"intent": "weather", "location": "北京", "time": "明天"}
```



### 4. 语音转换

基于本地部署的 Piper TTS 实现文字转换，Piper TTS 是一款完全离线运行、开源的高质量神经网络语音合成系统。它专为资源受限的环境设计，即使在树莓派（Raspberry Pi）这类嵌入式设备上，也能流畅地生成自然清晰的语音。所有语音合成任务均在设备本地完成，无需连接云端服务器，**不依赖网络**，也从根源上避免了语音数据泄露的风险，保障了用户隐私，专为树莓派等嵌入式设备优化，对算力要求低。部分轻量级模型（如`low`版本）体积仅约30MB，在树莓派4上也能实现**实时或超实时的语音合成**。



##### 4.1 piper TTS 服务搭建

使用 piper TTS 搭建了一个文字转语音服务的工具，效果还可以没有明显机械音。但是现在面临两个问题第一个会占用 CPU 资源，第二个不支持数字以及英文的转换只能转换纯中文文本。下面是部署和测试的具体流程。

- 环境准备：

  - Orange Pi 5S 通常运行的是基于 ARM64 的 Linux 系统（如 Ubuntu 或 Debian），你需要先确认系统架构：

    ```bash
    uname -m
    # 输出应为 aarch64（即 ARM64）
    ```

  - 安装依赖

    ```bash
    sudo apt update
    sudo apt install curl git unzip sox wget
    ```



- 下载并安装 Piper 可执行文件

  Piper 官方提供了预编译的 ARM64 版本，适合 Orange Pi 5S。

  ```bash
  # 下载并解压
  curl -LO https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_arm64.tar.gz
  tar -xzf piper_arm64.tar.gz
  cd piper
  
  # 确保 piper 可执行文件在当前目录下
  chmod +x piper
  ```



- 下载语音模型（中文示例）

  Piper 的模型托管在 Hugging Face，你可以手动下载或使用 `wget` 下载中文模型：

  ```bash
  # 创建模型目录
  mkdir -p models
  
  # 下载中文女声模型（示例）
  wget https://huggingface.co/rhasspy/piper-voices/resolve/main/zh/zh_CN/huayan/medium/zh_CN-huayan-medium.onnx -P models/
  wget https://huggingface.co/rhasspy/piper-voices/resolve/main/zh/zh_CN/huayan/medium/zh_CN-huayan-medium.onnx.json -P models/
  ```

  你也可以从 Hugging Face 浏览其他模型：https://huggingface.co/rhasspy/piper-voices/tree/main



- 运行 Piper TTS

  使用命令行测试语音合成并播放生成的音频（需安装 `alsa-utils`）：

  ```bash
  # 转换指令
  echo "你好，欢迎使用Piper语音合成！" | ./piper \
    --model models/zh_CN-huayan-medium.onnx \
    --config models/zh_CN-huayan-medium.onnx.json \
    --output_file output.wav
    
  #测试指令
  sudo apt install alsa-utils
  aplay output.wav
  ```

  后续如果使用可以使用下面的转换脚本

  ```python
  # 转换脚本
  from piper import Piper
  
  piper = Piper("models/zh_CN-huayan-medium.onnx")
  audio = piper.synthesize("你好，这是通过Python调用的语音合成。")
  
  with open("output.wav", "wb") as f:
      f.write(audio)
  ```



##### 4.2 piper TTS 语音转换音色问题：

```bash
[2025-09-26 10:48:21.652] [piper] [info] Loaded voice in 0.392397075 second(s)
[2025-09-26 10:48:21.652] [piper] [info] Initialized piper
[2025-09-26 10:48:23.422] [piper] [warning] Missing 3 phoneme(s) from phoneme/id map!
[2025-09-26 10:48:23.422] [piper] [warning] Missing "1" (\u0031): 1 time(s)
[2025-09-26 10:48:23.422] [piper] [warning] Missing "2" (\u0032): 7 time(s)
[2025-09-26 10:48:23.422] [piper] [warning] Missing "5" (\u0035): 20 time(s)
huayan_medium_test.wav
[2025-09-26 10:48:23.424] [piper] [info] Real-time factor: 0.14969781776494565 (infer=1.762841502 sec, audio=11.776 sec)
[2025-09-26 10:48:23.424] [piper] [info] Terminated piper
```

Missing 3 phoneme(s) … Missing "1" "2" "5"说明 **文本里混进了全角数字**（①②⑤ 这类 Unicode 圆圈序号），模型词表里找不到对应音素，于是跳过发音，听起来会漏字。

```bash
echo "春天来了，微风拂面，柳树抽出嫩芽，小草从土里探出头来，大地一片生机勃勃，仿佛一切都重新开始。" \
| perl -CSD -pe 's/[^\p{Han}，。！？、]//g' \
| ./piper \
    --model models/zh_CN-huayan-medium.onnx \
    --config models/zh_CN-huayan-medium.onnx.json \
    --output_file huayan_medium_test2.wav
```

- `perl -CSD`：开启 UTF-8 输入/输出/错误流
- `\p{Han}`：Unicode 汉字属性，等价于 `\u4e00-\u9fff` 但通用
- 只保留汉字和常用中文标点，**彻底剔除隐藏字符、全角数字、emoji 等**

运行后若日志里 **不再出现 `Missing ...`**，就说明净化成功，可放心播放 `huayan_medium_test2.wav`。



### 5. 机器人控制模块 

VoiceControl 是一个基于 ROS 2 的语音控制节点，用于接收语音识别结果（如“前进”、“后退”），并通过调用 motor_control_server 服务向机器人底层运动控制器发送线速度与角速度指令。该节点具备自动停止功能：在执行移动指令 3 秒后自动发送停止命令，防止机器人持续运行。本节点适用于语音交互式机器人系统，作为高层语音指令到低层运动控制的桥梁。



##### 5.1 功能特性

1. 订阅语音识别结果话题 asr_exactue
2. 支持“前进”和“后退”两种基本指令
3. 通过服务客户端调用 motor_control_server 发送运动指令
4. 自动在指令发出 3 秒后停止机器人
5. 支持客户端名称与优先级标识（用于多客户端仲裁）
6. 异步服务调用，避免阻塞主线程



##### 5.2 依赖接口

**5.2.1 订阅的话题（Subscriptions）**

| 话题名       | 消息类型            | 描述                                            |
| ------------ | ------------------- | ----------------------------------------------- |
| /asr_exactue | std_msgs/msg/String | 接收语音识别引擎输出的精确文本指令（如 "前进"） |



**5.2.2 调用的服务（Service Client）**

| 服务名                | 服务类型                          | 描述                         |
| --------------------- | --------------------------------- | ---------------------------- |
| /motor_control_server | robot_interfaces/srv/MotorControl | 向运动控制服务器发送速度指令 |



##### 5.3 参数配置

当前版本使用硬编码参数，未来可扩展为 ROS 2 参数：

| 参数         | 类型     | 默认值    | 说明                               |
| ------------ | -------- | --------- | ---------------------------------- |
| linear       | float    | 0.05      | 前进/后退线速度（m/s）             |
| angular      | float    | 0.1       | 转向角速度（rad/s）（当前未使用）  |
| Client_name  | string   | "Voice"   | 客户端标识名                       |
| Client_level | uint     | 2         | 客户端优先级（数值越小优先级越高） |
| 自动停止延时 | duration | 3 seconds | 发出移动指令后自动停止的时间       |

**建议：**后续可通过 declare_parameter() 支持动态配置。



##### 5.4 工作流程

节点voice监听asr_exactue话题，创建motor_control_server客户端用于发布速度，当节点监听到“前进”时，向motor_control_server发布0.05m/s的速度，并在发布速度后设置一个定时器，发布速度3秒后会发布一个0速给机器人停止，“后退”指令同理。

- 关键逻辑说明：
  1. 每次收到有效语音指令，先取消旧的停止定时器（防止多个定时器冲突）
  2. 发送移动指令后，立即创建新的 3 秒一次性定时器
  3. 定时器回调中发送零速度指令实现自动停止
  4. 所有服务调用均为异步，不影响语音订阅实时性



##### 5.5 使用方法

```bash
# 编译依赖：
确保工作空间包含以下包：
robot_interfaces（含 MotorControl.srv）
robot_control_service

colcon build --packages-select robot_control_service

# 运行节点：
ros2 run robot_control_service voice_control_node

# 测试指令：
模拟语音识别结果
ros2 topic pub /asr_exactue std_msgs/msg/String "data: '前进'" --once

# 预期行为：
终端打印 “Robot name: 前进”
机器人前进 3 秒后自动停止
日志显示 “已自动停止”
```



## 三、 测试和部署

本模块记录项目的运行测试和开发板部署问题，持续更新。

### 1. 测试

##### 1.1 误触测试：

在 orangepi 上完成5小时开启中途出现过一次关键识别但是没有触发语音助手。



### 2. 部署
