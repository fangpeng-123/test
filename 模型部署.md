# vLLM本地部署模型

大语言模型的落地应用离不开高效推理框架的支持，vLLM以其卓越的性能在众多框架中脱颖而出。本文将带你深入探索如何使用 vLLM 框架部署 DeepSeek-R1-Distill-Qwen 大语言模型，无论是深度学习新手还是有经验的开发者，都能从中获取实用的知识和技能。



## 系统要求

为了高效部署 DeepSeek-R1-Distill-Qwen，推荐使用 Ubuntu 22.04 LTS 操作系统、Python 3.12  环境、CUDA 12.1 与 PyTorch 2.3.0，并配备至少 24GB 显存的 NVIDIA GPU，以确保模型推理的高性能和稳定性。

- Linux 操作系统（推荐 Ubuntu 24.04+）

- Python 3.9+ （本文使用 Python 3.12）

- CUDA 支持的 GPU（可选，用于加速推理）

- 足够的内存空间（7B 模型建议至少 16GB RAM）

  

## 1. 驱动安装

### 1.1 检查默认显卡驱动

ubuntu 默认驱动 Nouveau 不能完全发挥显卡性能，需要手动禁止并安装 NVIDIA 显卡驱动。检查默认机器是否使用了默认的显卡驱动，如果使用了则需要删除然后安装 CUDA > 12.x 版本的驱动。

##### 清理旧驱动：

```bash
# 卸载所有 NVIDIA 相关组件
sudo apt purge *nvidia* *cuda* *cudnn*
sudo apt autoremove --purge
sudo apt autoclean

# 删除残留文件
sudo rm -rf /etc/apt/sources.list.d/cuda*
sudo rm -rf /usr/local/cuda*
sudo rm -rf ~/.nv/
```



##### 禁用 Nouveau 驱动：

```bash
# 创建黑名单文件
sudo nano /etc/modprobe.d/blacklist-nouveau.conf

# 添加内容
blacklist nouveau
options nouveau modeset=0

# 完成后执行
sudo update-initramfs -u
sudo reboot
```



### 1.2 安装 CUDA

##### 安装推荐驱动（带 Secure Boot 支持）

```bash
# 刷新软件列表
sudo apt update && sudo apt -y upgrade

# 添加官方显卡 PPA
sudo add-apt-repository ppa:graphics-drivers/ppa -y
sudo apt update

# 安装驱动 550（2024-06 验证 3070 稳定）
sudo apt install -y nvidia-driver-550

# 重启，使驱动生效
sudo reboot
```



##### 验证安装

```bash
# 验证内核模块
lsmod | grep nvidia

# 检查安全启动状态
mokutil --sb-state

# 驱动检查指令
nvidia-smi
```

若驱动安装成功则运行驱动检查指令会如下所示：

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05 Driver Version: 535.104.05 CUDA Version: 12.2         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| 30%   35C    P8    10W / 220W |     10MiB /  8192MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```



##### 安装 CUDA Toolkit（与驱动匹配的 12.2）

```bash
sudo apt install -y nvidia-cuda-toolkit

# 验证
nvcc --version
```



## 2. 虚拟环境创建

### 2.1 MiniConda 配置虚拟环境

##### 安装 MiniConda

- 下载安装包：根据系统选择对应版本安装包 https://www.anaconda.com/download/success



##### 创建环境

- 创建一个虚拟环境，其中`env_name`是自定义的虚拟环境名字，`python=x.x`指定python版本，创建成功终端如下图所示。

  ```bash
  conda create -n env_name python=x.x --yes
  ```

  ![image-20250810100459133](/home/yls/.config/Typora/typora-user-images/image-20250810100459133.png)

- 激活环境：conda 支持虚拟环境的查看、进入虚拟环境以及删除虚拟环境，如下所示：

  ```bash
  # 查看当前存在哪些虚拟环境
  conda env list
  
  # 进入虚拟环境
  conda activate env_name
  
  # 退出当前虚拟环境
  conda deactivate
  
  # 删除虚拟环境
  conda remove -n env_name --all
  ```

  

### 2.2 UV 配置虚拟

##### 安装 UV 工具

安装完成后，重新加载终端环境或执行 `source ~/.bashrc` 使 UV 命令生效。

```bash
# pip 安装 UV
curl -LsSf https://astral.sh/uv/install.sh | sh
```



##### 创建环境

```bash
# 创建新的虚拟环境，使用 Python 3.12
uv venv vllm --python 3.12 --seed

# 激活虚拟环境
source vllm/bin/activate

# 安装虚拟环境所需要的 python 包
uv pip install -r requirements.txt
```

`--seed` 参数会在环境中预装 pip 和 setuptools，确保基础工具可用。



## 3. vLLM本地部署

- 使用国内镜像源加速 vLLM 的安装过程：这个过程可能需要几分钟，因为 vLLM 包含了许多深度学习相关的依赖。

  ```bash
  uv pip install vllm -i https://pypi.tuna.tsinghua.edu.cn/simple
  
  # 或者采用国内镜像源
  pip install -i https://pypi.tuna.tsinghua.edu.cn/simple vllm==0.7.1
  ```



## 4. 模型下载与运行

### 4.1 模型下载

- 使用 ModelScope 来下载模型文件：

  ```bash
  # 安装 ModelScope CLI
  pip install modelscope
  
  # 下载 DeepSeek-R1-Distill-Qwen-7B 模型
  modelscope download --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --local_dir ./deepseek-ai
  ```

- 使用 huggingface_hub下载模型：

  ```bash
  # 安装 huggingface_hub
  pip install huggingface_hub
  
  #  下载 模型 deepseek-ai/DeepSeek-R1-Distill-Llama-70B  
  # ./deepseek-r1-70b  存放路径
  huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Llama-70B --local-dir ./deepseek-r1-70b 
  ```

- 启动 vLLM API 服务器

  ```bash
   python -m vllm.entrypoints.openai.api_server \
   --model ./deepseek-ai \
   --host 0.0.0.0 \
   --port 8000 \
   --max-model-len 4096 \
   --tensor-parallel-size 1
  ```

  参数说明：

  --model deepseek-ai: 指定模型路径
  --host 0.0.0.0: 监听所有网络接口
  --port 8000: API 服务端口
  --dtype float16: 使用半精度浮点数，节省显存
  --max-model-len 4096: 最大序列长度
  --tensor-parallel-size 1: 张量并行大小（单GPU设为1）
  --swap-space 8 增加用于 CPU 临时缓存的空间（单位 GB）。



### 4.2 模型检测优化

- 验证服务状态：服务启动后，可以通过以下命令验证：curl http://localhost:8000/v1/models。正常情况下会返回如下 JSON 响应：

  ```json
  {
      "object": "list",
      "data": [
          {
              "id": "./deepseek-ai",
              "object": "model",
              "created": 1748315666,
              "owned_by": "vllm",
              "root": "./deepseek-ai",
              "parent": null,
              "max_model_len": 131072,
              "permission": [
                  {
                      "id": "modelperm-40b207f111c642bd8ff80529c32faffb",
                      "object": "model_permission",
                      "created": 1748315666,
                      "allow_create_engine": false,
                      "allow_sampling": true,
                      "allow_logprobs": true,
                      "allow_search_indices": false,
                      "allow_view": true,
                      "allow_fine_tuning": false,
                      "organization": "*",
                      "group": null,
                      "is_blocking": false
                  }
              ]
          }
      ]
  }
  ```

  

- 性能优化：

  ```bash
  # 多GPU进行（如果有多个GPU）
   
  CUDA_VISIBLE_DEVICES=2,3 \
  python -m vllm.entrypoints.openai.api_server \
  --model ./deepseek-ai \     
  --host 0.0.0.0 \    
  --port 8000 \    
  --tensor-parallel-size 2 \
  --swap-space 8
  ```

  另外可以设置监控日志，用以记录详细的日志记录和性能监控。

  ```bash
  # 多GPU进行（如果有多个GPU）
   
  CUDA_VISIBLE_DEVICES=2,3 \
  python -m vllm.entrypoints.openai.api_server \
  --model ./deepseek-ai \     
  --host 0.0.0.0 \    
  --port 8000 \    
  --tensor-parallel-size 2 \
  --swap-space 8 \
  --log-level debug
  ```








```
sk-rahlmeuphajsszwmckmnvfosbovkurubpzpaftmebuugwvbr
sk-fekilrbdeqimrxpfifxbsnormhlhjabneyjygulphawkyhnf
sk-ozjsmyueavyqfuzsdwjqklwzuljdgoxpnuqkezhopanbvdwz
sk-ytcfqzxujtajmvyhtmoptwfivtemhlpldydtpjskasnwtrgn
sk-quqiiyzudprkkdceetjjtcwguuqjlzvelucegrhmwzhcotld
sk-ywrcvdkiyzvvkvtfhayvgcxxiqiukskkhuipwflixstbtrwb
sk-hgxuqbygcuqfifljlmrpatyfsvqbfqbyazqljjtltgqgryfc
sk-jccyhuocnyhlrqnsfmmlxvyqfwbbmxsouumwvllapnlwcfbn
```



```
1.sk-skbrlrdpccvgcnxkkxaqokgmekbifobgdqvjwjkcmbpykkkh
2.sk-jbxgrcxakmeshwgdkrpddjejljjtmlmnznjyosdiqhtodski
3.sk-exxmywnwercgekmrxvqlloseebfiilefmaieqjjigvkjafhn
4.sk-ehoqerukaxgoawyvamgfwjuzghlxnvydtfqqhutcgrerygpq
5.sk-xrxkqwqweybveqkrlskjscbgdocrahbokzqjycjsfjdbteyp
6.sk-yaoxfgamcrmkjbbhjuhtxermatoevugcodpbqdypdoftfmxd
7.sk-yyywqikqieblkkxrhzeifbfxdkxkiusnzyvvtitvfjmfdvch
8.sk-vfhnmjhpgxjifimqbgudlwrgvxpqukyeghfczfaoqlxiyhfo
9.sk-qfdqvfoyowvjsmuoywcydvmkcdwsxmppxcyouxyunrdadpkg
10.sk-enyjcwzmmuktydpvimnwnidlwkncgngttubdsuagfwksbxvf
```

